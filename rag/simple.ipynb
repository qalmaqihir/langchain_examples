{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A: Thank you so much for taking the time, we know how incredibly busy you are. You also have DG, and of course, you are dealing with Feli. So, we are already very happy that we have the opportunity here to get a shock and just before my brief maternity leave with 2 up to delivery, so I am really happy that we can at least now take a good deep dive into the DG1 process, because that\\'s more or less the intention of this meeting. We are trying to first map out the current A6 situation of those processes. B: You have already given us one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, one, the first. Well, this is like the email you sent, explaining how you go through such a process. I don\\'t want to bother you too much right now, but we have at least tried to, based on your description. C: I have plenty of time, actually, so it\\'s super great. Well, all good. But what I\\'m trying to do is also, based on your email where you give an example.\\nA: Noe Mad about the electric bikes for example, right? Yes, that you are sort of in a way there. We will walk through. Which step you must take and I have tried to translate that into a process. Flow uh, so the goal is that we get a view of Okay how does generic work now, I really emphasize on generic eh how the process goes eh and the goal is not to eh, so what I\\'m trying to prevent is not to focus too much on the deviations and variations of the process, because I understood that eh if you receive a request from a client it\\'s usually the information you wish for that you don\\'t always get and that is probably the case nine out of ten times eh, but they are less than you would actually say. No, generally speaking - we want to know a bit more now. Okay, okay, but it wasn\\'t there. Okay, because that\\'s why I\\'m saying that. B: <translated text> C: <translated text>\\nA: Because, of course, you can\\'t modulate the deviations from the process, so if you automate it, you can\\'t map out all possible scenarios. So you have to step back a level and look at it like, okay, if things are going well. How does the path run, so that\\'s a bit the goal. B: Had you seen this Fancy already? I don\\'t know if you have shared this with Filip yet.  C: Well no, okay, that\\'s fine. It\\'s not a big deal. But let\\'s go through it, is it a bit visible or yes, it\\'s just a small detail.  A: Okay, okay, they are welcome to come a bit closer. So what I\\'ve done is, I have eight points based on the folder structure that includes documents relating to customer visits, customer feedback, inbound and outbound services, and pending services.\\nA: Logged off and a TNO storage and then yes, and then you have to invoice so I would say it is based on the information we have received, but yes, so the reason why we are having this meeting is to get this right and to make the necessary adjustments that need to be made. So my question really is, yes, can we go through this and delve deeper into the layers, to go deeper into the layers. B: So if indeed they want that, that means Oh look, that is Yes okay yes, so um, So actually it\\'s a question of, is this recognizable as the first version in the story that you indeed it is. The first time I see it, so I must uh yes, so let\\'s indeed, so I have a start here and then there is a request from a customer and then, you need to confirm requests with a request for additional information. C: <Translated text>\\nA: Yes, because a request, in my opinion, only comes after you have a relationship with each other multiple times. Then the chances of the information you receive being complete, right? Yes, or it can be overly complete. It sometimes does happen.  B: It\\'s okay. They generally just throw everything they have over the fence and then we have to make something out of it. That also happens sometimes.  C: You either receive a truly comprehensive list or a good start on the right. It can happen. But do you have specific requirements for each product category, or can you make a fairly standard list that you must have anyway and cut, you know which customers it is. That\\'s one. Yes, you need to know if it happens with us or if it needs to happen somewhere else or what kind of assignment?\\nA: Yes, um and in terms of invoicing do you have that and then uh then you\\'re basically done if you just need to know what to do, where, what to say, you know? So that means what you need to do and with what, so what are those items that you need to handle, and uh and that, and with that, that is a packing list and those are MSDS, okay? B: And then uh where we could be here, the rest is listed. Okay, okay. C: And so there you do a check on that and if it is indeed not complete with the things you have mentioned, then uh, you see a path there, right? So information is not complete. Then a request for additional information goes to the customer and then it goes back, right, because then there comes another information moment from the customer, there\\'s another check and let\\'s just assume that it is now complete, right? And then it moves on to the second uh flow, so to speak, is that then is that a bit of glue with this looks quite normal\\nA: The only thing for sure is that, is that it is pretty good for everything I think, because when they ask what is correct? Yes, and if it\\'s not correct. Then we ask for more and then it will be correct again, so that\\'s logical. Yes, right? B: Yes, that seems right to me, yes. Then you go to the next path here and then you go to making a quotation if I understand correctly, right? Then I could contact Gregory, right? C: And then the quotation goes to the customer and then So it\\'s acceptable. I had to check what AC stands for and the other acceptance. So if the quotation is not accepted for whatever reason, then it comes back. It will be adjusted and then the quotation will be made again. And at some point it goes to the incoming if it\\'s accepted only that whole quotation making Or will the customer agree, so that can in principle also be skipped, because Okay, especially if you have been working together for a while and people just get a price list from us, so they are aware of us three.\\nA: Not forever they are hardly. Well, I don\\'t want to say hardly ever free, but it is indeed or they\\'re just really asking for a quote. So that\\'s the assignment and then the assignment comes later. So they ask that of. Hey, what would it cost to do this and then they come back later and say Okay, that\\'s fine and then you don\\'t need all that information like Where is it going and then you need all that sumehr information. Okay because then it\\'s not yet an assignment. Okay, but but uh it\\'s possible, it\\'s just that they may not need a quote. Then you go straight to number three, well level 3 so to speak but but but uh so if I understand correctly, the assignment and a quote are separate things. There are two different things, right. It is not always necessary for a quote to be made, right? B: No, no because sometimes there are just standard rates and then they already know what it costs, but are those already customers who are familiar with you. Yes, yes, that is generally the case. It is eh, and that yes, that is simply 90% uh 99% repeats. Yes, customers who are familiar with us of course, there are also many new offers uh over. C: Yes, that makes sense, it\\'s important to distinguish between the assignment and the quote, especially considering the existing customers and the standard rates applied.\\nA: Not me no, okay, okay, but every customer does have a specialty for example. Do you know things they don\\'t always do or need as Transport, right, but then uh, if we look at a percentage how many are specialties and how many are Regular, is it 90 10 is it 70 30 No well, I don\\'t know. Then I haven\\'t talked about that, but it is significant enough to take into account. So I don\\'t want to say fifty-fifty or something, but it does happen. It is more common that the price comes in higher, so say I think 40% or something, they want to know in advance what it costs and those are larger assignments.  B: Yes yes, but that is generally all based on known data and formulas, so you could say I have even processed it suddenly.  C: Yes yes we can still extract that from the data. Oh for sure. And they could also speak for themselves, but well, so maybe they will leave. No, okay okay okay, understood.\\nA: It is clear that indeed you don\\'t necessarily say the quotation preparation is not a crucial element and, uh, in the process, but that an assignment can be made and they know at Yes, Yes, I saw. Yes for completeness, well so that rectangle can be inclusively pulled down by you, right? B: No, the next one, this one, yes yes. So she learns completely and then you actually move on to cats, but yes, yes, yes, yes, yes, yes, yes, yes, okay, because then there will be another check or something else will come if, like, that, uh, yeah, um there are, like, around that the shipment information is complete and then that must actually go on to, but that is actually not the next thing and then for example, if it is incoming, then a separate reference must be given. C: Okay, uh, a pink oh okay. Oh yes. Yes, okay, okay, our dossier number, okay. And, uh, do we have to pick it up ourselves? That is coming in with the TV in this case.\\nA: Would like to know now, Where should we pick it up between What time at whose place should we report? What is ours later a little bit that is then again, but that is actually something that information is a complete already given must be, because there you already know what to do namely and where, And that that makes a complete assignment. B: Okay okay, does it happen that at this stage there is still information missing? C: Yes, I think we might have yes, but you often only find out when you see the shipment yourself, then you can for example see well okay, they have given 3M SDS, but they are team products. Yes yes. Yes, and then that for example or yes, the women always something for example it doesn\\'t fit in that packaging. Those were agreed upon or or, it\\'s more or it\\'s less or but that yes, you just handle all of that. You don\\'t need to, you can\\'t really record that. No no no no, fetched Why are quotes? They should also just be nicely subject to change.\\nA: Because it\\'s simply based on the data. That are known. We estimate that it may cost this of course, there can always be some inaccuracy.  B: Yes. Yes, okay. And um if we look at indeed entering, entering by own transport. If it\\'s in-house transport, what all is involved in that or what is not involved if you have to do it yourself, for example, you need to know dimensions weight. That is one, because of course, we have a map that you can use in principle.  C: Yes, 100 kilos have to be broken, if I remember correctly, something like that. Okay and it could also easily be that for example inbound, for example, it could be the order for transport by yourself or they come to deliver. Those are also two options actually. Oh, that is in the case of inbound that indeed you may need an additional external party, oh a transporter for example and that would be something where I see possibilities, coming from a future system.\\nA: Generating an order from that dude to where you just have all that information and that he can also be kicked in the sense that you can give a Transporter order, and this is 3-for, agreed? B: Well, that sounds nice and maybe I could. Well, that could communicate with the system of that guy, but I really don\\'t use it exactly. That\\'s nice. C: I think so too, because you see that there is quite a lot of work involved if you have to arrange that yourself, or do you have something in place? The thing is, it\\'s not even that much work. But you see that it\\'s always one or two persons who do it. And essentially, in my division, the principle has always been that if you\\'re a DG Expert in the office, then you shouldn\\'t have to, then you should be able to do everything. And now it\\'s mostly Youssef who does everything, and sometimes we\\'re both involved and now he\\'s also getting a bit busy with that, wanting to create such a group chat with the drinks for the door, and then we were in that, so it\\'s sort of followable, but otherwise not like one or both of those guys and then sort of to take it out.\\nA: Nobody knows anymore about rates, and also not about this information on the other side, so to speak. B: Yes, in any case, you lose access to information. Eventually, it will all be okay again, but because someone locally is working on it, with his own - for everyone. C: And how does that happen? Because the information is provided per email, for example, or via WhatsApp, for example, and per email.\\nA: During is generally even through WhatsApp and then they sent him a ref and he sends costs and if you\\'re lucky, then those costs will be properly entered in the file, but sometimes it is also forgotten and this is the specificity of that assignment. They actually disappear at the moment that the person who has access to that data stream is temporarily not there or something, or whatever actually it\\'s not very sustainable. I prefer it to be just like it used to be. We gave a transport assignment from the system and then you saw that Transporter assignment the details of it the agreed price and all the CM marks. For example, you know a biodi. They were simply listed there and that\\'s what you need. B: Okay, but isn\\'t that part of - if I may see myself as a layman, right, so I\\'m trying to paint a picture of how your process goes, because someone then does that Jack there right? Of that information provision at the second lien. That\\'s where the sender\\'s information is ugliest comp. C: <translated text>\\nA: Yes, but that happens if I understand correctly. It depends on how someone gathers that information, so for example, the way you do it is different from a Yusuf or uh, I think we\\'re quite on the same page, I think it\\'s not even that strange, because generally coordinating transportation is a bit of a level. 3rd, do you mean our almost two o\\'clock?   B: Yes, that mistake indeed. Yes, two two. Yes, those are secretly 9. So just yeah, apologies, 3 are indeed.   C: Yes, but upon arrival. What do you mean, huh, So this one huh. So the third one, to accept moments to give a quote is just ok, or there is no quote. We are complete and we move on to, say, bang then, that\\'s the moment when actually we have made a bit of a decision on what we need, because that is after accepting the assignment, then we proceed to executing, but how much more information will he provide that we need or yes, well, for example, if you are busy with Just Want.\\nA: For example, you and he are omitted. For example, if you or someone else then doesn\\'t give any more insight about who? Carries out the transporter and for example if that information comes in addition after this.    B: Say yes, depending on how the cargo arrives with us, it could be that someone could bring it. Yes, it could also be our own supply.  C: Yes, you could be someone who could bring it from the customer. The customer brings the same thing to us. That\\'s option 1, isn\\'t it. Actually, that\\'s actually for when picking up from the customer.\\nA: When he sees the order for \"Okay, this is for delivery using private transport, so he needs to collect everything.\"  B: Yeah, that is actually the only reason why the distinction was made, to keep it clear for the planning of \"Okay, he needs to do this\" and also visible for himself, because he can also see it, okay, but in my opinion, this doesn\\'t necessarily have to stay like this.  C: You just need to know that there are several options available. So either we don\\'t do it on site, yes, or the customer brings it, or we pick it up from the customer, and we either do it ourselves or have it done, which actually gives a bit of an idea of the possibilities.\\nA: Even central, actually not even self, it\\'s just directly to him, so, and that\\'s not really a problem in principle, but it would be nice if that was all centralized. B: Well, you see, We picked up on that, because that also costs money for such a youngster. He\\'ll drive for it, and as soon as everything is transparent in one file, then you know Okay, we have costs there.  C: We sometimes have, but now he purely goes based on orders that he sees on WhatsApp. So he\\'ll get a WhatsApp message from, say, Youssef, who says to pick this up. And while he\\'s driving, there might be an additional task to do.\\nA: But then where should you go to? I\\'m not very cold by the way, but I uh I have I just caught a cold. So, thanks for I think yes, so do you have children? B: Ah okay, well, how old are they? Ten Oh, then you are already over that, it\\'s fine. They don\\'t have anything, but with my wife and I, I think we\\'ve been sick for like three weeks or so and I\\'m still coughing after the second week. It doesn\\'t make any sense, but okay, so we actually have to and then, incoming that the customer does that, right? C: So, he actually arranges everything himself. We just have to pass something on to you, I assume when you arrive or come downstairs, he should, so when does The And who is coming to drop off? Because where we are now and that might be different in the future, but I think it\\'s best to stick to that, I would prefer to just have the name of the driver and the license plate of the car. We might need that if imagine we are going to belsim.\\nA: Wherever you don\\'t need that anymore, but it is useful to know for other purposes, because ultimately you also have to check who could bring the cargo and it is nice if each division is done fancy and you know, Okay, you are familiar. You are the same person. It\\'s all okay. B: Yes, that has to do with the safety aspect. We have to take a big swing at that, because we are not doing anything with it now. That is also obligatory, the security aspect. Wise and whether you really achieve security by making a security. C: Okay, okay. Well, the 2nd and the 3rd are then incoming, but yes, you are dealing with someone and right, but to pick that up. The customer does that, I assume. The customer then indicates who or what to do. Yes, the customer sometimes has their own cars driving or they hire someone, or it can be anything, so it is simply used by someone else. Shall I refer to it as something we have no influence over. No, no, and also no costs on our part or we have it picked up by a transporter.\\nA: Or we let it be picked up by our own guys, okay okay B: Okay okay and and and and eh it might not be possible with our own guys if for example the products are large or if it\\'s too far away, for example. I don\\'t feel like sending anyone to the tripoint border for a console and a car ten times cheaper there is just proof speaking, I don\\'t know 17 is to pick up something small from Maastricht, but if we don\\'t travel ourselves for a 4-hour journey, I won\\'t insure a few tens. C: No no no, because okay and and and and and and and okay okay okay okay nice, then that\\'s clear right. And that is three horses for number 3 and then she comes in once. What happens then? A: Yes, as soon as it\\'s in and there, yes that\\'s okay according to the rules. So you have different horses you can promise, but in general a load is received, so what you should do is check it.\\nA: That which you have received is also what you should receive. Yes, um, checking on ramping up and completeness and all that, and you give it a place and the file comes in, so that can be worked on and it needs to be assigned to a colleague who will then make sure it is complete, but with incoming stuff, it is checked based on the information. Yes, of course, not looking at it as if you have ordered two pallets of perfume and you receive two pallets of explosives, for example. It\\'s nice if you are relieved from having to deal with that and the complete thing at the right time, because yeah, that\\'s an ideal situation. It\\'s also about minimizing damage, because at the moment if you have evidence showing—well, such a fat, something like that colored packaging. If someone trips over something like that, it\\'s good if you already make an assumption about it, so you can sign that off on the CMR and so you are covered. Otherwise, if you don\\'t do that and you say. B: <translated text> C: <translated text>\\nA: Next day before Never van Hey damaged. That\\'s really a Yes that\\'s nice, but you didn\\'t sign anything so that\\'s your responsibility. B: It happened at your end, so that is then done in the room, very important at that moment. C: Then we must take photos, so, So that\\'s something that is happening now, but a bit on WhatsApp you know, so don\\'t want it centralized, so photos of the cargo as it arrives BVV does it very well for example.\\nA: Hey Everyone knows this is in. So they didn\\'t have to type or anything or sometimes it\\'s really a photo of well you can see it here and there I\\'ll put it there. B: Yes yes yes yes yes, but there\\'s not really a Line and I should just have a presumption procedure and there should be dimensions weights maybe not less size, but that requires a photo. C: For example, I do think that in any case you should believe to check for damage around it. That\\'s a requirement. Yes, that needs to be done. Yes. Yes, it was renting a car. I would say yes, it\\'s just signing off Okay yes, I received it like that.  A: Yes, but for the current situation, if I understand you correctly, now more emphasis is placed on taking a photo instead of typing, right? B: That\\'s like, the cargo is gaining and then it is indeed what I\\'m saying, my establishment Okay, it\\'s Lotte, then at the moment when they ask to enter, and now if you started with him a bit, but it is really important to mark that cargo with your eh eh eh with your eh reviens eh, because\\nA: Although it is of course that you then imagine it is stored centrally. It is nice that you can actually see lenses as well of Okay this is a reference he said you at 20023 where that blue one is this and and it exists so much Collie just basic information is nice. Imagine that eh due to storage reasons. You may have to move along with the relationship and with dangerous substances et cetera it can be that you might have to slide along a bit Well it is nice that you can see then. This is the cargo Okay and you can make it as fancy as you want with a QR code and yeah, of that indeed yes, what you say and that would of course be super nice, but yeah, those labels are unaffordable, so I actually don\\'t even consider using them I find label prices quite expensive. B: Yes indeed, so it\\'s like a 15th of April label or something. I print them nowadays, you know. C: No, but there\\'s an eh, yeah, there\\'s one attached to Dennis and yes, but you can print that one nowadays. Yes, good that it.\\nA: Costs a bit as an investment, but much bigger for large transport companies. That printer is won there. B: Yes, exactly, but that\\'s not us. C: That is, I would like it if you could simply indeed geolocate almost, you know, with an eh, because that gets scanned and then you immediately get a notification, you know. A: Well, I\\'m quite happy with it if you have something with a scan. This does indeed say, well, this Collie is at this location. I find that already good. B: That would be very nice if you could automate it in that way with a barcode or QR code or something, that is already quite a whole, yes. C: Yes, if you have a system behind that, but of course we don\\'t have that now. A: No, but that is what you see when indeed entering, right, that a reference is created based on a QR code. That would already be a whole, and why and and that is. B: Yes, maybe it\\'s a bit of a side step in this unfortunately in the direction of this story that I have to make a lot of side steps. No problem, used to it gladly. I like to hear them. C: Yes, but what is very important for us is that there is an automated man?\\nA: Comes to make a dangerous substances journal, and what is a dangerous substance? If you maybe reveal the name already a bit is that you can just make a print-out on Any Give a moment of Okay this is on this location and then go into detail, right. B: So you the number, proper shipping take the whole bunch with net liters kilos yes and especially Just yes, what is it the notebooks two three? You just have to make a print-out of this training the last. It must be from the fire department and and that is now is that just everything by hand a bit clicking and that just doesn\\'t happen. C: Yes, if you have all the data correct already a lot. You already have a lot of calls and then you have to Yes check it here you already have information and here you check and then you make a qr-code of it and then you have registration. Yes, and if you know it again for your administration system. Yes yes, I have a dozen two possible there.\\nA: There are several options, but there are some programs that basically have the ability to retrieve that kind of information, which indeed are masters in that area. You all have looked into that as well. I think 9 Plus, for example, is one of those programs. B: Yes, that\\'s right. That is a partner that I would just call it, because we have been working with them from the start. Okay, but they are ridiculously expensive. Really not normal at all. And even though it was known from the beginning that they were so expensive, it\\'s always shocking when those invoices finally come. C: I am also looking into a similar system. Yes, but I know that, and I have also mentioned it with the photos that were discussed and such. We must not underestimate how much work that actually is. Yes, yes. And there are also other solutions that you all should look into. For example, Jata, that company also organizes the matches for your Transporter of hazardous materials by air.\\nA: Have a date for all of that with all the, uh, such information is just there, you can just sign subscriptions and, because we\\'re looking at the towels now, you know those pills they said, uh that is, that is, yeah, I don\\'t have anything here, but no, but yeah, you know what I mean, obviously.  B: Yeah, yeah, but that\\'s way too much information. We don\\'t need all of that, but where it\\'s free are all hours numbers all proper Shipping on behalf of dangerous was 70 that you need, but the point is that all of that needs to be kept track of, because everything changes and, and that\\'s a crappy job if you purely rely on, well, accessing the database only once, you know, where everything is accessible from? Everything is copying. I understand and it\\'s not his, right? It\\'s not his stuff, because it\\'s actually just one set of numbers that are available to everyone, but if you have to keep track of that and accuracy is really super important, who the hell is going to keep track of that. Then you have to go through all those books, so you also often check how it changes, right? C: Yes yes yes yes yes, true, what year, L2 years so ADM and img change every year. So the l\\nA: Yes, does that change anything? Look, we basically have models that do that, right. So they really look at those two thick books and see okay these are the changes. Yes, we can basically do that, but I think you\\'re right. It\\'s probably very valuable that we get it from the source. I would personally do that, because ultimately I don\\'t think it\\'s for us. Yes, it\\'s nice and it\\'s of course very fun. F*ck you somewhere, but that all came a bit out of the emotion of the height of the bills, but they were just known to us, so it\\'s a bit silly to be very shocked by that. I\\'m also shocked by it.    B: Yes, after that all of a sudden such a, such a. Yes, not to maintain and, a full score The Earth reaction of Well, we\\'ll just go for ourselves of course maybe it\\'s a bit exaggerated, but yeah, Anyway I can send you that. It\\'s, I, that is, I find that quite yes, and data Solutions are included.   C: Yes, because what you say, I can just\\nA: Grab it B: Grab it C: Grab it\\nA: Look, you\\'re doing this, right? Look at this, then you are working better if you can catch me now. I am significantly less. And yes, I hear it was really quickly taken out. I will immediately go for these things, you know? Then you have a new one every time. I will be out of here soon, But you definitely have it here. This one is from 24, you know?  B: Yes.  C: Yes, this is IKEA, that\\'s a different story, that\\'s the book number so much IKEA where we work there completely differently. Okay, okay okay, We work with theater and Java is, let\\'s say, everything that stands in your IQ, they also don\\'t like it Only yes, then they throw in a few more things.\\nA: What did you say? Yes, because he is the book we are working with and IKEA is the law and the year is the guideline that all Airlines work with, so do we.   B: Okay, so IKEA. Oh, I would be surprised if maybe more than two people at our company have ever read the book, because I simply don\\'t use it. That iata is actually a nuisance.   C: Yes, indeed. Okay, so we are looking into that, right? What can we do with that information, indeed, and the responsibility it entails?   A: Okay, but oh yes, that\\'s what I wanted to ask about, the information about the numbers, for example. Is that already being noted in the information provision? Because currently we don\\'t note anything at all. What we do is we receive an email, create a folder, and drag that email and the relevant attachments into it. Just everything in there. \\nA: In the folder Yes so that uh so that it is certified and if later some more comes up then we drag that along. Can we drag documents along with it. You know that is that is quite nice and that was Central not very sustainable, because you lose a folder. So you are yes, but if you do, you just miss money and and that is happening now I think. It\\'s happening now 100% Yes it is this is just this is just this is just terribly dangerous and then we really need to get rid of it as soon as possible and um. We have taken steps in that regard. I had a uh a thing with uh xperity then we got quite far in that and then based on their brake system, which they have, dynamics. Uh yes, then they did indeed on the Dynamics and then their own exploit form so to say, they built something for us, but yes, I just I just can\\'t figure out what to do with it at that moment I thought that uh I thought that didn\\'t go smoothly for us and uh what was not going so smooth Well for example, the fact that for example and that\\'s why you take the side steps on uh is that uh Imagine that we have to carry out a task purely. B: <translated text> C: <translated text>\\nA: A foundation of documentation so you receive a, you have the basic is a file. Yes and that file. You can also have multiple assignments for example if we do a check for a customer. That can be with us in wood, but it can also be somewhere else, right. But then we just Sometimes that customer just wants to be invoiced once and there are 10 checks in there. Well then you need 10 folders with the individual documentation where you can also drop the photos of that shipment and everything and also the communication from that are multiple emails, but they want it in one. So you have because one invoice that system does not allow that. No B: Yes that was indeed very difficult and what I\\'m so afraid of and that is yes, that in my entire career I encounter that in all sorts of different ways, if you don\\'t make it super easy for someone to do something, then it just doesn\\'t happen from people are just those fascinating elephant horses that I find beautiful in principle. C: I do not know if you ever\\nA: Yes, maybe, I think you should still... How old are you actually? I am 34. B: Yes, I think you just didn\\'t quite... Sloterdijk Station in Amsterdam. Can you see that building of the tax office? In the past, with the age, there used to be a very large lawn there. So, people who went to the office didn\\'t walk around it but walked straight across. C: Yes, there is a worn elephant path there from all those, from all those civil servants. Well, I find that fascinating to look at. I find it very nice, but you see that with people too, at the moment they have to make too much effort to keep a list. They eventually stop doing it and that... Those are such important things for us, because yeah, it\\'s just a standalone. It\\'s mandatory to do that, and we don\\'t do it, and you can\\'t even get them in, because it\\'s just too much work nowadays and too much is asked of them, so they are logically driven to the things that are criticizing at that moment in their perception. A: Prioritize, right? B: Yeah, yeah, of course. You have to.\\nA: Finish the assignment, but what you should do is, for example, if we\\'re talking and we have a task. You do it because it will probably come. You pick something up and it\\'s in this packaging, and it needs to go in this packaging. Ultimately, are you left with this packaging empty? About that, it\\'s just waste. A procedure with hazardous substances has been carried out by someone and it\\'s not being recorded. What if, for instance, you pour 10 barrels for me and in 20 years, you get cancer. Then it\\'s really nice to know what you\\'ve done for us and where that could potentially lead. Yes, all important things and we\\'re not doing that now, and I find that very unfortunate, these little things, and it\\'s really not, I don\\'t blame them, but that just doesn\\'t happen because the registration obligation lies with the person. They have to pour it, right? B: Yes, but we have a duty of care as an employer. C: Yes, I understand that. I understand that, but the responsibility lies with them, and they have limited hours to carry out that pouring.\\nA: Ah yes, we are doing that B: Yes that\\'s true, But it is just the administration that is currently very heavy. C: That is a lot of work to administrate. It could all be done if you have 100 years, then you can do everything. Of course, it\\'s not in one.\\nA: Isn\\'t this not appreciated enough. Just say how important it is. Yes yes, because it\\'s working now. B: Yes, I was thinking about that. If something works, yeah, then um yeah, we had at one point something with for example. C: I had to think about it this morning. Coincidentally. Remember all the hassle with that server or whatever it was. I\\'ve already forgotten what it\\'s called where we host our website and then you have to put it in your hosting. But, v-max. That was so important back then. Everyone relies on it, but it\\'s working fine now. So, let it go. But if the website goes down, everyone panics. Yes, yes. Yes, but now we\\'re moving on to other things and soon it happens again, and nothing really happens. Yes. Yes. Yes, but this is important, because this is how we earn some money. And the one here is just leaving money on the table. It takes a lot of time and doesn\\'t meet our requirements. I think you should this if you  \\nA: It\\'s good that you realize that it\\'s not just about the money, but indeed, as you say, about Safety and, uh yes, you confirm a lot of things. It is this.  B: Yes, but it\\'s everything, right. Just name it. Yes, it\\'s the time you save and that is.  C: Yes, you could even have the customer do the work if you, say, put levels 1 and 2 in a portal for them to do on their own. Well, this can. We bring soft and if it doesn\\'t, maybe someone can check for us then.  A: Yes, I have to admit, I was always a bit skeptical because it requires some experience, but if I assume that an AI can do that and it could very well be, then that\\'s great. It means not having to reply to an email.  B: Wonderful, the customer has influence and receives a number right away instead of waiting for 2 hours, trying to get hold of someone in reality. We are now. We are now also working on, on, on, and you Aachen.\\nA: So also actively generates emails based on the input, so if you miss things automatically all, you know, sends a reference of, hey, for this and this assignment we are missing these things. Please fill these in, or you don\\'t have to do it anymore, because everything is outsourced for you. So we try to look at every aspect. B: Okay. Where can we indeed make an efficient process next to the Automation process, and that latter especially, that, yeah, goes hand in hand with Automation in the Vision certainly is the intention for sure but that piece of convenience. That should also be included, because hey, not that you suddenly struggle over a structure, then I know that without convenience it is difficult for the user to have convenience and also that it goes a bit quickly, because yes, at the CS we had at one point and the new system and then I also helped a bit in a sort of comparison setup. C: Yes, at some point when you click from step to step there were a few seconds in between, buddy sounds hahaha yes yes yes and on the one hand, I understand all that. You know, you shouldn\\'t aim too high but damn.\\nA: Frustrating, yes, it drives you completely crazy. B: Yes, those risks, counting at the goal, and then you can continue for a minute, but well, it\\'s a weird situation, right? C: That\\'s right, what I\\'m saying is that we are not doing it well now, and we can\\'t do it well.\\nA: Uh said step as we then Indeed look at the interior points, right So that is what comes in. Um, is now already how I then a photo made registered. There is still No ref code is created from the Marco qr code you have. Here you already have at the moment the quote is accepted and is about, then a number is immediately made to assist with Okay, Okay, then you will enter it and then you have a number okay and the file number. That you use to communicate with the carriers. That you use to communicate with our own advantage that everyone uses that central. Yes yes. Yes. Yes, that\\'s the only one, at least on one of the yellow things, but we do that well, I understand you this is central and it must really stay. Yes. Yes. B: And when it is in, then that cargo must be marked with that refi right. Yes, with the physical. Yes yes yes yes yes yes yes, so you know indeed well, this this is an n. C: That\\'s it exactly.\\nA: By the way, so it must all indeed come in dossier and then and then it is ready then it will be saved. I assume or is there a whole system where it is saved or is it just uh but that is uh that there No system for that yet, but that\\'s also because BVV is there eh, the location is too limited for Maarten if they go back to Bellsingel, then I have a whole plan for that to say well, this is allowed there, this is not allowed. You know? We have designated spots for that, okay. B: And then is that communicated with you or is it then or do you give the order? It needs to be stored there now, eh or in the Netherlands you drive around with that right so we when it is unloaded we then accept that load and then we set it down somewhere, but wait. Wait a second, So it\\'s the person who makes the first link, doesn\\'t drive it in, right? Eh What is this that you the customer eh that you the C: Sorry, can you repeat that?\\nA: The order has been confirmed. But that\\'s not the people in the administration, right? They don\\'t usually come in, do they? Sometimes they do, by the way, but now that\\'s because we\\'re a bit short-staffed. B: Basically, what we need is a Warehouse manager or more Warehouse coordinators, or whatever, and people who are there and help out with the forklift and such, right? C: Yes, we have a bit of that now, they are the ones who basically handle receiving the freight, but they have little to no insight into the dossier.\\nA: So you just throw that into the Hey group, there\\'s a driver uh we that driver who also often calls, so when they hear that, and then we say for example to eh Younes, well, now it says the duration is 16, there is an angry one. They will charge this and this, and that\\'s this and this customer, but that\\'s the only thing the person who actually closes the query does, just to sign them off and throw them away.  B: Yes, but actually that person should go through the entire acceptance process, so they need to ensure that the request is reported correctly in our system, so that you can also see in the system that Ok, it was reported on this date and maybe at this time by this driver. C: That would be nice, as you say. That\\'s a whole report. Then you make it and it just becomes. Yes, they just need to press the button to speak. It\\'s just a timestamp. You know, when it happened is not too difficult, but just a simple field.\\nA: Uh and and marking, because if I understand it now, you want to know where it is stored now inside. Then you would have to call someone in the warehouse first, right? B: We\\'ll take a look in a moment. Yes, because we don\\'t have that many requests the same in general. Especially if we take some pictures and find it and the customers are known and we have an idea of what we are looking for. C: Yes, but whether it needs to be communicated in general, and at the moment the Warehouse manager or the one handling the freight inbox, can do all that autonomously. So indeed, he can take a round and give a location. Then it\\'s no longer necessary, because you simply have a location.\\nA: Eh and that is okay, these are really exotic, by the way, but proof is we will discuss something in about a month. So by then there should be just storage preparation costs, because being angry is not free, how is it going now? B: Well, actually we will only start charging costs when someone requests it. C: Yes, so if he asks to prepare storage, he will get it, and then we will start. And I have tried to anticipate it a bit by setting a deadline and a formula that calculates the difference between those two dates - 2 days. Then they will receive a cost based on weight, but they need to fill in the dates and weight dimensions, and then a standard price will be calculated. We have taken a first small step, but it needs to be done again.\\nA: Uh, it\\'s going to be sometimes just oh uh no, what is it basically what we do and that is so uh step uh unsubscribe say six seven then actually Dia that is uh that is uh then we unsubscribe from No, we\\'re done We are done. You can come. Yes, or we are done. This is the result and I already hear what we need to do with it, because it is also sometimes the case that we really have to deliver it.   B: Ok, about having to deliver as they say. Yes, or the customer can come pick it up, but at least they know Ok then it\\'s done and then the storage starts to run. Because of course we\\'re not going to charge a week of storage because we need a week to finish it. That would be very unfair.   C: Yes yes yes, ok, uh, are there any other matters. That need to be done inside that you say. Ok, he can go to the field service. Yes, that\\'s not uh the outside a bit the kind of certificate how I like it. Well no, the field service is uh when something is inside then uh is it yes, then it sounds very logical, but this.\\nA: So you have, so then something, at the moment we do something. The Out-of-Office does something different, really only things on location at customers.   B: Okay, and that is part of that. It\\'s a completely different trajectory you could say, so it is indeed a separate.   C: Yes, yes exactly, more like from here you can definitely go outside.   A: So you have not inside yet and you have outside.   B: Ah okay okay. That\\'s how you should see it I think.   C: Exactly, it is not a follow-up, but indeed an alternative trajectory.   A: Yes, exactly and that is then with the customer.   B: I assume if you say with the customer or at a location that the customer indicates for example.   C: The customers also have tasks to do and okay, but that is then again a different trajectory, but that is what you will do then on location somewhere else.   A: Okay, so it\\'s a completely different track you could say.   B: And is that, does that then again depend on the reference number become the whole structure of the file.\\nA: The same, and it could also be that we have to pack something there or check a shipment or stick a label there or whatever. So the setup is the same and the approach to it, because the customer requests something, we ask for complete information to see if they want to proceed further. If they don\\'t want a quote, then we proceed to execution. Only then it\\'s just, it\\'ll happen somewhere else.  B: Yes, okay, but indeed the steps that are done here at the entrance are actually done elsewhere, in principle. Is that correct if I understand you correctly? Well, what I see upon arrival is that it\\'s really the acceptance and intake of freight, then is it just that a box needs to be packed or a label needs to be set or does the freight also need to be delivered? It\\'s purely executing and there is no transaction taking place further. We don\\'t deal with freight. We do take photos. C: Exactly, it\\'s all about following the same process and ensuring everything is in order before proceeding to the next steps. We focus on the execution part rather than handling freight directly.\\nA: Maybe before but certainly after yes, that you, for example, decorated is or that it is packaged, so that we have the end result, because we work with the principle of four eyes. That is to say that also. And therefore a photo is nice in itself and the exterior is mainly about the name so, if I check your documentation, well, you were busy on location and everyone knew you did a fantastic job. However, you made a mistake, and I should be able to check it remotely before you leave actually. B: That\\'s why it\\'s now done again on WhatsApp, here are the photos, please. They often also remain there while actually, everything should be Okay what needs to happen or what should actually be, because how do you communicate to the customer then? Because I assume that then, yes, that\\'s the one sitting in the office often while, and that\\'s a shame because as I once envisioned, everyone gets the tablet and is truly on location. Okay, I have been here. These are the costs. The right photo Bobby runkle auton. C: Alright, I understand now the importance of ensuring accuracy and efficient communication with the client. Thank you for clarifying the process.\\nA: Right, now it\\'s like this, they just take a photo and that guy leaves, and often it\\'s just one person at the office who has to do everything, so they have to adjust the costs. They also need to... and they still need to handle a Jackie. They have to email the client, while also receiving all kinds of emails and calls from clients, so that guy is going crazy. B: I have also invited that guy, but you say you must hold his hand a little bit, and that\\'s true. You receive additional instructions with a lot of information and you have to adjust and manage accordingly. For example, if you arrive and the information you were given is incorrect, and thus the documentation you prepared must be changed, that is often asked for, but it\\'s just a matter of fact. C: You just have a computer with you, you should just do it yourself, because you are there. You must create an end result and that end result must be verified, and that\\'s the only thing that person does. You are a dossier owner.\\nA: Yes and the one who is in the field service is the owner and has to make sure everything is completely ready and the only thing a third party has to do is to check their work. B: That\\'s the ideal situation but it doesn\\'t always work out because that guy is also extremely busy. C: Yes, I understand, but you might be able to mitigate that by simply lightening his administrative burden. A: Don\\'t make someone else do the work because they are also not busy. That\\'s an important thing, because it is also our obligation to check the freight.  B: We have to make sure that mistakes are minimized, as a skipper, as it could either be very expensive or even dangerous. C: Yes, and then if needed, just allocate around 700 euros to fix it properly because the communication lines are not functioning smoothly. Everyone is doing their best with what they have. A: Yes, with the resources they have of course, it\\'s essential to follow a checklist for a proper check.\\nA: And that checklist is actually now only eh yes, you have to fill it in separately again, and that\\'s undoable. Yes. B: Yes, but is all of that really just very cumbersome and and and and and and I unfortunately don\\'t have one lying around, but those are just questions that you have to answer and, ideally, if you have that in a tablet, then it\\'s just ticking off, you know, adding photos, so it\\'s easier. Yes, as I recall, just before I left ECS, we already completed most of that and finished it up. C: Okay, so what we had at one point was that there, on the screen, you have like a a two-part thing where you have documentation here and then you\\'re simply asked questions as the user. They just ask you a question. Yes, that\\'s a Yes, I don\\'t know, is the box re-inspected. Well, you check the certification mark, and that\\'s how simple it is. So you have a list and you can just work through it without really reading it. You just have to answer questions and take a quick photo.\\nA: Confirmation and then you have the perfect check. Yes, but it\\'s not happening yet, okay, because that\\'s already included, right? Okay, understood. B: If you look at the pending, I assume it goes beyond, right. So both internally and externally, is that the same process in the pending is at the moment when we can\\'t go further, so we gathered information. The cargo is in or the colleagues have arrived at the location, right? These are the two tracks. C: And we come across something, or there is damage, or a situation is encountered that is different than expected. We keep coming across obstacles, right. We can\\'t proceed, or we find that it becomes much more expensive. Then we ask for cost estimates, until then it just stays on hold and that\\'s what pending is. Okay, but this is also not necessarily a standard route, is it? No, for that reason, it\\'s purely at the moment when there\\'s just a.\\nA: Yes yes, you just can\\'t uh Clear Clear okay, Yes okay and and and and and and um and depending on it is, so to say. I\\'ll call it an incident, right, because yes, that\\'s exactly it. Yes. And and then and then when that incident is resolved, then it goes back. The Flow just right. Let\\'s continue now. B: Okay, okay, are there specific incidents that occur very regularly or is it really anything uh taking into account information the most dangerous the most common For example, we found a product where we did not receive the MS could not receive the rest and let\\'s see that enables us to identify the product uh what dishwasher is that. Is that already here upon arrival? C: No, that is at the moment when you are really treating it, so at the moment when it enters, then only a general acceptance of the cargo is done. We have, as I said, take a walk around. Take a picture and assign a place and then it is detected in msdm is missing.\\nA: No no, then at a certain point it is received, so it is stored in the warehouse and then it is picked up by someone. Then, it gets an owner, right? B: For example, I go. I will receive it there. Then I will check. I will check the information I have received, because it is new to me. So, I will look at the packing list and the MSDS. I will inspect the cargo and I see that I have a two-component thing here, but I have only received an SDS for one component. Material Safety Data Sheet for 17. It states a month and the other part, I should have that as well. I might need to do some more research on the internet. But basically, we assume that the customer must provide that, right? C: Yes, you think the customer contract does not take place again. It is often not centrally registered in the file. It seems that hope is being done, though. It will just be emailed or called. But it seems that only remains in the email, not in the file.\\nA: Actually already at step 1 and then we can arrange and then, yes, if he as the Skippers had processed it then it wouldn\\'t have been necessary.  B: Indeed. What happens sometimes?  C: Yes yes yes yes yes, but then you could request information indeed, right. So that email that you then process and then and then it goes out and pending it, because it just comes to one and he becomes a sort of Frank, you just have to make a Look.  A: Yes, like you did above with me to talk about checking information, not completely certain. It could also just be that the damage is for example, because that is my question. So what, besides developing information, what else and then I assume the damage and that he notes what or a dent as you said, right.  B: And then if there is damage for example, then a document must be signed by the customer in which he and I have written those documents at some point.  C: That would have been a couple of times where he gives us permission to pour that into knowledge that impurities might be introduced. That will be exposed to oxygen and.\\nA: I hear. It gives for the type of packaging we are going to use, so that he knows all that, so that we are just free from it, yeah you used a standing drum and now a warehouse has exploded, you know what to do, but just a sideways move, it\\'s nice if it gives permission as the standard so we don\\'t have to come up with it ourselves. B: Yes, yes, all important, but I assume that is also a process on its own, so that\\'s more.  C: Yes but that\\'s another thing again. Ideally, you should just do that in one system where, for example, when you press the button saying \"I want to create a new document\", it should automatically have a date, a name, a product it\\'s related to, and all of that should be done from the document and the permission should also stay in the document.\\nA: It takes him a month to finish it, that\\'s on our end, and we will only start taxing when it\\'s unregistered.   B: Yes, and that\\'s like, we didn\\'t sign up for that, of course, but these are all those things that are not being captured at the moment.   C: Right, it\\'s kind of getting out of hand. It requires attention again, and now it\\'s as if someone is left hanging and there is no exact reminder.\\nA: A correction Sorry I want to forget that.   B: Yes okay, but then it will proceed. Will the costs also be a bit better or higher?   C: Yes, and then you have to communicate if it needs adjusting and then it can be obtained.   A: Yes, communicate back to the customer, indeed.   B: Yes, those are very important things, and because that\\'s something that doesn\\'t happen now. We hardly ever provide cost overviews ourselves.   C: Okay because we just have an excel sheet and our purchases and sales are recorded in it.   A: Well, I hope that goes smoothly and of course nothing goes wrong, so once I made a separate tab for that but yes, it\\'s all so clumsy and then you have to cut and paste and it looks messy in the email.   B: Yes, I find it very very bad because sometimes we do ask for cost overviews from each other. You have to type it all out and check it thoroughly, and then I already had a price in mind and then we thought of something else again and it\\'s all so um, it can all be much better.   C: Yes yes yes yes yes, we don\\'t have much, we just have all card prices.\\nA: Okay, so basically I just, um, I do 1 DVD. I put so many, um, labels on it, one on pikkie and we have, you know, where we put the fucking Street around it or something. And that is your overview and then you should just be able to send it when unsubscribing or updating a quote or when passing on a quote. Yes, that should be much more professional if I understand correctly for such a cart, right now you have it in Excel or we just have Excel. I can send it to you if you like and, um, in there are just the products we have and what it costs and what, um, what round costs and what it costs them, you know, yeah, and then with a storage and they all have little bags and this is for example for Gieten and this is for example, uh, well that is then, uh, Transport is also mentioned based on our fixed partners or actually on BPV actually yes, but we can\\'t correct communicate that in a decent way. B: <translated text> C: <translated text>\\nA: This is mine, because then you have to cut and paste and all that. Yes yes. B: Yes, and typing it out and it just takes another five minutes to do that properly during busy times. C: One press of the button, you have to be on time, you know? Yes, okay. And yes, maybe in the end we\\'re still not there, but ultimately it should also ensure that our invoicing becomes easier, because right now it\\'s a hassle with all that dragging over OneDrive. And ending up in the right place with Nathalie where the ventilation is exact, right? So all that is done manually, so it\\'s taken out of Excel and yes, if you start now, he then says just wait until but that but that those invoices must therefore be made manually. I am and sometimes I happen to find out. Yes, for example, that an invoice doesn\\'t match at all. Yeah, people are working, right.\\nA: To work and you just have to spare her that, because it is really unnecessary. Well Oeh Nou, okay okay, but it is so Okay good that we have come to this, right? B: Is that part of the cancellation, then, that it goes to Nathalie for the cancellations in principle, that it goes to the customer so the customer C: Okay I need to know okay, my cargo is ready. Very important, what are the dimensions and weights now? It could easily be that something that was one pallet has become 5 pallets.\\nA: And then you have to be requested again. That\\'s an extra step and the product owner should just do that, or the one who packaged it, at least she should take care of it. That should become a standard and then they need to be helped through a reminder or hmm. It\\'s just yeah, there\\'s a lot in it. This is so important and Our customers want it too, because an expensive customer survey was done at scs just before the moves and that came out at number 1 by far. We just want to know what the new dimensions and weight of our cargo are, so super logical but it\\'s extra work. B: Yes, and sometimes it doesn\\'t even have to be much work, but it\\'s still extra. It just has to be done. Yes. C: Yes, and yes, it\\'s a nice server and and uh but good dimensions weights new documentation and uh final costs. That\\'s basically what you pass on. Okay what do you mean by documentation, so um, so that\\'s where ah yes, we have a DVD made up for example something has to fly and then you\\'re supposed to create a Stack relation in this area.\\nA: And he needs that again to set up his booking process and go. Yes yes yes yes yes yes, and ultimately the cancellation is just from Okay we are all set and we have done this for you.   B: Right right, and then and then the costs indeed. Yes yes yes yes, what are we talking about? Costs? Yes yes yes yes yes yes, and of course the settlement.   C: Yes, and then, uh, a notification goes to an account. I assume so they then handle the invoicing the accounting. Okay this is a cancellation.   A: Yes, there is a question after at our end. Okay, Okay so then it uh needs to be temporarily stored, so then you get that uh in the area for example until further notice.   B: Oh of course it must be delivered or uh then I must come pick it up. Yes, that\\'s what we provide. That cancellation is also a ref, so that\\'s clear.   C: Yes, and uh So something else needs to be done it needs to be finalized and and it is up to the customer how they want that part sometimes is known in.\\nA: Sometimes yes and sometimes we just have to wait, so it\\'s either/or indeed. You have information about what the customer wants and if not, then it\\'s communicating with the customer. B: Yes, it\\'s just a cancellation and from that cancellation it should just be clear that Okay, it\\'s done. We look forward to hearing from you. C: Yes, and what would, um, what would the customer have to provide in terms of what needs to be done next, I assume, but if we simplify things. Well, in terms of what needs to happen with aviation, ultimately, it goes up into the air and is delivered to the airline. A: Okay, okay, and we do that ourselves or we have it done by, well, a utility knife, or others do it, because sometimes you can wait for 3 hours before getting help and particularly on Fridays. Today is the export day with people waiting up to 3 a.m.  B: Yeah, it\\'s a challenge, so that goes, yes, but it\\'s really crazy, and it\\'s getting worse, so you just have to have a console truck do it.\\nA: One person waiting for 3 hours with 100 shipments is better than my round of 3 hours tomorrow morning.  B: Yes, yes, then you actually see Bullet, well, what you want, right? C: Yes, and then a Transport task is given, namely one to deliver. There are stores involved. And that needs to be updated again and that\\'s right, yes, and that can sometimes be left behind. A: Okay, so we have, for example, screened a shipment. Everything must be meticulously documented in the file, because otherwise, for example, you might need water again. B: Oh yes, shit, I forgot to ask if you can make it secure again. C: Yes, okay, and then we do that, but sometimes those words just don\\'t get updated. That, I get that invoice in. I am not the fastest and we pass on the Kruit, but it may just be that you have to come up with just normal water for the customers. A: Yes, well, sorry. We received €40 in waiting costs because we stood there for so long. Can I now call back and then he says something like yes, this is nickel. I have burdened my customers, because I am just doing my job, and those costs are also for you, there you go.\\nA: Yes, sometimes I say it doesn\\'t happen often, only it is so easy to prevent. Yes. B: Yes, and all just by having a tight process from A to Z yes yes yes, okay, okay. C: And then after that, if this is over, then we talk about invoicing arrangement. When the freight is with us and um and and actually placed an order, so you have left outside, right. If the freight leaves us, then it is picked up and then a biodi is signed again, so you have a signed dressage. Okay, once the freight is gone and that question is gone, then you are really done and nothing more can happen, and then the file can be closed in principle. You bring it to the airline as well. Then there is an ACM receipt, right. Now you also get it signed, a beauty actually. As soon as that period is in your hands again, then the customer is ready again and has all the passwords and such and then you know precisely.\\nA: Then it is also for Vaart between to the port, or something like that. B: This is really like the holy Grail of logistics, because that\\'s where you really get rid of your responsibility, once you\\'re done and then it\\'s someone else\\'s delivery. C: Okay, and that must also be done. This is also an important document for us and is often not stored in the WC; it happens sometimes, but you see that someone is just very eh Duty, it\\'s too full or something in general, then we invoice, we speak, just done.\\nA: And it\\'s not a procedure of him a Pod is just the start screen for us now we can start invoicing. That\\'s how it should be and only then will we send invoices. We won\\'t send invoices. So if something is wrong, then Oh we knew in biology. Where is that? Okay, that is there, pof piiri going.  B: Before that, you just shouldn\\'t invoice, because then there can be evidence speaking something else happen.  C: Yes. Yes. And then it is so difficult again at the moment that you are in a software or in one. In the past, of course, we had physical files. And at one point that stack is a bit on that day of hey, all that needs to be invoiced, but that\\'s just very physically what\\'s happening.  A: But now you don\\'t have a sense of it and you don\\'t really have something like. Well, look, I don\\'t have such a good one in the stack that encourages me to get to work.  B: Yeah, yeah. So how do you do that?  C: Yes, nice ticket in, yes, that you just eh there. And then you get a reminder every time and if you get the 20 rinne, then you know that he still hasn\\'t been there for a while. Do you want to do it? Then you get to work there. Yes, that is something that is true.\\nA: Wherever I place my hope, is that boys are simply reminded of the fact and that could also just consciously be in the knowledge that nothing is forgotten and that they are not always left behind or invisible, but that they are not helped. That is important. B: Yes, exactly. And indeed, it is important that they do not completely lose track due to all those heroes. C: Okay. So, it\\'s about treating them, then, yeah, do you still check the invoicing or before, because you mentioned, \"That incurs additional costs or the actual incurred costs.\" I believe that there should be a check on invoicing, because of the principle of four eyes. But I think that is already happening, so there is just one person who makes the invoice, makes mistakes, and then... Because that is, you break that, is that then someone who is at the beginning? A: Ah, okay. The person who makes the invoice. We never actually see them working from home, because...\\nA: Yes, Wednesday okay okay. Yes, so we should actually have direct contact with them, right. So yes yes, but what I also find annoying for example is that we don\\'t have We can\\'t We have to look for those invoices in the mail in the sent items to find out where they are and then we\\'ll probably find them if there are no typos in the address and yes, that\\'s with the fame so that invoice I think we should actually ask some questions about. No, I thought this invoice, how does that work, yes, and then they come up with the invoice number. Yes, and maybe also a stamp. Yes, it took a while before our return even got on that invoice. Super important. And yes, but yeah, then we have to ask them like, yes, if you can put the invoice on the refs, yes and then I should just see that invoice. Then you can search based on that. Yes, we have billed you for this and that happened then and yes, so there is still room for improvement.  B: <translated text>  C: <translated text>\\nA: Yes, those are butcher sat what also well also well references. Do you actually. You have your own website. Yes, only those two. Well, and what you thus have, Because we work with exactly is. Our invoices also have yet, So you have an invoicing. Our invoices. They just have invoice numbers.  B: Yes, I would prefer seeing that since our reference is also just a unique number and that we that not nice that you just okay you can link, right. So that in itself and then I understand you. And you make it harder, but well, then comes the number by the number was only used to refer to the reference of something.  C: Yes, that\\'s correct, the reason why you might want to do that is because your partner partner uh horse invoicing uh yes, So you have a file with beer that storage and then a token goes out of the way. Yes, and that is invoiced and yes, going. I have to keep it in my bakeries and I have to register and then you just have a result for Yes so And then.\\nA: That\\'s okay so if you see it like that, because if you have such a degradé, that\\'s also its own ID number or everything. Okay, also our rest is on, and that\\'s when a customer is taking a trip, does he also send one if she can bring something or if you can pick me up or what Yes and of those two well for us it\\'s just our concentration that you do have and that\\'s good that you say by the way is that the customer looks well with our ref but for him that\\'s also a hurdle he has to overcome when we deliver a shipment then it also has an Airways image number and an airbag part number is built up from the first. Those digits are the airline that is going to fly it and then follows and yes. B: I really don\\'t know those numbers, but that\\'s also relevant information. C: Because again and they step in the moment that we have a sentence in\\nA: They have declared. She said he there omgi. That said it through Whatever Then we make a statement out of that and on that declaration of course our revje, because actually there should also be a very necessary number on it, because that is for the customer. That is very recognizable and suffering for the customer Yes, I of course have his own rap our billing reviens is actually his file number, but such an Airways will number is. Yes, that\\'s just that. Yes, that is just the number under which. It is known by the airline among others and often and sometimes you see that they even have an Aero will number for the Dreft and the customer is that it goes that far. Okay. Yes okay, then we have reached the end of the process. Yes, of this process yes yes yes yes yes yes, it is eh, look, in the field service we have now. Look then we have essentially established that a large part of it is the same Execute on storage story. Erhm, but you also have the shipment and that erhm  B: Yes, that was very detailed. It\\'s important to have all the necessary information on the statement for the customer. And it seems like we have everything in order for the process.  C: Absolutely, it\\'s crucial to have clarity and accuracy in these statements and processes. It ensures smooth transactions and good customer experience.\\nA: You are inside because we are in a shared building. Okay, for example, bdb is there, for example, from the dirty Global is there, for example, let\\'s call it a plum and that is what he asked we do not have and manage.  B: Okay, it stands in our warehouse and we need to give it the same treatment, but she has it for that. So, you do not have an input. Trajectory, because that stuff is only there, we do not know where it is, because we did not put it in there that she did it, so we have to search.  C: It is often in the same place and we kind of recognize it, but that is something that at that moment it should be and there should just be a completely new link, just taking a dossier and actually knowing exactly what to do. Then you just have to. That should not be that someone has already chopped with the hatchet a 100 times and know exactly how the moment the shipment looks like. That would of course be great, but the warehouse as we still work now, is simply a warehouse. They have their own tracking system in there.\\nA: That\\'s just for now. The reality in the future is, of course, no longer like this, because we just want to have our own shed and then you won\\'t have this hassle anymore. Then indeed you just have like this Yes and what and what, and you also just have someone who only asks for storage, so you completely bypass the whole Okay the whole process of doing things. Then it\\'s actually just about accepting a place. Yes, and then it should be there and maybe a part will be taken out or the whole thing will go out again. It can also Okay and what what what what then we skip so the Heerde check on do you mean on completeness or what then you still have to make a request. Information still needs to be provided, maybe a quote. So we\\'re actually doing the same. Yes, it still needs to be picked up or brought to us. B: <translated text> C: <translated text>\\nA: Acceptance check for the next always alone we don\\'t do anything further with it, so we actually go straight to this reporting story. Yes yes, There are no interim steps that don\\'t make sense. We go about yes, what well uh, What still needs to be done is that this is actually the final stage anyway you must then see what that is for us it is of course important from this dimension weights. These kinds of things are also in there. There is administration with impact again. Yes yes, only it it it it it it. It is of course shorter, right. Yes yes, it\\'s just less to do above, but ultimately. Yes, but for what do I have to think about? What kind of products are those then? Someone comes to bring a few pallets of paint for example. Okay Okay, but then we have to so we have to eh and that often happens when the information supply process the dossier entry is then a bit more work, because you have to indicate \"Well I have two pallets of you and 12.63 Class 3 PM 3. I\\'ll just give it\" B: <translated text> C: <translated text>\\nA: So then I indicate already, well, this is what we expect in terms of hazardous substances, so that it will be recorded and then it will be in our often substances journal. You can actually already prepare them, right?  B: Yes, yes, yes. Only then I have to prepare in advance, well, this is coming in, and if that doesn\\'t happen there, then it must happen here.  C: Yes, yes, so it will either be done in advance and it will only be checked if it is correct or it will not be done beforehand and then everyone can do it here once more, but I can already tell you. You don\\'t have. Are you going to the train station? No, no, no, no, no, no, but what happens in reality is that it also happens here.  A: Yes, but that shouldn\\'t be the intention, in fact, now it doesn\\'t happen at all. That\\'s why we need to store something at hand. Then it won\\'t be registered anywhere. Really? That\\'s Except in our sea, it\\'s just that packing list and everything but that journal. What\\'s also important? There was that exists and now it just doesn\\'t that you on WhatsApp yes really.  B: Yes, an attempt was made to have that environment via that way. He has Youssef made a list of it.\\nA: It\\'s just something I\\'ve made before as well.  B: Ok, but yeah, that\\'s Youssef again who just goes through things he shouldn\\'t be doing at all. C: Do you know duct tape? Yes, it\\'s indeed all patchwork, that\\'s true. And then he, we\\'ve spent a really long time talking about it to make a list and you would think, okay, let\\'s start keeping track of it now, but still. It just becomes outdated again because it\\'s just a matter of time, and often a very short time, because it all just goes in and out and it\\'s all handwork, and that\\'s where we just need time for, well, you\\'re not going to make it, no. No, you won\\'t make it through. That\\'s where it won\\'t last, and the good intentions are really there and everyone wants that. Yes, it just needs to be easier. It should just actually be automatic.  A: Yes, exactly, exactly.  B: Are there any other processes we should consider besides the ones you mentioned? Performing TG checks if I have mentioned that, so that\\'s actually also back to the customer.  C: Yes, um.\\nA: The Field Service can also check the Bible location, but then we often miss nothing. And yes, I\\'m trying to think in the situation where we receive cargo that only needs to be checked. B: Okay, what happens sometimes, so we have 30 checks that we receive from the customer eh this is my question. C: So, the cargo is now there, then we can check it there or pick it up and store it with us, check it here, then deliver it to the customer. If it\\'s okay or not okay, then this should change and sometimes you might say, okay, but I feel safe, but it happens that I do it on location regularly, and then we don\\'t do anything with it, except check it. The cargo based on that checklist, preferably. That\\'s all we do. Okay, and I assume there\\'s a briefing beforehand, right? So, yeah, that\\'s going to be a bit of a hassle and photos need to be taken of that again.\\nA: Yes, and that\\'s his thing, but it could easily be that there\\'s 10-20 cm sitting on a file when speaking about it, and a distinction needs to be made.  B: Well, this is shipment A. These documents belong here, this communication has taken place, and here is a photo of the cargo and the checklist, because that\\'s where I actually want to go.  C: I just want some sort of documentation or evidence of who didn\\'t check them based on size.  A: Yes, because now it\\'s like, \"oh, then Henning gets rejected\", and then we have to do a bit of Sherlock Holmes work, I think. Who checked that shipment in the space?  B: Yes, and then it\\'s often like, \"oh, his hair\". Just busy, busy, busy, only the hassle and the curse. It could even be based on photoshopped images, right? The boy from transportation. He drives past, takes photos, and someone checks it once. And then the colleague is gone again.  C: Yes, it\\'s all, well, understandable, once again, busy, few people.\\nA: Yes, but these are exactly the circumstances in which Suarez can simply pray that you can do much more work with fewer people. Yes, definitely yes, okay.  B: Furthermore, in the process, it may also happen that we only do transport, uh carry out where actually we forgive only a Transport or actually We do a service so whether that is transport or sending someone to, uh, speak proof to make a casing. Also, that is done via WhatsApp by the way, then you send to our carpenter.  C: Here are the details approximately and then he sends back. Okay, this will cost and then you send the price through and then he says Okay, it\\'s good, when do you have time? So we want him. That\\'s a time. When do you have time and then you send it from there. He can\\'t even do it right. Then I\\'ll never send him there and then you know it\\'s very and all not really. I eventually created a group app with that guy.\\nA: Number of people from the department so that it is clear again, is agreed upon, that\\'s fine. Yes, it would of course be even better if it\\'s just nicely traceable in the file.  B: Yes, yes, yes, yes, yes, yes, it really needs to leave a trace. Yes, and a trace needs to be there in the file and indeed, clear. Okay, okay.  C: And that can be then Transport with the transport order or another work order to, for example, Arthur, our carpenter, so that he simply receives on that and that date you have to do this and then you have agreed on this and this, or send an assignment, indeed and follow through. Yes, okay.\\nA: And that detachable one is just too expensive or too small or gives the name Yes exactly right.  B: I suggest that we further develop the current processor with the given information. C: Yes, exactly. Oh yes, we have a quite large client and that sounds quite subpage that I could throw that in a bit as a kind of Bosch Script. It\\'s our big piece of land actually, we even have separate storage for that and essentially that is uh it\\'s not really charged separately or anything, but it is documented separately and a separate process is carried out for that.  A: Okay and also what needs to be stored there. There are mainly her and where they are placed and then also containers for that and that is being done and that we also need to do something with, because that is also now a manual list.\\nA: The products being made and the documentation being made for them are all generic products, right, otherwise, based on the product number you can already automatically create a lot of documents, for example, oxygen cylinders. It\\'s all standard.  B: Yes, I have to do a lot, and now it all has to be done manually again by someone who actually doesn\\'t do all of that themselves, so I do some treatments, but then that person asks someone else to do it.  C: This can also be done. It should be much easier and, well, no, what exactly that is, I can tell you. Only, it might be easier if the one doing it is involved, and that is Younes, who has been doing it from the beginning, because I also know it, and, Youssef, you know this.  A: Yes, but I still prefer it, but since he also kind of said that he... I think and yes, he is doing it now, but he should coordinate the Warehouse.  B: Ok, and he is doing this now, but actually, I think he shouldn\\'t be doing this anymore in the future. So this should be his Legacy, so to speak. He needs to make sure of this.\\nA: Someone else can do this work with the same level of competence and he has always done it so he just has his hands free to keep an overview and, and just That person is going to do what we are actually going to replace him with. Yeah, yeah, yeah, I would suggest that we maybe have a chat with him at some point, maybe you can join if you also have time. B: Yes, let\\'s map out that process properly. C: Sounds good. I understand that I don\\'t have time for that now. And he probably doesn\\'t either, so you\\'ll have to choose separately. If I at your... But this is just something that needs to be sorted out. I think that if we map this out well and make a plan, we already have a large part of the information provision in place. Yes, that will prevent a lot of hassle downstairs. Yes, yes, exactly, the reduction there, indeed. That\\'s the dossier reduction, that\\'s what you need to do.\\nA: Setting up, I think that\\'s really crucial, man. It\\'s really about information flows, you know, how that information comes into a dossier. Because that was also one thing we found a bit challenging with the previous system, the system we bought to set up. Now we\\'re used to receiving an email and working from Outlook, that\\'s how we work now. If you\\'re really going to work with a ticketing system, then I think you\\'ll have to dedicate the whole day to it. B: Yeah, so, but I think the problem is that the system wasn\\'t adaptive enough, if I understood correctly. Why did it fail? It was just a feeling I had, actually. Because in principle, you could receive an email and in that email, tickets would be sent. It just goes on like that. C: And if you let that mail into the system, you\\'d just get a list of mails and then you have a test.\\nA: Are only uh between those 5 attachments or is it part of it, but one is important evidence, so you couldn\\'t drag that. Documents had to be filled in and that will come, but we have purchased a separate software for that. And so it didn\\'t go smoothly dynamically Hm Okay and I I wish I could show it to you guys. I still have all that communication and a screenshot and so Where I stumbled a bit, but it was mainly about the part shipments. The part with multiple checks and one file and a piece of user-friendliness especially there eh it really was a but that was a eh it\\'s not a small thing. So it was online and eh and there was a link in between us with our CRM that\\'s good. Yes, that is also a DMX, it is quite logical that there are some links and then CRM is exactly I assume or eh We have we have a Costner relations. Management system actually exists for the C\\nA: Yes, I have a Visions uh management system. Yes, we do have that through them and that is where we also manage our customer contact for sales and such. Feline Horst, that\\'s taken care of completely there operationally so we don\\'t do anything with that. The only link we have is, of course, her customers. Yes, they are just our registered customers, so that\\'s handy when we receive a Koos valentine by email and the registration takes place in that system.    B: Yes, that\\'s not CRM because we don\\'t use that. Okay. The only reason I wanted to have it alongside is because, uh, because, you know, when you create a file for a customer, you select the customer, and all those names, for example of the main contact person, are listed in CRM. If they are not in there, we add them in. But it\\'s very handy because there\\'s also a sort of transferability and it also fills in details on its own and later on it will provide all kinds of information that we could use.    C: So that\\'s why I thought, well, that.\\nA: You are building some kind of databases. Tell me, where do you, uh yes yes yes yes yes, and and so it\\'s also important to register my name, of who have I received these assignments from and how do I reach them, and maybe you can even click on his name and start calling with teams, because we call with 10. Yes. And uh yes, all these things may, but the main point was that there had to be some sort of file structure and that just uh, that just didn\\'t go very smoothly. It wasn\\'t very easy. Ok and uh we haven\\'t even touched the part about costs yet by the way, and uh and uh that\\'s when you do the cost building, but we haven\\'t even gotten that far. It\\'s uh that is important, uh the journal. The dangerous element journal is important, because documentation is important, for example when you create a document, we currently do that separately in this plus. Then a document is created, and that document is not printed out, because that B: <translated text> C: <translated text>\\nA: And that must then be slid in. Yes, because in which father? You do that at the first phase, right?  B: Yes, if that can be done.  C: But basically there is between inside and eh unsubscribe that your documents are only here then, probably things to do.  A: You are now only sorting out your shipment and only then do you make your documentation.  B: Okay, okay, okay, okay, okay, okay. Yes.  C: Yes, and that and that sit in a separate system, though.  A: If you have a 9 Plus, each document simply has a separate URL, so you could just do that and maybe even something plus, well, what I said.  B: Maybe we can just do it ourselves and that, yes, or based on that, to date.  C: Just get to work there.  A: Right now, one thing that is, for example, is that if you indeed make a DVD for air transport, you actually still need to make a separate document for road transport, because we have to give it with us again, and those are two separate actions.\\nA: There is also overlap between them, they both have the same number eh Both the same amount Collie Do you know that if you fill in something once, you would have to do multiple documents with it, exactly exactly and against plus it doesn\\'t do that at all against Office for example another program. That does do that. You fill in a lot at the front and that ensures that at the back end three documents just come out simultaneously. Yes yes, that shouldn\\'t be complicated Hear but okay.   B: No, but so much is a product of 9 Plus a eh of eh another Products okay But against they have looked a lot cheaper by the way, uh also fine, but we just don\\'t use that. No no no no no, Clear Clear yes, but yes, but that has to do with among other things with the fact that at the moment that you pay your bill from eh from against Plus eh say they   C: Okay.\\nA: Rule giving that is valid for 2 years, so she basically just adheres to a contract of 2 years, um. B: Yes, it could just be that you are then stuck there again for a longer period. C: Yes yes yes yes, and often because it\\'s busy, and stuff. Then you forget, that will get you, yes, then you\\'re stuck for another two years. Yes yes, it\\'s just crappy, this plus okay overall for us now because it\\'s just as reactive and, well, and hard to understand as we are, which is easy, because you Just fill one of Okay, I have this, yes, I saw I think \\'The Office\\', then you really need to work with boxes and packaging inside to paint a complete picture of what you have, and then you can roll out all documentation from there and that\\'s actually quite nice, based on all that information you can also carry your hazardous vacuum journal if you fill in clear information once, then everything should just roll out, so to speak yes yes yes yes yes a, but we don\\'t have that now, that\\'s why this plus is easier. We just fill it in in one go in a pop-up, it only takes 30 seconds. Yes.\\nA: But then you have a dgd. Yes ok, and then you still have to do all the rest. B: Does it ever happen that you receive a request. You have all the information and then you would do that in our new system. C: We have processed all the information and eventually all the documentation is there. It comes in and at some point.\\nA: Wrong receive. Ah, shit. It is the wrong pallet. We will come back to exchange it. That happens too.   B: Ok, but then it\\'s just a matter of okay, so they will come to pick it up and then deliver the correct one.   C: Yes, because if they still have all the correct information, but step by step in terms of the request, they have provided all the information and you just receive a slightly different product.\\nA: Yes, you still need to have Jesse if you want this and it\\'s not completely clear, and I don\\'t know. This is not on the packing list at all, what is that actually and should it be included and yes, all items that should basically be done already. This happens when you do it differently, you can\\'t execute the assignment, But it would be nice if it comes more centrally from the dossier again. The head dossier is still often something like an initial email, all communication that takes place in between. It\\'s just in the email somewhere, not in the dossier, and ultimately just the document that the initial email initiale documents sometimes documents that can be added to them all, and costs on themselves yes, It could be much more that is eh yes and must be because that\\'s important too, and it\\'s maybe that I\\'ve actually mentioned a bit. That\\'s eh insects in its ex and that kind of incident and accident reporting, is that. B: Yes, it does sound like there\\'s a need for more centralization and clarity in the process. It seems like there are some missing pieces that need to be addressed to ensure smooth execution of the assignment. C: I agree, having a more streamlined and organized approach will definitely help in avoiding confusion and ensuring that all necessary documents and information are readily available. It\\'s important for proper documentation and reporting purposes.\\nA: That there is a q and uh, say a quality report and um, that could be in that system, they are obliged to do better. Yes, you indicated that, yes, yes, where and that should simply be able to be linked to the file, so I actually want for the management and um, but not beyond who that is that that a report can simply be made of Okay um this did not go well and that has definitely happened in any case. We have been involved with this person. Yes, and that there should just be a clear something that arises like well, shall we just have a clear evidential framework of the guidance of that person or can indicate a person. Okay, in the past month, a few things have not gone well and we have had this leadership unknown.    B: Do you have any extra information or are you just being a jerk? That\\'s also possible and um but we do not have that now. No, we do not, if you now ask like Okay, who made a fuckup last month. No idea, eh, the user knows because he simply has it in his memory, but otherwise actually not, yes.  C: It\\'s important that we can track these issues and have a clear system in place to address them. We need to create a report for management to show what went wrong and how we were involved, so we can improve our processes moving forward. It seems like we need more communication and accountability in this process to prevent these mistakes from happening again.\\nA: And it is not about accidents, for example, no incidents there. Reports are made. Absolutely nothing, and that is just not good, and that just has to happen, but then yes, what is it all about now and so if you save the file for me, where everyone can see, for example. You see that I am messing up. Haha, you know, that evidence, I find that not nice.  B: No no, but what if a cargo gets damaged, or someone has driven it wrong, how do you justify that to the customer, do you just call them or send an email again?  C: Well, we have to make a report of an incident or damage that occurred and then report it to the customer, hmm, but it would be nice if a report is created there, so that there is a reference, that\\'s more the link, right. It may be separate from the file for me at least, that there is a sort of report of the incident. Yes, and it must be linked, because if you didn\\'t.\\nA: Also sometimes not linked to the file, for example, a Filip drove a ceiling out because he lowered his forklift. So yes, yes, yes, it has happened to me too by coincidence. B: Yes, and well, it\\'s just nice to know, well, that back then, it happened by that specific person, purely just to see so that you can get a bit of an idea of how someone performs when mistakes are made, what kind of processes take place, and by whom those mistakes are made and why, right? C: Yes, but I assume that there must be some sort of supervisor there in the warehouse who coordinates the people and keeps an eye on them, and well, I\\'m often in the office looking the other way.  A: Ok, so you don\\'t see the incidents happening, right? But it\\'s dealt with during the conversations, if someone behaves very conscientiously. Then he mentions on the app that he has done something, but that\\'s all just if we are, let\\'s say, expanding the newest close-knit team. And there, ownership is simply taken, you know, does something?\\nA: Often just admit honestly, but not very human on time, but when you grow a bit more later, it all becomes less personal.  B: Yes, that\\'s in the acorns, right? Yeah, and he just covers up what he\\'s done, because he blames someone else even worse, right?  C: Yeah, that\\'s just, um, yeah, okay, yeah, that\\'s something that, um, look, you can easily link such an incident report with a shipment to the reference number. That\\'s easy to link, but if indeed, yeah, the ceiling, yeah, then you have to briefly mention a person and then you have to fill in that person in the system, because you need to give a dossier to the owner, right.  A: Exactly, and you need to hold on to that, right. So this one goes into the file and incidentally also happened to ride the ceiling again. So yeah, that name is already there.  B: Yes, in the um, that one, you can select that one. Right, that\\'s an employee or a user or whatever, right.  C: Yeah, yeah, okay, and you might also base it on the fact that, um, that it\\'s a dossier that has an owner.\\nA: So, you just do it too. We agree that person then has a list of my things. This is my Workout, you know, this is my outfit, and that way we have evidence to show to others as well. Maybe they can see it if they look it up, but it\\'s not in their favor and that\\'s the case now. You actually have 20 files that are in, and who does what? It\\'s not clear at all who does what or if anything has been done. B: No, no, no, it seems like I\\'m really criticizing that. But that\\'s just the way it is, of course. Yes, that\\'s just the way we work. On one hand it\\'s nice and fast and easy, pragmatic. But it\\'s just lacking clarity and can be risky because you lose track of things. C: It\\'s unpleasant because it\\'s not clear, not transparent. It\\'s not traceable. There are a lot of things missing. It\\'s cheap and fast, but it\\'s not perfect. We had to do it, because we had to start, but now we are really on track.\\nA: That really requires a few more glowing strokes. Yes, it really needs to move forward. Yes, I assume all to grow further, because this is not the longest, but well. B: I don\\'t need you guys. You probably have much more insight into why Rive here is just unsuitable if four people are in one shower, with one leaving and duplicating lights, leading to synchronization issues exactly.  C: Well, that\\'s just terrible. It\\'s already happening in large quantities. Yes, that shouldn\\'t happen. No, so that\\'s just not okay. And then you could say go work online at this Intertoys or something, but that also has its limitations.\\nA: Yes okay okay, but that\\'s also bad. Yes, it\\'s just really. The heat is just its time really.  B: Yes, not even the best time has just passed. It\\'s done exactly exactly.  C: I agree with you. Yes okay, yes, I think we have enough information Filip so for now at least.\\n\\n', metadata={'source': 'transcript.txt'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"transcript.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web base loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load, chunk and index the content of the html page\n",
    "loader = WebBaseLoader(web_path=(\"https://qalmaqihir.github.io/bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"md-content\")#(\"md-content_inner\",\"md-sidebar\")\n",
    "                       )),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\n================\\nby Jawad Haider\\n00 - PyTorch Gradients¶\\n\\n\\n\\n\\n\\nCopyright Qalmaqihir\\n\\n\\nFor more information, visit us at\\nwww.github.com/qalmaqihir/\\n\\n\\n\\nPyTorch\\n  Gradients\\nAutograd - Automatic\\n    Differentiation\\nBack-propagation on one step\\nBack-propagation on multiple\\n    steps\\nTurn off\\n    tracking\\n\\n\\nPyTorch Gradients¶\\nThis section covers the PyTorch\\nautograd\\nimplementation of gradient descent. Tools include:\\n*\\ntorch.autograd.backward()\\n  *torch.autograd.grad()\\nBefore continuing in this section, be sure to watch the theory lectures\\nto understand the following concepts:\\n* Error functions (step andsigmoid)\\n* One-hot encoding\\n* Maximum likelihood\\n* Cross entropy(including multi-class cross entropy)\\n* Back propagation (backprop)\\n\\n\\nAdditional Resources:\\n\\n\\nPyTorch\\nNotes:\\xa0\\xa0Autograd mechanics\\n\\nAutograd - Automatic Differentiation¶\\nIn previous sections we created tensors and performed a variety of\\noperations on them, but we did nothing to store the sequence of\\noperations, or to apply the derivative of a completed function.\\nIn this section we’ll introduce the concept of the dynamic\\ncomputational graph which is comprised of all the Tensor\\nobjects in the network, as well as the Functions used to create\\nthem. Note that only the input Tensors we create ourselves will not have\\nassociated Function objects.\\nThe PyTorch\\nautograd\\npackage provides automatic differentiation for all operations on\\nTensors. This is because operations become attributes of the tensors\\nthemselves. When a Tensor’s .requires_grad attribute is set to\\nTrue, it starts to track all operations on it. When an operation\\nfinishes you can call .backward() and have all the gradients\\ncomputed automatically. The gradient for a tensor will be accumulated\\ninto its .grad attribute.\\nLet’s see this in practice.\\nBack-propagation on one step¶\\nWe’ll start by applying a single polynomial function\\n\\nto tensor . Then we’ll\\nbackprop and print the gradient\\n.\\n\\nStep 1. Perform standard imports¶\\nimport torch\\n\\nStep 2. Create a tensor with requires_grad set to True¶\\nThis sets up computational tracking on the tensor.\\nx = torch.tensor(2.0, requires_grad=True)\\n\\nStep 3. Define a function¶\\ny = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\\n\\nprint(y)\\n\\ntensor(63., grad_fn=<AddBackward0>)\\n\\nSince  was created as a\\nresult of an operation, it has an associated gradient function\\naccessible as y.grad_fn The calculation of\\n is done as:\\n\\nThis is the value of \\nwhen .\\nStep 4. Backprop¶\\ny.backward()\\n\\nStep 5. Display the resulting gradient¶\\nprint(x.grad)\\n\\ntensor(93.)\\n\\nNote that x.grad is an attribute of tensor\\n, so we don’t use\\nparentheses. The computation is the result of\\n\\nThis is the slope of the polynomial at the point\\n.\\nBack-propagation on multiple steps¶\\nNow let’s do something more complex, involving layers\\n and\\n between\\n and our output layer\\n.   \\n1. Create¶\\na tensor\\nx = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\\nprint(x)\\n\\ntensor([[1., 2., 3.],\\n        [3., 2., 1.]], requires_grad=True)\\n\\n2. Create the first layer with ¶\\ny = 3*x + 2\\nprint(y)\\n\\ntensor([[ 5.,  8., 11.],\\n        [11.,  8.,  5.]], grad_fn=<AddBackward0>)\\n\\n3. Create the second layer with ¶\\nz = 2*y**2\\nprint(z)\\n\\ntensor([[ 50., 128., 242.],\\n        [242., 128.,  50.]], grad_fn=<MulBackward0>)\\n\\n4. Set the output to be the matrix mean¶\\nout = z.mean()\\nprint(out)\\n\\ntensor(140., grad_fn=<MeanBackward1>)\\n\\n5. Now perform back-propagation to find the gradient of x w.r.t out¶\\n(If you haven’t seen it before, w.r.t. is an abbreviation of with\\nrespect to)\\nout.backward()\\nprint(x.grad)\\n\\ntensor([[10., 16., 22.],\\n        [22., 16., 10.]])\\n\\nYou should see a 2x3 matrix. If we call the final out tensor\\n“”, we can calculate the\\npartial derivative of \\nwith respect to \\nas follows:\\n\\n\\nTo solve the derivative of\\n we use the\\nchain rule, where\\nthe derivative of\\n\\nIn this case\\n\\nTherefore,\\n\\n\\n\\n\\nTurn off tracking¶\\nThere may be times when we don’t want or need to track the computational\\nhistory.\\nYou can reset a tensor’s requires_grad attribute in-place using\\n.requires_grad_(True) (or False) as needed.\\nWhen performing evaluations, it’s often helpful to wrap a set of\\noperations in with torch.no_grad():\\nA less-used method is to run .detach() on a tensor to prevent future\\ncomputations from being tracked. This can be handy when cloning a\\ntensor.\\n\\nA NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch\\nv0.4.0 (April 2018) Tensors (torch.Tensor) only held data, and\\ntracking history was reserved for the Variable wrapper\\n(torch.autograd.Variable). Since v0.4.0 tensors and variables\\nhave merged, and tracking functionality is now available through the\\nrequires_grad=True flag.\\n\\n\\n\\n', metadata={'source': 'https://qalmaqihir.github.io/bootcampsnotes/pytorchDLbootcamp/02-ANN-Artificial-Neural-Networks/00-PyTorch-Gradients/'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_content = loader.load()\n",
    "web_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader #, PDFMinerLoader, PDFPlumberLoader,PDFMinerPDFasHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human\\nknowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching\\net al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,\\na common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani\\net al., 2017; Devlin et al., 2018; Brown et al., 2020);\\n•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve\\nthe performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together\\nwith reinforcement learning (RL) algorithms to further align the model with complex and subjective\\nhuman values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).\\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the\\ndistance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;\\nDevlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are\\n∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the\\nlanguage model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward\\nmodel produces a scalar score for each generated response, and is, therefore, able to provide the language\\nmodel with online feedback. This accessibility to online feedback allows the language model to be trained\\nvia reinforcement learning (RL), giving rise to the RLHF stage.\\nAmong the RL techniques that are applied to language models, one of the most prominent algorithms\\nis proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO\\n(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample\\nefficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\\nlonger requires estimating the importance ratio. We show that such differences bring huge benefits in terms\\nof sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,\\n2022) and the StackExchange Beeching et al. (2023) dataset. We first evaluate using the reward model,\\ntrained on the same dataset to produce a scalar reward for each prompt-response pair. We also evaluate\\nthe human preferences of the resulting language model using GPT-4 to demonstrate the effectiveness of the\\nalgorithm. Our empirical results highlight three major advantages of APAoverPPO:\\n(i)APAis more sample-efficient . Fine-tuned on the same number of samples, the language model\\nobtained via APAscores consistently higher on the evaluation set than the one obtained with PPO.\\n(ii)APAaffords steadier control over the deviation from the language model’s initial policy .\\nMeasured by KLdivergence, the deviation of the ultimate policy generated by APAis comparable\\nwith that of PPO, yetAPAis less prone to sudden performance degradation during training, which\\nis occasionally observed in PPO. Note that previous study has shown that the control over deviations\\nfrom the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).\\n(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable\\nparameter for KL control, whereas in PPOone has to carefully calibrate the combination of various\\nextra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the\\ncoefficients of the KL controller.\\nMore broadly, this work is related to the line of literature on leveraging ideas from RL to improve the\\nperformance of language models. A few notable examples in this literature include Paulus et al. (2017),\\nwho propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A\\nthorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE\\nbenchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best\\ngenerated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).\\nThe remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section\\n3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.\\nExperimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the\\nexperimental results.\\n2', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='2 Preliminaries\\nIn this section, we overview the standard RL setting in Section 2.1, and discuss how language model training\\nfits into this setting in Section 2.2. We use the following notation. For a positive integer n, we will use\\nthe bracket notation [n]to refer to the set of integers {1, . . . , n }; for a finite set Z, we denote by ∆(Z)the\\nset of probability distributions on Z, and |Z|the cardinality of Z. We use Bdto denote the unit ball in\\nd-dimensional space.\\n2.1 Reinforcement Learning\\nReinforcementlearning(RL)capturestheinteractionbetweenanagentandanenvironmentviatheformalism\\nof a Markov decision process (MDP). We consider a finite-horizon MDP represented by a tuple M=\\n(S,A, H, P, r, ρ ), where Sis a finite state space, Ais a finite action space, His the horizon, P:S×A 7→ ∆(S)\\nis a probability transition matrix, r:S × A 7→ [0,1]is a reward function, and ρ:S 7→ ∆(S)is the initial\\nstate distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive\\nsteps. At the end of an episode, the agent is reset to a state drawn from ρ(·), and a new episode begins.\\nA policy π:S 7→ ∆(A)is a function that maps a state to a distribution over actions. The value function\\nVπ:S 7→Rof policy πis defined as the expected sum of discounted rewards when the agent starts from\\ninitial state sand follows policy πthroughout the episode. Let γ∈[0,1]be the discount factor. For any\\ns∈ S, we have\\nVπ(s):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nGiven a policy π, the state-action value function, also known as the Q-function, can be defined analogously.\\nFor state s∈ Sanda∈ A, we have\\nQπ(s, a):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action\\na, when the agent is in state sat step h.\\nWe also define the occupancy measures dπ\\nstate:S 7→ [0,1]anddπ\\naction :S × A 7→ [0,1]as\\ndπ\\nstate(s):=1\\nHHX\\nh=1P(sh=s|π)and dπ\\naction (s, a):=1\\nHHX\\nh=1P(sh=s, ah=a|π),\\nwhere P(· |π)signifies that all actions are drawn from π. To avoid clutter, we overload the notation dπsuch\\nthatdπ(s)refers to dπ\\nstate(s), and dπ(s, a)refers to dπ\\naction (s, a).\\n2.2 Language Model as Reinforcement Learning Agent\\nIn its simplest form, a language model receives as input a sequence of tokens (x1, . . . , x n), and generates a\\ndistribution over the next token xn+1. All tokens lie in a finite set X. Whenever the agent selects a token\\nthat represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens\\nreaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.\\nComparing with the RL formulation in Section 2.1, a language model can be viewed as an agent that\\noperatesin anenvironmentwithstate space S=SH\\nk=0Xkandaction space A=X, where Histhe maximum\\nnumber of tokens. The transitions are always deterministic, with the next state equal to the concatenation\\nof all the previous tokens and the current token P(sh+1= (x1,···, xk)|sh= (x1,···, xk−1), ah=xk) = 1.\\n3', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='Traditionally, each episode involves the generation of one complete sequence, and a reward is delivered only\\nwhen an episode terminates. In this context, fine-tuning is equivalent to improving the agent policy π. The\\nfield of RL offers a formidable arsenal for this task. In this work, we will focus on policy-based RL algorithms,\\nwhich parameterize the set of agent policies by a set of parameters θand optimize in the parameter space.\\nIn what follows, we will omit the step index h, as its information is already encoded in each state.\\nWe note that most transformer-based language models map a state (context) sand an action (next\\ntoken) ato a logit qθ(s, a), and the next token is sampled according to the distribution induced by the logits\\n{qθ(s, a)}a∈A. This naturally gives rise to the following parameterization of language model policy:\\nπθ(a|s) =exp(qθ(s, a))P\\na∈Aexp(qθ(s, a)).\\n3 Fine-Tuning Based on Reinforcement Learning\\nAs is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is\\ntrained from a human-labeled dataset. An RL algorithm is then applied to improve the language model\\npolicy, using the rewards generated by the reward model. Here we focus mainly on the second step with a\\ngiven reward function.\\nWe summarize a typical policy-based RL algorithm in Algorithm 1. In practice, the parameter update\\nin Equation (1) usually involves several gradient steps rather than a full minimization.\\nAlgorithm 1 Policy Gradient\\n1:Input:An initial policy parameter θ0, a given loss function L(θ;D).\\n2:Setπ0=πinit.\\n3:Foriteration t= 1,2···, T\\n4:Roll out πθt−1to produce dataset Dt=n\\n(s(t)\\n1, a(t)\\n1, r(t)\\n1),···,(s(t)\\nn, a(t)\\nn, r(t)\\nn)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.\\nAs a first step, for each fixed state s, we consider the following KL-regularized optimization problem as\\na target of policy improvement:\\nmaximize\\nθF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL\\x10\\nπθ(· |s)\\r\\rπinit(· |s)\\x11\\n. (2)\\nHere πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy\\nthat we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,\\nand to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state\\ns. The second term in F(θ;s, π), aKLregularizer, controls the deviation of πθfrom πinit. Such regularization\\nis essential, as language models are prone to over-optimization when rewards are generated by an imperfect\\nreward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem\\nin (2) aims at improving upon policy πin state swithin the proximity of πinit.\\nThe optimization (2) is usually broken down into multiple iterations. In each iteration, we maximize\\nF(θ;s, πold), where πoldis the policy that the agent arrives at in the previous iteration. This technique,\\nreferred to as Conservative Policy Iteration (CPI) , was first presented in Kakade and Langford (2002).\\nThe optimization was subsequently generalized to KL-constrained and regularized methods referred to as\\nTrust Region Policy Optimization (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimization\\n(PPO) (Schulman et al., 2017), respectively. In addition to these core methods, there have been several\\n4', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='other policy optimization methods inspired by (2), with one notable example being the Advantage-Weighted\\nRegression (AWR) method (Peng et al., 2019; Nair et al., 2020).\\nIn the following subsection, we will discuss how F(θ;s, π)is connected with the loss function L(θ;D)in\\nvarious algorithms, and propose a new proximal optimization problem whose solution approximates that of\\n(2). The loss function in APAwill be based on this new proximal optimization problem.\\n3.1 Proximal policy optimization\\nPPO leverages importance sampling to circumvent sampling from πθ, arriving at\\nEa∼πθ(·|s)[Advπold(s, a)] =Ea∼πold(·|s)\\x14πθ(a|s)\\nπold(a|s)Advπold(s, a)\\x15\\n,\\nwhere the expectation on the right-hand side can be estimated in an unbiased manner from finite samples.\\nPPO also involves the following innovation: Instead of penalizing the expected advantage with the\\nestimated KL-divergence as in (2), PPO directly subtracts the KL penalty term from the reward received\\nby the agent. And one may also adaptively adjust the penalty weight λbased on the deviation of πθfrom\\nπinit(Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). The KL-penalized reward is then\\nused to estimate a new advantage function dAdv. To avoid ill-conditioned gradients caused by large values or\\nimportance ratio estimates, PPO applies clipping to the objective function. The final loss function is thus\\nLPPO(θ;D) =−1\\n|D|X\\n(s,a)∈Dmin\\x1aπθ(a|s)\\nπold(a|s)dAdv(s, a),clip\\x12πθ(a|s)\\nπold(a|s),1−ϵ,1 +ϵ\\x13\\ndAdv(s, a)\\x1b\\n.\\nNote that the loss function relies on extra tunable hyperparameters. The clipping also makes the estimator\\nbiased.\\n3.2 Advantage weighted regression\\nIf the parameterized policy space {πθ}contained all possible policies including the ground truth policy, the\\nmaximizer of F(θ;s, πold)(2) would induce a policy π⋆that satisfies\\nπ⋆(a|s) =1\\nZ(s)πinit(a|s)·exp(Advπold(s, a)/λ), (3)\\nwhere Z(s) =P\\na′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ)is a normalizing factor. In the case that {πθ}does\\nnot contain all policies, a natural way to maximize F(θ;s, πold)is to project π⋆to{πθ}with respect to\\nKL-divergence. From (3),\\nKL\\x00\\nπ⋆(a|s)∥πθ(a|s)\\x01\\n=−πinit(a|s)\\nZ(s)exp\\x12Advπold(s, a)\\nλ\\x13\\nlog\\x00\\nπθ(a|s)\\x01\\n+C(s), (4)\\nwhere C(s)is a constant that does not depend on θ.\\nTo facilitate online update, AWR makes three changes from Equation (4):\\n•AWR replaces the first-round policy πinitwith the previous-round policy πold. This ensures that one\\ncan utilize the new roll-out samples from previous-round policy to approximate (4).\\n•The KL-divergence in (4) only accounts for one state s. AWR minimizes a distribution of states dπold.\\n•AWR approximates Z(s)≈1. We also provide a related discussion in Appendix A on why such an\\napproximation is warranted.\\nThese changes lead to the loss function introduced in AWR:\\nLAWR(θ) =−E(s,a)∼dπoldh\\nexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01i\\n. (5)\\n5', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 4}),\n",
       " Document(page_content='Given a finite dataset D={(si, ai) :i= 1, . . . , n }sampled from dπold, the corresponding empirical loss\\ncan be written as\\nLAWR(θ;D) =−1\\n|D|X\\n(s,a)∈Dexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01\\n. (6)\\nFor the well-specified case where the parameterized family {πθ}contains the minimizing policy, the\\nminimizer of the population loss is as follows:\\nπ′(a|s) =πold(a|s) exp( Advπold(s, a)/λ)P\\naπold(a|s) exp( Advπold(s, a)/λ). (7)\\nDue to the discrepancies between the original target in Equation (4) and the final loss in Equation (5),\\none can see that the policy AWR converges to is different from the the original target in Equation (3).\\nFurthermore, since πoldchanges in each round, the policy it converges to continues to change.\\nAs we observe in Section 4 and Appendix D.3, AWR can be unstable in the online case due to this reason.\\nThis motivates us to introduce APA, which alleviates this issue, provably converges to the right target in\\nEquation (3), and demonstrates great empirical performance.\\n3.3 Advantage-Induced Policy Alignment\\nTo project the optimal policy π⋆in (3) onto the parameterized policy space, we consider another distance\\ninstead of KL-divergence. In APA, we employ the squared error between log probabilities in place of the\\nKL-divergence:\\n\\x00\\nlogπ⋆(a|s)−logπθ(a|s)\\x012=\\x10\\nlogπθ(a|s) + log Z(s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n.\\nSimilar to the approximation in AWR, we also apply Z(s)≈1, and minimize the expected loss under a state\\ndistribution dπoldin each round, giving rise to the following population loss:\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112i\\n. (8)\\nThe empirical loss on a finite dataset Dsampled from dπoldis thus\\nLAPA(θ;D) =1\\n|D|X\\n(s,a)∈D\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n. (9)\\nAssuming that the parameter space is Θ =Bdand that the parameterized policy space is well-specified\\nsuch that π⋆∈ {πθ|θ∈Θ}, where π⋆is defined in Equation (3), we can establish theoretically that the\\nempirical loss is a reasonable surrogate for the population loss.\\nTheorem 1. Letθ⋆∈arg minθ∈ΘLAPA(θ)be a minimizer of the population loss. Then\\nπθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).\\nFurthermore, let ˆθ∈arg minθ∈ΘLAPA(θ,D)be an empirical loss minimizer. Assume that min(πθ(a|\\ns), πinit(a|s))≥B1and|Adv(s, a)| ≤B2for any s, a, and that log(πθ)isL-Lipschitz with respect to θ\\nunder ℓ2-norm for any s, a. Then for all δ >0, with probability at least 1−δ, for some universal constant\\nC,\\nLAPA(ˆθ)≤CL(B2−log(B1))2r\\ndlog(nL/δ )\\nn.\\nThe proof is deferred to Appendix E. From the theorem, we see that the minimizer of the population\\nAPAloss is exactly the target policy π⋆if the policy πoldis supported on all state-action pairs. In contrast, as\\nwe discussed earlier, convergence properties of the PPO and AWR algorithms have not yet been established.\\nWe also provide alternative interpretations of the proposed loss in terms of f-divergence and soft-Q\\nlearning in Appendix B.\\n6', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 5}),\n",
       " Document(page_content='4 Experimental Results\\nIn our implementation of all of the algorithms that we test, including APA, we define the advantage function\\ntobe Advπold(s, a), whichisestimatedfromdata. Weusethesamegeneralizedadvantageestimationapproach\\nto estimate the advantage as discussed in earlier work Mnih et al. (2016); Schulman et al. (2015b). In\\nparticular, for the rollout (s0, a0, r0, s1, a1, r1,···, sT−1, aT−1, rT−1, sT), the generalized advantage estimator\\nis\\nˆAπold(st, at) =δt+λγδt+1+···+ (λγ)T−1δT−1,\\nwhere δt=r(st, at) +γVπold(st+1)−Vπold(st).\\nHere the value function is another standalone network that we fit throughout the training process with a\\nsquared loss, ˆLV(D) =P\\nsi,ai(V(si)−ˆAπold(si, ai)−Vπold(si))2. Thus the overall loss function is\\nLAPA\\nθ(D) =ˆLAPA(D) +η·ˆLV(D)\\nLAWR\\nθ(D) =ˆLAWR(D) +η·ˆLV(D).\\nFor the implementation of PPO, we use the PPO2 version from Dhariwal et al. (2017), with the adaptive\\nKL controller from Ziegler et al. (2019). We implement PPO with the same hyperparameters as the\\nimplementation in trlX1, which also follows default hyperparameters suggested by Schulman et al. (2017).\\nThe main difference between our version of PPO and that in trlXis that we create a completely separate\\nvalue network rather than creating a value head on top of the language model. In APA, we take λ= 0.1\\nto impose a weaker constraint on the KL coefficient. For AWR, we find that setting λ= 0.1leads to an\\nexplosion of the loss; thus we take λ= 1to stabilize training.\\n4.1 Results on the StackExchange dataset\\nIn this section, we present our experimental results with StackExchange dataset. This dataset includes\\nquestions and their corresponding answers from the StackExchange platform (including StackOverflow for\\ncode and many other topics). The answers are then voted by the users on the platform and an accepted\\nanswer is labeled. Following Beeching et al. (2023); Askell et al. (2021), we assign a score to each answer\\ndepending on the number of upvotes:\\nscore =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3−1, upvotes ≤0,\\n1 +⌊log2(1 +upvotes ) + 0.5⌋,if the questioner accepted the answer ,\\n⌊log2(1 +upvotes ) + 0.5⌋,otherwise .\\nFigure 1: Win rates computed by GPT-4 for\\nStackExchange dataset for models trained by SFT,\\nPPO and APA. Compared to SFT and PPO, APA\\ntrained models generated better responses.We used the pre-processed dataset provided in\\nBeeching et al. (2023) for all SFT, reward\\nmodeling and RL training purposes, available\\nintheHuggingFaceDatasetsas lvwerra/stack-exchange-paired2\\n. We use LLaMA-7B Touvron et al. (2023) models\\nfor this experiment. We use Low-Rank Adaptation\\n(LoRA) method Hu et al. (2021) to reduce the\\nmemory consumption while training. We used\\n8xA100 GPUs for our experiments. The hyper-\\nparameters for this experiment are listed in the\\nAppendix C.4. Fig. 2 shows the reward on\\nthe left and KL divergence from the initial policy\\nfor the three algorithms, PPO, APA and AWR.\\nWe adjust the hyper-parameters to achieve similar\\nKL divergence values, allowing us to compare the\\n1https://github.com/CarperAI/trlx\\n2https://huggingface.co/datasets/lvwerra/stack-exchange-paired\\n7', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='rewards for various algorithms. In the case of AWR,\\neach hyper-parameter set displayed some level of\\ninstability. Clearly, APA quickly converges to a\\nhigher reward than PPO and AWR.\\nFigure 2: Comparison of the performance of three methods on the StackExchange dataset. Left: The x-axis\\nrepresents the total steps, which are proportional to the amount of data used in the training procedure. The\\ny-axis is the reward computed by the same reward model during training. Right: The x-axis represents the\\ntotal steps. The y-axis is the KL divergence between the trained model and the initial model.\\nGPT-4 Evaluation: We conduct a GPT-4 evaluation to evaluate the models trained by different RL\\nmethods on StackExchange dataset. GPT-4 compares the outputs produced by two models, using a reference\\n(chosen) response as a basis for comparison. Fig. 1 shows the win-rates for comparing SFT vs PPO, SFT\\nvs APA and PPO vs APA models. APA consistently outperforms the other two models.\\n4.2 Results on the HH dataset\\nIn this section, we compare PPO, AWR and APAon the human-labeled Helpfulness and Harmlessnes (HH)\\ndataset from Bai et al. (2022a).3. We fine tune three models, including Dahoas/pythia-125M-static-sft ,4\\nDahoas/pythia-1B-static-sft ,5andDahoas/pythia-6B-static-sft .6All the models have gone through\\nsupervised fine-tuning with labeled prompt-response pairs, similar to the protocol in Ouyang et al. (2022)\\nand Ramamurthy et al. (2022). We present the performance of the RL algorithms for pythia-125M and\\npythia-1B in Fig. 3. It shows that after some steps, PPO’s performance begins to deteriorate while APA\\nand AWR are stable. APA achieves the higher reward while maintaining KL divergence from the initial\\npolicy smaller. The details about hyper-parameters used for training and additional results for larger models\\nare discussed in Appendix C.1.\\n5 Conclusions\\nInthispaper, westudytheproblemofonlinepolicyoptimizationinRLHF.Webenchmarktheperformanceof\\nexisting algorithms PPOandAWR, and introduce a new method, APA, which has a theoretical convergence\\nguarantee and affords several advantages over existing algorithms. The key takeaways from our study can\\nbe summarized as follows.\\n3https://huggingface.co/datasets/Dahoas/static-hh\\n4https://huggingface.co/Dahoas/pythia-125M-static-sft\\n5https://huggingface.co/Dahoas/pythia-1B-static-sft\\n6https://huggingface.co/Dahoas/pythia-6B-static-sft\\n8', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 7}),\n",
       " Document(page_content='0 2500 5000 7500 10000 12500 15000 17500 20000\\nsteps3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nPPO\\nAWR\\nAPA\\n2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.51.01.52.02.53.03.5KLKL divergence between the trained and the initial policy for 125M model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\n0.6\\nrewardOnline Policy Optimization with 1B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.01.21.4KLKL divergence between the trained and the initial policy for 1B model\\nPPO\\nAWR\\nAPAFigure 3: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\n9', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 8}),\n",
       " Document(page_content='Stability. As we discussed in Section 1, one of the challenges in RLHF is instability. It is crucial in\\nRL algorithms to impose control on the divergence between new policies and the initial policy after SFT.\\nHowever, the clipping of the objective function and the adaptive KL controller can make the behavior of\\nPPO unstable; for AWR, the update in (7), which reweighs the previous policy by a multiplicative factor\\nin each iteration, also has unknown ramifications. APA, on the other hand, provably converges to π⋆when\\nthe advantage function is fixed, which is close to the initial policy in KL divergence. From the experimental\\nresults, we see that APAis able to provide better and easy-to-adjust KL control by explicitly tuning the\\nhyperparameter λ. Our experiments reveal different levels of instability for PPO and AWR. Specifically,\\nPPO suffers from occasional performance degradation whenever the model policy diverges too much from\\nthe initial policy πinit, and such effect is more pronounced for smaller models. We attribute this to the KL\\ncontroller in PPO. In Appendix D, we demonstrate that PPO can achieve a similar sample efficiency as\\nAPAwithout the KL penalty, albeit at the cost of weaker KL efficiency.\\nSampleefficiency. WiththesamelevelofcontroloverKL-divergence, APAshowshighersampleefficiency\\nthan PPO and AWR. One possible explanation is that in both PPO and AWR, policy improvement critically\\ndepends on using finite samples to reconstruct the sampling policy πold, whereas in APA, minimizing the\\npopulation loss (8) hinges less on the reconstruction of πold. In fact, the APApopulation loss (8) can be\\neffectivelyminimizedaslongasthedataset Dhasadecentcoverageoverstate-actionpairsthatarefrequently\\nvisited by πold. For more discussions on sample efficiency, please refer to Appendix B.\\nOnline vs. offline learning. Our experiments primarily examine the online case, where new data can be\\ngenerated during the training process. The offline setting, where a fixed dataset is given and new samples are\\nnot available, may yield qualitatively different results. In particular, suppose that the offline dataset consists\\nof rollouts from a policy πoff. In this case, if it were trained with infinitely many samples, AWR would\\nconverge to the policy specified in (3). However, the performance of APAmay suffer from distribution shift\\nbecause it can only learn from state-action pairs covered by πoff, and there is no guarantee that the learned\\npolicy performs well on the state-action pairs visited by the current policy. Such distribution mismatch can\\nlead to a significant performance drop for APA, as we observe in Appendix D.3. We also observe that AWR\\ntypically outperforms ILQL for offline learning, although both perform poorly with larger models.\\n10', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,\\nN. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,\\nJ. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for\\nalignment, 2021. 4.1\\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,\\net al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\\npreprint arXiv:2204.05862 , 2022a. 4.2\\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\nC. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 ,\\n2022b. 1\\nE. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert. StackLLaMA:\\nAn RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://\\nhuggingface.co/blog/stackllama . 1, 4.1, 4.1\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\nA. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 1\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from\\nhuman preferences. In Advances in Neural Information Processing Systems , pages 4299–4307, 2017. 1\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\\nfor language understanding. arXiv preprint arXiv:1810.04805 , 2018. 1\\nP. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and\\nP. Zhokhov. OpenAI baselines. https://github.com/openai/baselines , 2017. 3.1, 4\\nS. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning\\nwith agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1\\nD.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,\\net al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.\\narXiv preprint arXiv:2209.07858 , 2022. 1\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint\\narXiv:2210.10760 , 2022. (ii), 3\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\\nadaptation of large language models, 2021. 4.1\\nS. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of\\nthe Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 3\\nW. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE\\nInternational Conference on Development and Learning , pages 292–297. IEEE, 2008. 1\\nA. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human\\nfeedback. In Robotics research , pages 161–176. Springer, 2018. 1\\nJ. Maghakian, P. Mineiro, K. Panaganti, M. Rucker, A. Saran, and C. Tan. Personalized reward learning\\nwith interaction-grounded learning (IGL). arXiv preprint arXiv:2211.15823 , 2022. 1\\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.\\nAsynchronous methods for deep reinforcement learning. In International Conference on Machine Learning ,\\npages 1928–1937. PMLR, 2016. 4\\n11', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with offline\\ndatasets. arXiv preprint arXiv:2006.09359 , 2020. 3\\nR.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.\\nWebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 ,\\n2021. 1\\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\\nA. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint\\narXiv:2203.02155 , 2022. 1, 4.2\\nR. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint\\narXiv:1705.04304 , 2017. 1\\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable\\noff-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:\\nYour language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1\\nR. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi.\\nIsreinforcementlearning(not)fornaturallanguageprocessing? benchmarks,baselines,andbuildingblocks\\nfor natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022. 1, 4.2\\nD. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.\\nInRobotics: Science and Systems , 2017. 1\\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In\\nInternational Conference on Machine Learning , pages 1889–1897. PMLR, 2015a. 3\\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\\ngeneralized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.\\narXiv preprint arXiv:1707.06347 , 2017. 1, 3, 3.1, 4\\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline RL for natural language generation with\\nimplicit language Q-learning. arXiv preprint arXiv:2206.11871 , 2022. 1\\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.\\nLearning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:\\n3008–3021, 2020. 1\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃšre, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient\\nfoundation language models, 2023. 4.1\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\\nAttention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1\\nC. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz. A survey of preference-based reinforcement learning\\nmethods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017. 1\\nR. Yuan, R. M. Gower, and A. Lazaric. A general sample complexity analysis of vanilla policy gradient.\\nProceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.\\n1\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language\\nmodels with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023. 1\\n12', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or\\nk-wise comparisons. International Conference on Machine Learning , 2023. 1\\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.\\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. 1, 3.1, 4\\n13', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 12}),\n",
       " Document(page_content='A Argument for Z(s)≈1\\nNote that in both advantage-weighted regression and advantage-based squared loss, we approximate Z(s)\\nwith 1. Here we justify why this does not hurt the performance.\\nConsider an infinitesimal scenario where |Adv/λ| ≪ | logπinit|. In the scenario of language model, this\\nis usually true since πinitis supported on approximately 50kdistinct tokens and can be very close to zero,\\nwhile Adv/λcan be adjusted to small numbers by adjusting λ.\\nIn this case, we have\\nZ(s) =X\\na∈Aπinit(a|s) exp( Adv(s, a)/λ)\\n=Ea∼πinit[exp( Adv(s, a)/λ)]\\n=Ea∼πinit[1 +Adv(s, a)/λ+o(Adv2(s, a)/λ2)].\\nThis advantage is usually estimated as Advπold, which can be close to Advπinit. And we have\\nEa∼πinit[Advπold(s, a)/λ]≈Ea∼πinit[Advπinit(s, a)/λ] = 0.\\nThus we know that\\nZ(s)≈1 +Ea∼πinit[o(Adv2(s, a)/λ2)]≈1.\\nIn practice, we observe that the squared loss decreases very slowly due to a small learning rate ( 8e−6).\\nThis suggests that the policy changes very slowly, which is another reason why the normalizing factor is not\\nimportant.\\nB Alternative Interpretation of APA\\nRecall that APAcan be written as\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−logπ⋆(a|s)\\x112i\\n,\\nwhere π⋆=πinit·exp(Adv/λ). In the case when πoldis close to πθ, minimizing the squared loss in APAis\\nequivalent to minimizing the following distance between π⋆andπθ:\\nd(π⋆(· |s), πθ(· |s)) =X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132\\n.\\nThis can be viewed as a new f-divergence with f(x) =xlog2(x). We can show by Cauchy-Schwarz that this\\nis always a upper bound for the KL divergence:\\nd(π⋆(· |s), πθ(· |s)) = X\\naπθ(a|s)! X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132!\\n≥X\\naπθ(a|s)\\x0c\\x0c\\x0c\\x0clog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x0c\\x0c\\x0c\\x0c\\n≥X\\naπθ(a|s) log\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\n.\\nC Additional Experiments\\nC.1 Results on the HH dataset\\nIn this dataset, each item is comprised of a prompt, a chosen response and a rejected response labeled by\\nhuman to evaluate the helpfulness and harmlessness of the responses. For the reward model, we use the\\n14', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 13}),\n",
       " Document(page_content='proxy reward model Dahoas/gptj-rm-static7with 6B parameters trained from the same dataset based on\\nEleutherAI/gpt-j-6b .8For all three algorithms, we run two epochs of update after generating 64 responses\\nfrom randomly sampled prompts. For the 125M model, we use batch size 8and learning rate 8×10−6. For\\nthe 1B model, we use batch size 2and learning rate 10−6. For the 6B and larger models, we use batch size\\n1and learning rate 10−6. We use a 32GB Nvidia V100 GPU for fine-tuning 125M and 1B models, and a\\n64GB AMD Mi200 GPU for fine-tuning the 6B and larger models. The maximum response length is set to\\nbe 128 tokens, and the maximum total sequence length is set to be 1024 tokens. We unfreeze the last two\\nlayers during fine-tuning. For each experiment, we run 20k steps in total. The results are plotted as below.\\nIn the left of Figure 4, we compare the three methods on the HH dataset. For all three models, we repeat\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k2.4\\n2.2\\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\nrewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.0KLKL divergence between the trained and the initial policy for 6B model\\nPPO\\nAWR\\nAPA\\nFigure 4: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\nthe experiments with three random seeds 0,100,1000, and plot their min, mean and max. We see that with\\nthe same amount of data, APAis able to achieve the highest reward in all three cases. We also observe that\\nPPO becomes more stable with large models, potentially due to smaller batch size, or the ability of getting\\nhigher reward with a smaller deviation in KL divergence.\\nOn the right of Figure 4, we show how the KL divergence between the current policy and the initial\\npolicy changes as a function of the training process for the three seeds. We can see that for all three models,\\nAPAprovides similar or better KL control than PPO and AWR, although we note that for the 6B model the\\nKL control for PPO is slightly better than APA. Combined with the left part of the figure, we can see that\\nAPAis more KL-efficient than PPO and AWR; i.e., it attains a better performance on the reward model\\nunder the same KL divergence.\\nWe include more experiment results in Appendix C, where we fine tune databricks/dolly-v2-7b9on\\nthe same HH dataset, and 2.7B and 6B models on the TLDR dataset10for the summarization task.\\nWe also conduct ablation studies on the effect of the adaptive KL controller on PPO and the effect of\\ndifferent choices of λfor AWR; see Appendix D. We show in Appendix D.2 that without KL control, PPO\\ncan be as sample efficient as APA, but less KL-efficient. We also observe instability even without the KL\\ncontroller. On the other hand, we observe that changing λprovides a straightforward tradeoff between KL\\ncontrol and performance in APA.\\n7https://huggingface.co/Dahoas/gptj-rm-static\\n8https://huggingface.co/EleutherAI/gpt-j-6b\\n9https://huggingface.co/databricks/dolly-v2-7b\\n10https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n15', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='C.2 Results on the TLDR Dataset\\nWe fine-tune the EleutherAI/gpt-neo-2.7B11and 6B CarperAIopenai_summarize_tldr_sft12models\\non the TLDR dataset13for the summarization task. For EleutherAI/gpt-neo-2.7B , we first fine-tune it\\nwith supervised fine-tuning on the labeled response in the same summarization dataset, and run RLHF on\\nthe supervised fine-tuned policy. The 6B model CarperAIopenai_summarize_tldr_sft has already gone\\nthrough the supervised fine-tuning stage. The reward model is a pre-trained EleutherAI/gpt-j-6b14reward\\nmodel for summarization dataset CarperAI/openai_summarize_comparisons15.\\nWe follow the default setting in trlX with seed 0 and 100, and plot the results in Figure 5. One can see\\nthatAPAis more sample efficient and provides better KL control than PPO in both 2.7B and 6B models.\\n0 2 4 6 8 10\\nsteps / k2.02.12.22.32.42.52.62.72.8rewardOnline Policy Optimization with 2.7B model\\nPPO\\nAWR\\nAPA\\n0 2 4 6 8\\nsteps / k0.00.51.01.52.02.5KLKL divergence between the trained and the initial policy in 2.7B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k2.42.62.83.03.23.43.6rewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k0.20.30.40.50.60.70.8KLKL divergence between the trained and the initial policy in 6B model\\nPPO\\nAWR\\nAPA\\nFigure 5: Comparisons of the performance on TLDR dataset. Left: The x-axis represents the total steps,\\nwhich are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\n11https://huggingface.co/EleutherAI/gpt-neo-2.7B\\n12https://huggingface.co/CarperAI/openai_summarize_tldr_sft\\n13https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n14https://huggingface.co/EleutherAI/gpt-j-6b\\n15https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n16', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 15}),\n",
       " Document(page_content='C.3 Results on the Dolly Model\\nWe fine-tune the databricks/ dolly-v2-7b16model on the HH dataset. We follow the default setting in\\ntrlX with seed 0 and 100, and plot the results in Figure 6. We only include the results for APAand PPO\\nsince AWR drops directly. Different from all other experiments, here for APAwe set λ= 1rather than 0.1\\nto stablize the training and impose stronger KL control. One can see that APAcan still improve over the\\noriginal dolly 7B model and provide better KL control, while PPO fails to bring further improvement.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nrewardOnline Policy Optimization with Dolly 7B model\\nPPO\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k0.000.020.040.060.080.10KLKL divergence between the trained and the initial policy in dolly 7B model\\nPPO\\nAPA\\nFigure 6: Comparisons of the performance on the dolly 7B model. Left: The x-axis represents the total\\nsteps, which are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\nC.4 StackExchange experiment details\\nThe hyper-parameters for the Fig. 2 are listed in table 1.\\nParameter Value\\nMax sequence length 1024\\nMax output length 128\\nLearning rate 2e-5\\nBatch size 16\\nGradient Accumulation 8\\nSFT LoRA dimension 16\\nRM LoRA dimension 8\\nRL LoRA dimension 16\\nAdaptive KL initial KL coeff (PPO) 0.1\\nAdaptive KL target KL coeff (PPO) 6\\nλ(APA) 0.1\\nTable 1: Hyper-parameters for StackExchange experiments as shown in Fig. 2\\n16https://huggingface.co/databricks/dolly-v2-7b\\n17', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 16}),\n",
       " Document(page_content='D Ablation Studies\\nD.1 KL control in APA\\nIn this section, we show how the performance and KL divergence change with different values of λ. We set\\nλ= 0.1,1for the 125M model and plot their performances in Figure 7 with seed 1000. One can see that the\\nchoice of λdirectly determines the level of KL control, along with the convergent point APAreaches. This\\nshows that λprovides a clear trade-off between KL control and model performance.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nLambda=0.1\\nLambda=1\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.0000.0250.0500.0750.1000.1250.150KLKL divergence between the trained and the initial policy for APA in 125M model\\nLambda = 0.1\\nLambda = 1\\nFigure 7: Comparisons of the performance between different λon the 125M model. Left: The x-axis\\nrepresents the total steps, which are proportional to the number of data used in the training procedure. The\\ny-axis is the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The\\ny-axis is the KL divergence between the trained model and the initial model.\\nD.2 KL control in PPO\\nWe show how the performance and KL divergence change with or without adaptive KL control in PPO. We\\nplot their performances in Figure 8 for 125M model with seed 1000. For PPO with adaptive KL controller,\\nthe initial KL coefficient is set to be 0.05. One can see that without KL control, PPO converges to a higher\\nreward compared to APAin Figure 7, at a cost of a significantly higher KL divergence. On the other hand,\\nthe reward of PPO with adaptive KL control begins to drop in the middle. This is due to the large deviation\\nfrom the original policy, which leads to a much larger KL regularization term that dominates the reward.\\nCompared with Figure 7, one can see that APAprovides more stable and controllable KL regularization.\\nD.3 Experiments for Offline Learning\\nWe conduct experiments for offline learning as well. The offline dataset is selected to be all the prompts and\\nresponses from the HH dataset, with reward labeled by the reward model. We use the trained GPT-J reward\\nfunction to label the reward for all the offline data, and compare ILQL, AWR and APAon the same 125M\\nand 1B model after supervised fine-tuning with seed 1000. The result is given in Figure 9. From the results,\\none can see that AWR performs better than ILQL, and APAcannot be directly adapted to the offline case.\\nFurthermore, offline learning cannot help too much after the supervised fine-tuning stage, potentially due to\\nthe large distribution shift between the offline data and the current policy.\\n18', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 17}),\n",
       " Document(page_content='0 2 4 6 8 10 12 14\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nAdaptive KL Control\\nNo KL Control\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k01234567KLKL divergence between the trained and the initial policy in 125M model\\nAdaptive KL Control\\nNo KL ControlFigure 8: Comparisons of the performance of PPO on the 125M model. Left: The x-axis represents the\\ntotal steps, which are proportional to the number of data used in the training procedure. The y-axis is the\\nreward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the\\nKL divergence between the trained model and the initial model.\\n0 2 4 6 8 10 12\\nsteps / k2.8\\n2.7\\n2.6\\n2.5\\n2.4\\n2.3\\n2.2\\n2.1\\n2.0\\nrewardOffline Policy Optimization with 125M model\\nILQL\\nAWR\\nAPA\\n0 2 4 6 8 10\\nsteps / k2.1\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\nrewardOffline Policy Optimization with 1B model\\nILQL\\nAWR\\nAPA\\nFigure 9: Comparisons of the performance between ILQL, AWR and APA on the offline learning dataset.\\n19', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 18}),\n",
       " Document(page_content='E Proof of Theorem 1\\nProof.From the well-specified assumption π⋆∈ {πθ|θ∈Θ}, we know that there exists some θ⋆∈Θsuch\\nthatπθ⋆=π⋆. For the population loss, we know that\\nLAPA(θ⋆) =E(s,a)∼dπolds,ah\\n(logπθ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n=E(s,a)∼dπolds,ah\\n(logπ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThus for any θ′∈arg minθ∈ΘLAPA(θ), there must be LAPA(θ′) = 0, which is equivalent to\\nE(s,a)∼dπolds,ah\\n(logπθ′(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThis means that for any s, aon the support of dπolds,a, we have πθ′(a|s) =π⋆(a|s).\\nFor the second part of the theorem, we know from Hoeffding’s inequality that for any fixed θ∈Θ,\\n|LAPA(θ)−bLAPA(θ;D)|=\\x0c\\x0c\\x0c\\x0c\\x0c1\\nnnX\\ni=1\\x10\\nlogπθ(ai|si)−Adv(si, ai)/λ−logπinit(ai|si)\\x112\\n−Eh\\x10\\nlogπθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\x0c\\x0c\\x0c\\x0c\\x0c\\n≤C·(B2/λ−2 log( B1))2r\\nlog(1/δ)\\nn.\\nLet the Θϵbeϵ-covering of Θunder ℓ2norm, i.e. for any θ∈Θ, one can find some θ′∈Θϵsuch that\\n∥θ−θ′∥2≤ϵ. We also have |Θϵ| ≤(1/ϵ)d. By taking union bound, we know that for all θ∈Θϵ, with\\nprobability at least 1−δ,\\n|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn. (10)\\nLetˆθbe the minimizer of bLAPA(θ;D). Then we know that there exists some ˆθϵ∈Θϵsuch that ∥ˆθ−ˆθϵ∥ ≤ϵ.\\nThis further implies that\\n|LAPA(ˆθ)− LAPA(ˆθϵ)|\\n=|Eh\\x10\\nlogπˆθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n−Eh\\x10\\nlogπˆθϵ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n|\\n≤C(B2/λ−log(B1))Lϵ. (11)\\nSimilarly, we also have |bLAPA(ˆθ)−bLAPA(ˆθϵ)| ≤C(B2/λ−log(B1))Lϵ.Overall, we have\\nLAPA(ˆθ) = (LAPA(ˆθ)− LAPA(ˆθϵ)) + (LAPA(ˆθϵ)−bLAPA(ˆθϵ)) + (bLAPA(ˆθϵ)−bLAPA(ˆθ)) +bLAPA(ˆθ).\\nFor the first and third difference, from Equation (11) we know that they are both bounded by C(B2/λ−\\nlog(B1))Lϵ.For the second difference, we know from Equation (10) that it is bounded by C(B2/λ−\\nlog(B1))2q\\ndlog(1/(ϵδ))\\nn. Lastly, we know that bLAPA(ˆθ) = 0since ˆθ∈arg minθbLAPA(θ)andbLAPA(θ⋆) = 0.\\nThus overall, we have\\nLAPA(ˆθ)≤C((B2/λ−log(B1))Lϵ+ (B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn).\\nTaking ϵ= 1/(Ln)finishes the proof.\\n20', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"Fine-Tuning_Language_Models.pdf\")\n",
    "pdf_content = loader.load()\n",
    "pdf_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='empiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='games (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human\\nknowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching\\net al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,\\na common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani\\net al., 2017; Devlin et al., 2018; Brown et al., 2020);\\n•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve\\nthe performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='the performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together\\nwith reinforcement learning (RL) algorithms to further align the model with complex and subjective\\nhuman values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).\\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the\\ndistance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;\\nDevlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are\\n∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=250)\n",
    "documents = text_splitter.split_documents(pdf_content)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector Embeddings and Vector Store\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma #, Qdrant, Pinecone etc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents[:20],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'page': 0, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='task, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A\\nthorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE\\nbenchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best\\ngenerated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).\\nThe remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section\\n3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.\\nExperimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the\\nexperimental results.\\n2', metadata={'page': 1, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='from the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).\\n(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable\\nparameter for KL control, whereas in PPOone has to carefully calibrate the combination of various\\nextra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the\\ncoefficients of the KL controller.\\nMore broadly, this work is related to the line of literature on leveraging ideas from RL to improve the\\nperformance of language models. A few notable examples in this literature include Paulus et al. (2017),\\nwho propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A', metadata={'page': 1, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='empiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human', metadata={'page': 0, 'source': 'Fine-Tuning_Language_Models.pdf'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector database\n",
    "query = \"Who are the authors for this research paper?\"\n",
    "result = db.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'efficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='efficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no', metadata={'page': 1, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable', metadata={'page': 0, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='problem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\\nlonger requires estimating the importance ratio. We show that such differences bring huge benefits in terms\\nof sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,\\n2022) and the StackExchange Beeching et al. (2023) dataset. We first evaluate using the reward model,\\ntrained on the same dataset to produce a scalar reward for each prompt-response pair. We also evaluate\\nthe human preferences of the resulting language model using GPT-4 to demonstrate the effectiveness of the\\nalgorithm. Our empirical results highlight three major advantages of APAoverPPO:', metadata={'page': 1, 'source': 'Fine-Tuning_Language_Models.pdf'}),\n",
       " Document(page_content='2:Setπ0=πinit.\\n3:Foriteration t= 1,2···, T\\n4:Roll out πθt−1to produce dataset Dt=n\\n(s(t)\\n1, a(t)\\n1, r(t)\\n1),···,(s(t)\\nn, a(t)\\nn, r(t)\\nn)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.\\nAs a first step, for each fixed state s, we consider the following KL-regularized optimization problem as\\na target of policy improvement:\\nmaximize\\nθF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL\\x10\\nπθ(· |s)\\r\\rπinit(· |s)\\x11\\n. (2)\\nHere πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy\\nthat we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,\\nand to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state', metadata={'page': 3, 'source': 'Fine-Tuning_Language_Models.pdf'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Adventage induced policy alignment?\"\n",
    "result = db.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = FAISS.from_documents(documents[:30],OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='task, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A\\nthorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE\\nbenchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best\\ngenerated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).\\nThe remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section\\n3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.\\nExperimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the\\nexperimental results.\\n2', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='from the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).\\n(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable\\nparameter for KL control, whereas in PPOone has to carefully calibrate the combination of various\\nextra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the\\ncoefficients of the KL controller.\\nMore broadly, this work is related to the line of literature on leveraging ideas from RL to improve the\\nperformance of language models. A few notable examples in this literature include Paulus et al. (2017),\\nwho propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='empiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human', metadata={'source': 'Fine-Tuning_Language_Models.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the authors for this research paper?\"\n",
    "result = db1.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\n",
      "policies (Yuan et al., 2022; Dong et al., 2022).\n",
      "To address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\n",
      "which leverages a squared error loss function that directly aligns the output policy of the language model\n",
      "with a target policy in each training epoch. The target policy combines the initial language model policy\n",
      "before the RLHF stage and a correction term based on the advantage function estimated from online samples.\n",
      "Wecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\n",
      "At a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\n",
      "problem, relying on the estimated importance ratio between consecutive policies to compute the policy\n",
      "gradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\n"
     ]
    }
   ],
   "source": [
    "## Similarity search by vector\n",
    "\n",
    "query = \"What is Adventage induced policy alignment?\"\n",
    "\n",
    "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\n",
      "policies (Yuan et al., 2022; Dong et al., 2022).\n",
      "To address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\n",
      "which leverages a squared error loss function that directly aligns the output policy of the language model\n",
      "with a target policy in each training epoch. The target policy combines the initial language model policy\n",
      "before the RLHF stage and a correction term based on the advantage function estimated from online samples.\n",
      "Wecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\n",
      "At a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\n",
      "problem, relying on the estimated importance ratio between consecutive policies to compute the policy\n",
      "gradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\n"
     ]
    }
   ],
   "source": [
    "## Similarity search by vector\n",
    "\n",
    "query = \"What is Adventage induced policy alignment?\"\n",
    "\n",
    "embedding_vector = OpenAIEmbeddings().embed_query(query)\n",
    "docs = db1.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever And Chain With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human\\nknowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching\\net al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,\\na common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani\\net al., 2017; Devlin et al., 2018; Brown et al., 2020);\\n•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve\\nthe performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together\\nwith reinforcement learning (RL) algorithms to further align the model with complex and subjective\\nhuman values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).\\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the\\ndistance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;\\nDevlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are\\n∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the\\nlanguage model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward\\nmodel produces a scalar score for each generated response, and is, therefore, able to provide the language\\nmodel with online feedback. This accessibility to online feedback allows the language model to be trained\\nvia reinforcement learning (RL), giving rise to the RLHF stage.\\nAmong the RL techniques that are applied to language models, one of the most prominent algorithms\\nis proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO\\n(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample\\nefficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\\nlonger requires estimating the importance ratio. We show that such differences bring huge benefits in terms\\nof sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,\\n2022) and the StackExchange Beeching et al. (2023) dataset. We first evaluate using the reward model,\\ntrained on the same dataset to produce a scalar reward for each prompt-response pair. We also evaluate\\nthe human preferences of the resulting language model using GPT-4 to demonstrate the effectiveness of the\\nalgorithm. Our empirical results highlight three major advantages of APAoverPPO:\\n(i)APAis more sample-efficient . Fine-tuned on the same number of samples, the language model\\nobtained via APAscores consistently higher on the evaluation set than the one obtained with PPO.\\n(ii)APAaffords steadier control over the deviation from the language model’s initial policy .\\nMeasured by KLdivergence, the deviation of the ultimate policy generated by APAis comparable\\nwith that of PPO, yetAPAis less prone to sudden performance degradation during training, which\\nis occasionally observed in PPO. Note that previous study has shown that the control over deviations\\nfrom the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).\\n(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable\\nparameter for KL control, whereas in PPOone has to carefully calibrate the combination of various\\nextra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the\\ncoefficients of the KL controller.\\nMore broadly, this work is related to the line of literature on leveraging ideas from RL to improve the\\nperformance of language models. A few notable examples in this literature include Paulus et al. (2017),\\nwho propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A\\nthorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE\\nbenchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best\\ngenerated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).\\nThe remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section\\n3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.\\nExperimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the\\nexperimental results.\\n2', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='2 Preliminaries\\nIn this section, we overview the standard RL setting in Section 2.1, and discuss how language model training\\nfits into this setting in Section 2.2. We use the following notation. For a positive integer n, we will use\\nthe bracket notation [n]to refer to the set of integers {1, . . . , n }; for a finite set Z, we denote by ∆(Z)the\\nset of probability distributions on Z, and |Z|the cardinality of Z. We use Bdto denote the unit ball in\\nd-dimensional space.\\n2.1 Reinforcement Learning\\nReinforcementlearning(RL)capturestheinteractionbetweenanagentandanenvironmentviatheformalism\\nof a Markov decision process (MDP). We consider a finite-horizon MDP represented by a tuple M=\\n(S,A, H, P, r, ρ ), where Sis a finite state space, Ais a finite action space, His the horizon, P:S×A 7→ ∆(S)\\nis a probability transition matrix, r:S × A 7→ [0,1]is a reward function, and ρ:S 7→ ∆(S)is the initial\\nstate distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive\\nsteps. At the end of an episode, the agent is reset to a state drawn from ρ(·), and a new episode begins.\\nA policy π:S 7→ ∆(A)is a function that maps a state to a distribution over actions. The value function\\nVπ:S 7→Rof policy πis defined as the expected sum of discounted rewards when the agent starts from\\ninitial state sand follows policy πthroughout the episode. Let γ∈[0,1]be the discount factor. For any\\ns∈ S, we have\\nVπ(s):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nGiven a policy π, the state-action value function, also known as the Q-function, can be defined analogously.\\nFor state s∈ Sanda∈ A, we have\\nQπ(s, a):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action\\na, when the agent is in state sat step h.\\nWe also define the occupancy measures dπ\\nstate:S 7→ [0,1]anddπ\\naction :S × A 7→ [0,1]as\\ndπ\\nstate(s):=1\\nHHX\\nh=1P(sh=s|π)and dπ\\naction (s, a):=1\\nHHX\\nh=1P(sh=s, ah=a|π),\\nwhere P(· |π)signifies that all actions are drawn from π. To avoid clutter, we overload the notation dπsuch\\nthatdπ(s)refers to dπ\\nstate(s), and dπ(s, a)refers to dπ\\naction (s, a).\\n2.2 Language Model as Reinforcement Learning Agent\\nIn its simplest form, a language model receives as input a sequence of tokens (x1, . . . , x n), and generates a\\ndistribution over the next token xn+1. All tokens lie in a finite set X. Whenever the agent selects a token\\nthat represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens\\nreaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.\\nComparing with the RL formulation in Section 2.1, a language model can be viewed as an agent that\\noperatesin anenvironmentwithstate space S=SH\\nk=0Xkandaction space A=X, where Histhe maximum\\nnumber of tokens. The transitions are always deterministic, with the next state equal to the concatenation\\nof all the previous tokens and the current token P(sh+1= (x1,···, xk)|sh= (x1,···, xk−1), ah=xk) = 1.\\n3', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='Traditionally, each episode involves the generation of one complete sequence, and a reward is delivered only\\nwhen an episode terminates. In this context, fine-tuning is equivalent to improving the agent policy π. The\\nfield of RL offers a formidable arsenal for this task. In this work, we will focus on policy-based RL algorithms,\\nwhich parameterize the set of agent policies by a set of parameters θand optimize in the parameter space.\\nIn what follows, we will omit the step index h, as its information is already encoded in each state.\\nWe note that most transformer-based language models map a state (context) sand an action (next\\ntoken) ato a logit qθ(s, a), and the next token is sampled according to the distribution induced by the logits\\n{qθ(s, a)}a∈A. This naturally gives rise to the following parameterization of language model policy:\\nπθ(a|s) =exp(qθ(s, a))P\\na∈Aexp(qθ(s, a)).\\n3 Fine-Tuning Based on Reinforcement Learning\\nAs is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is\\ntrained from a human-labeled dataset. An RL algorithm is then applied to improve the language model\\npolicy, using the rewards generated by the reward model. Here we focus mainly on the second step with a\\ngiven reward function.\\nWe summarize a typical policy-based RL algorithm in Algorithm 1. In practice, the parameter update\\nin Equation (1) usually involves several gradient steps rather than a full minimization.\\nAlgorithm 1 Policy Gradient\\n1:Input:An initial policy parameter θ0, a given loss function L(θ;D).\\n2:Setπ0=πinit.\\n3:Foriteration t= 1,2···, T\\n4:Roll out πθt−1to produce dataset Dt=n\\n(s(t)\\n1, a(t)\\n1, r(t)\\n1),···,(s(t)\\nn, a(t)\\nn, r(t)\\nn)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.\\nAs a first step, for each fixed state s, we consider the following KL-regularized optimization problem as\\na target of policy improvement:\\nmaximize\\nθF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL\\x10\\nπθ(· |s)\\r\\rπinit(· |s)\\x11\\n. (2)\\nHere πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy\\nthat we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,\\nand to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state\\ns. The second term in F(θ;s, π), aKLregularizer, controls the deviation of πθfrom πinit. Such regularization\\nis essential, as language models are prone to over-optimization when rewards are generated by an imperfect\\nreward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem\\nin (2) aims at improving upon policy πin state swithin the proximity of πinit.\\nThe optimization (2) is usually broken down into multiple iterations. In each iteration, we maximize\\nF(θ;s, πold), where πoldis the policy that the agent arrives at in the previous iteration. This technique,\\nreferred to as Conservative Policy Iteration (CPI) , was first presented in Kakade and Langford (2002).\\nThe optimization was subsequently generalized to KL-constrained and regularized methods referred to as\\nTrust Region Policy Optimization (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimization\\n(PPO) (Schulman et al., 2017), respectively. In addition to these core methods, there have been several\\n4', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='other policy optimization methods inspired by (2), with one notable example being the Advantage-Weighted\\nRegression (AWR) method (Peng et al., 2019; Nair et al., 2020).\\nIn the following subsection, we will discuss how F(θ;s, π)is connected with the loss function L(θ;D)in\\nvarious algorithms, and propose a new proximal optimization problem whose solution approximates that of\\n(2). The loss function in APAwill be based on this new proximal optimization problem.\\n3.1 Proximal policy optimization\\nPPO leverages importance sampling to circumvent sampling from πθ, arriving at\\nEa∼πθ(·|s)[Advπold(s, a)] =Ea∼πold(·|s)\\x14πθ(a|s)\\nπold(a|s)Advπold(s, a)\\x15\\n,\\nwhere the expectation on the right-hand side can be estimated in an unbiased manner from finite samples.\\nPPO also involves the following innovation: Instead of penalizing the expected advantage with the\\nestimated KL-divergence as in (2), PPO directly subtracts the KL penalty term from the reward received\\nby the agent. And one may also adaptively adjust the penalty weight λbased on the deviation of πθfrom\\nπinit(Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). The KL-penalized reward is then\\nused to estimate a new advantage function dAdv. To avoid ill-conditioned gradients caused by large values or\\nimportance ratio estimates, PPO applies clipping to the objective function. The final loss function is thus\\nLPPO(θ;D) =−1\\n|D|X\\n(s,a)∈Dmin\\x1aπθ(a|s)\\nπold(a|s)dAdv(s, a),clip\\x12πθ(a|s)\\nπold(a|s),1−ϵ,1 +ϵ\\x13\\ndAdv(s, a)\\x1b\\n.\\nNote that the loss function relies on extra tunable hyperparameters. The clipping also makes the estimator\\nbiased.\\n3.2 Advantage weighted regression\\nIf the parameterized policy space {πθ}contained all possible policies including the ground truth policy, the\\nmaximizer of F(θ;s, πold)(2) would induce a policy π⋆that satisfies\\nπ⋆(a|s) =1\\nZ(s)πinit(a|s)·exp(Advπold(s, a)/λ), (3)\\nwhere Z(s) =P\\na′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ)is a normalizing factor. In the case that {πθ}does\\nnot contain all policies, a natural way to maximize F(θ;s, πold)is to project π⋆to{πθ}with respect to\\nKL-divergence. From (3),\\nKL\\x00\\nπ⋆(a|s)∥πθ(a|s)\\x01\\n=−πinit(a|s)\\nZ(s)exp\\x12Advπold(s, a)\\nλ\\x13\\nlog\\x00\\nπθ(a|s)\\x01\\n+C(s), (4)\\nwhere C(s)is a constant that does not depend on θ.\\nTo facilitate online update, AWR makes three changes from Equation (4):\\n•AWR replaces the first-round policy πinitwith the previous-round policy πold. This ensures that one\\ncan utilize the new roll-out samples from previous-round policy to approximate (4).\\n•The KL-divergence in (4) only accounts for one state s. AWR minimizes a distribution of states dπold.\\n•AWR approximates Z(s)≈1. We also provide a related discussion in Appendix A on why such an\\napproximation is warranted.\\nThese changes lead to the loss function introduced in AWR:\\nLAWR(θ) =−E(s,a)∼dπoldh\\nexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01i\\n. (5)\\n5', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 4}),\n",
       " Document(page_content='Given a finite dataset D={(si, ai) :i= 1, . . . , n }sampled from dπold, the corresponding empirical loss\\ncan be written as\\nLAWR(θ;D) =−1\\n|D|X\\n(s,a)∈Dexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01\\n. (6)\\nFor the well-specified case where the parameterized family {πθ}contains the minimizing policy, the\\nminimizer of the population loss is as follows:\\nπ′(a|s) =πold(a|s) exp( Advπold(s, a)/λ)P\\naπold(a|s) exp( Advπold(s, a)/λ). (7)\\nDue to the discrepancies between the original target in Equation (4) and the final loss in Equation (5),\\none can see that the policy AWR converges to is different from the the original target in Equation (3).\\nFurthermore, since πoldchanges in each round, the policy it converges to continues to change.\\nAs we observe in Section 4 and Appendix D.3, AWR can be unstable in the online case due to this reason.\\nThis motivates us to introduce APA, which alleviates this issue, provably converges to the right target in\\nEquation (3), and demonstrates great empirical performance.\\n3.3 Advantage-Induced Policy Alignment\\nTo project the optimal policy π⋆in (3) onto the parameterized policy space, we consider another distance\\ninstead of KL-divergence. In APA, we employ the squared error between log probabilities in place of the\\nKL-divergence:\\n\\x00\\nlogπ⋆(a|s)−logπθ(a|s)\\x012=\\x10\\nlogπθ(a|s) + log Z(s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n.\\nSimilar to the approximation in AWR, we also apply Z(s)≈1, and minimize the expected loss under a state\\ndistribution dπoldin each round, giving rise to the following population loss:\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112i\\n. (8)\\nThe empirical loss on a finite dataset Dsampled from dπoldis thus\\nLAPA(θ;D) =1\\n|D|X\\n(s,a)∈D\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n. (9)\\nAssuming that the parameter space is Θ =Bdand that the parameterized policy space is well-specified\\nsuch that π⋆∈ {πθ|θ∈Θ}, where π⋆is defined in Equation (3), we can establish theoretically that the\\nempirical loss is a reasonable surrogate for the population loss.\\nTheorem 1. Letθ⋆∈arg minθ∈ΘLAPA(θ)be a minimizer of the population loss. Then\\nπθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).\\nFurthermore, let ˆθ∈arg minθ∈ΘLAPA(θ,D)be an empirical loss minimizer. Assume that min(πθ(a|\\ns), πinit(a|s))≥B1and|Adv(s, a)| ≤B2for any s, a, and that log(πθ)isL-Lipschitz with respect to θ\\nunder ℓ2-norm for any s, a. Then for all δ >0, with probability at least 1−δ, for some universal constant\\nC,\\nLAPA(ˆθ)≤CL(B2−log(B1))2r\\ndlog(nL/δ )\\nn.\\nThe proof is deferred to Appendix E. From the theorem, we see that the minimizer of the population\\nAPAloss is exactly the target policy π⋆if the policy πoldis supported on all state-action pairs. In contrast, as\\nwe discussed earlier, convergence properties of the PPO and AWR algorithms have not yet been established.\\nWe also provide alternative interpretations of the proposed loss in terms of f-divergence and soft-Q\\nlearning in Appendix B.\\n6', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 5}),\n",
       " Document(page_content='4 Experimental Results\\nIn our implementation of all of the algorithms that we test, including APA, we define the advantage function\\ntobe Advπold(s, a), whichisestimatedfromdata. Weusethesamegeneralizedadvantageestimationapproach\\nto estimate the advantage as discussed in earlier work Mnih et al. (2016); Schulman et al. (2015b). In\\nparticular, for the rollout (s0, a0, r0, s1, a1, r1,···, sT−1, aT−1, rT−1, sT), the generalized advantage estimator\\nis\\nˆAπold(st, at) =δt+λγδt+1+···+ (λγ)T−1δT−1,\\nwhere δt=r(st, at) +γVπold(st+1)−Vπold(st).\\nHere the value function is another standalone network that we fit throughout the training process with a\\nsquared loss, ˆLV(D) =P\\nsi,ai(V(si)−ˆAπold(si, ai)−Vπold(si))2. Thus the overall loss function is\\nLAPA\\nθ(D) =ˆLAPA(D) +η·ˆLV(D)\\nLAWR\\nθ(D) =ˆLAWR(D) +η·ˆLV(D).\\nFor the implementation of PPO, we use the PPO2 version from Dhariwal et al. (2017), with the adaptive\\nKL controller from Ziegler et al. (2019). We implement PPO with the same hyperparameters as the\\nimplementation in trlX1, which also follows default hyperparameters suggested by Schulman et al. (2017).\\nThe main difference between our version of PPO and that in trlXis that we create a completely separate\\nvalue network rather than creating a value head on top of the language model. In APA, we take λ= 0.1\\nto impose a weaker constraint on the KL coefficient. For AWR, we find that setting λ= 0.1leads to an\\nexplosion of the loss; thus we take λ= 1to stabilize training.\\n4.1 Results on the StackExchange dataset\\nIn this section, we present our experimental results with StackExchange dataset. This dataset includes\\nquestions and their corresponding answers from the StackExchange platform (including StackOverflow for\\ncode and many other topics). The answers are then voted by the users on the platform and an accepted\\nanswer is labeled. Following Beeching et al. (2023); Askell et al. (2021), we assign a score to each answer\\ndepending on the number of upvotes:\\nscore =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3−1, upvotes ≤0,\\n1 +⌊log2(1 +upvotes ) + 0.5⌋,if the questioner accepted the answer ,\\n⌊log2(1 +upvotes ) + 0.5⌋,otherwise .\\nFigure 1: Win rates computed by GPT-4 for\\nStackExchange dataset for models trained by SFT,\\nPPO and APA. Compared to SFT and PPO, APA\\ntrained models generated better responses.We used the pre-processed dataset provided in\\nBeeching et al. (2023) for all SFT, reward\\nmodeling and RL training purposes, available\\nintheHuggingFaceDatasetsas lvwerra/stack-exchange-paired2\\n. We use LLaMA-7B Touvron et al. (2023) models\\nfor this experiment. We use Low-Rank Adaptation\\n(LoRA) method Hu et al. (2021) to reduce the\\nmemory consumption while training. We used\\n8xA100 GPUs for our experiments. The hyper-\\nparameters for this experiment are listed in the\\nAppendix C.4. Fig. 2 shows the reward on\\nthe left and KL divergence from the initial policy\\nfor the three algorithms, PPO, APA and AWR.\\nWe adjust the hyper-parameters to achieve similar\\nKL divergence values, allowing us to compare the\\n1https://github.com/CarperAI/trlx\\n2https://huggingface.co/datasets/lvwerra/stack-exchange-paired\\n7', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='rewards for various algorithms. In the case of AWR,\\neach hyper-parameter set displayed some level of\\ninstability. Clearly, APA quickly converges to a\\nhigher reward than PPO and AWR.\\nFigure 2: Comparison of the performance of three methods on the StackExchange dataset. Left: The x-axis\\nrepresents the total steps, which are proportional to the amount of data used in the training procedure. The\\ny-axis is the reward computed by the same reward model during training. Right: The x-axis represents the\\ntotal steps. The y-axis is the KL divergence between the trained model and the initial model.\\nGPT-4 Evaluation: We conduct a GPT-4 evaluation to evaluate the models trained by different RL\\nmethods on StackExchange dataset. GPT-4 compares the outputs produced by two models, using a reference\\n(chosen) response as a basis for comparison. Fig. 1 shows the win-rates for comparing SFT vs PPO, SFT\\nvs APA and PPO vs APA models. APA consistently outperforms the other two models.\\n4.2 Results on the HH dataset\\nIn this section, we compare PPO, AWR and APAon the human-labeled Helpfulness and Harmlessnes (HH)\\ndataset from Bai et al. (2022a).3. We fine tune three models, including Dahoas/pythia-125M-static-sft ,4\\nDahoas/pythia-1B-static-sft ,5andDahoas/pythia-6B-static-sft .6All the models have gone through\\nsupervised fine-tuning with labeled prompt-response pairs, similar to the protocol in Ouyang et al. (2022)\\nand Ramamurthy et al. (2022). We present the performance of the RL algorithms for pythia-125M and\\npythia-1B in Fig. 3. It shows that after some steps, PPO’s performance begins to deteriorate while APA\\nand AWR are stable. APA achieves the higher reward while maintaining KL divergence from the initial\\npolicy smaller. The details about hyper-parameters used for training and additional results for larger models\\nare discussed in Appendix C.1.\\n5 Conclusions\\nInthispaper, westudytheproblemofonlinepolicyoptimizationinRLHF.Webenchmarktheperformanceof\\nexisting algorithms PPOandAWR, and introduce a new method, APA, which has a theoretical convergence\\nguarantee and affords several advantages over existing algorithms. The key takeaways from our study can\\nbe summarized as follows.\\n3https://huggingface.co/datasets/Dahoas/static-hh\\n4https://huggingface.co/Dahoas/pythia-125M-static-sft\\n5https://huggingface.co/Dahoas/pythia-1B-static-sft\\n6https://huggingface.co/Dahoas/pythia-6B-static-sft\\n8', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 7}),\n",
       " Document(page_content='0 2500 5000 7500 10000 12500 15000 17500 20000\\nsteps3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nPPO\\nAWR\\nAPA\\n2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.51.01.52.02.53.03.5KLKL divergence between the trained and the initial policy for 125M model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\n0.6\\nrewardOnline Policy Optimization with 1B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.01.21.4KLKL divergence between the trained and the initial policy for 1B model\\nPPO\\nAWR\\nAPAFigure 3: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\n9', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 8}),\n",
       " Document(page_content='Stability. As we discussed in Section 1, one of the challenges in RLHF is instability. It is crucial in\\nRL algorithms to impose control on the divergence between new policies and the initial policy after SFT.\\nHowever, the clipping of the objective function and the adaptive KL controller can make the behavior of\\nPPO unstable; for AWR, the update in (7), which reweighs the previous policy by a multiplicative factor\\nin each iteration, also has unknown ramifications. APA, on the other hand, provably converges to π⋆when\\nthe advantage function is fixed, which is close to the initial policy in KL divergence. From the experimental\\nresults, we see that APAis able to provide better and easy-to-adjust KL control by explicitly tuning the\\nhyperparameter λ. Our experiments reveal different levels of instability for PPO and AWR. Specifically,\\nPPO suffers from occasional performance degradation whenever the model policy diverges too much from\\nthe initial policy πinit, and such effect is more pronounced for smaller models. We attribute this to the KL\\ncontroller in PPO. In Appendix D, we demonstrate that PPO can achieve a similar sample efficiency as\\nAPAwithout the KL penalty, albeit at the cost of weaker KL efficiency.\\nSampleefficiency. WiththesamelevelofcontroloverKL-divergence, APAshowshighersampleefficiency\\nthan PPO and AWR. One possible explanation is that in both PPO and AWR, policy improvement critically\\ndepends on using finite samples to reconstruct the sampling policy πold, whereas in APA, minimizing the\\npopulation loss (8) hinges less on the reconstruction of πold. In fact, the APApopulation loss (8) can be\\neffectivelyminimizedaslongasthedataset Dhasadecentcoverageoverstate-actionpairsthatarefrequently\\nvisited by πold. For more discussions on sample efficiency, please refer to Appendix B.\\nOnline vs. offline learning. Our experiments primarily examine the online case, where new data can be\\ngenerated during the training process. The offline setting, where a fixed dataset is given and new samples are\\nnot available, may yield qualitatively different results. In particular, suppose that the offline dataset consists\\nof rollouts from a policy πoff. In this case, if it were trained with infinitely many samples, AWR would\\nconverge to the policy specified in (3). However, the performance of APAmay suffer from distribution shift\\nbecause it can only learn from state-action pairs covered by πoff, and there is no guarantee that the learned\\npolicy performs well on the state-action pairs visited by the current policy. Such distribution mismatch can\\nlead to a significant performance drop for APA, as we observe in Appendix D.3. We also observe that AWR\\ntypically outperforms ILQL for offline learning, although both perform poorly with larger models.\\n10', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,\\nN. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,\\nJ. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for\\nalignment, 2021. 4.1\\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,\\net al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\\npreprint arXiv:2204.05862 , 2022a. 4.2\\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\nC. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 ,\\n2022b. 1\\nE. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert. StackLLaMA:\\nAn RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://\\nhuggingface.co/blog/stackllama . 1, 4.1, 4.1\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\nA. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 1\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from\\nhuman preferences. In Advances in Neural Information Processing Systems , pages 4299–4307, 2017. 1\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\\nfor language understanding. arXiv preprint arXiv:1810.04805 , 2018. 1\\nP. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and\\nP. Zhokhov. OpenAI baselines. https://github.com/openai/baselines , 2017. 3.1, 4\\nS. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning\\nwith agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1\\nD.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,\\net al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.\\narXiv preprint arXiv:2209.07858 , 2022. 1\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint\\narXiv:2210.10760 , 2022. (ii), 3\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\\nadaptation of large language models, 2021. 4.1\\nS. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of\\nthe Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 3\\nW. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE\\nInternational Conference on Development and Learning , pages 292–297. IEEE, 2008. 1\\nA. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human\\nfeedback. In Robotics research , pages 161–176. Springer, 2018. 1\\nJ. Maghakian, P. Mineiro, K. Panaganti, M. Rucker, A. Saran, and C. Tan. Personalized reward learning\\nwith interaction-grounded learning (IGL). arXiv preprint arXiv:2211.15823 , 2022. 1\\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.\\nAsynchronous methods for deep reinforcement learning. In International Conference on Machine Learning ,\\npages 1928–1937. PMLR, 2016. 4\\n11', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with offline\\ndatasets. arXiv preprint arXiv:2006.09359 , 2020. 3\\nR.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.\\nWebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 ,\\n2021. 1\\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\\nA. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint\\narXiv:2203.02155 , 2022. 1, 4.2\\nR. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint\\narXiv:1705.04304 , 2017. 1\\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable\\noff-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:\\nYour language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1\\nR. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi.\\nIsreinforcementlearning(not)fornaturallanguageprocessing? benchmarks,baselines,andbuildingblocks\\nfor natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022. 1, 4.2\\nD. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.\\nInRobotics: Science and Systems , 2017. 1\\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In\\nInternational Conference on Machine Learning , pages 1889–1897. PMLR, 2015a. 3\\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\\ngeneralized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.\\narXiv preprint arXiv:1707.06347 , 2017. 1, 3, 3.1, 4\\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline RL for natural language generation with\\nimplicit language Q-learning. arXiv preprint arXiv:2206.11871 , 2022. 1\\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.\\nLearning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:\\n3008–3021, 2020. 1\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃšre, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient\\nfoundation language models, 2023. 4.1\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\\nAttention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1\\nC. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz. A survey of preference-based reinforcement learning\\nmethods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017. 1\\nR. Yuan, R. M. Gower, and A. Lazaric. A general sample complexity analysis of vanilla policy gradient.\\nProceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.\\n1\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language\\nmodels with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023. 1\\n12', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or\\nk-wise comparisons. International Conference on Machine Learning , 2023. 1\\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.\\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. 1, 3.1, 4\\n13', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 12}),\n",
       " Document(page_content='A Argument for Z(s)≈1\\nNote that in both advantage-weighted regression and advantage-based squared loss, we approximate Z(s)\\nwith 1. Here we justify why this does not hurt the performance.\\nConsider an infinitesimal scenario where |Adv/λ| ≪ | logπinit|. In the scenario of language model, this\\nis usually true since πinitis supported on approximately 50kdistinct tokens and can be very close to zero,\\nwhile Adv/λcan be adjusted to small numbers by adjusting λ.\\nIn this case, we have\\nZ(s) =X\\na∈Aπinit(a|s) exp( Adv(s, a)/λ)\\n=Ea∼πinit[exp( Adv(s, a)/λ)]\\n=Ea∼πinit[1 +Adv(s, a)/λ+o(Adv2(s, a)/λ2)].\\nThis advantage is usually estimated as Advπold, which can be close to Advπinit. And we have\\nEa∼πinit[Advπold(s, a)/λ]≈Ea∼πinit[Advπinit(s, a)/λ] = 0.\\nThus we know that\\nZ(s)≈1 +Ea∼πinit[o(Adv2(s, a)/λ2)]≈1.\\nIn practice, we observe that the squared loss decreases very slowly due to a small learning rate ( 8e−6).\\nThis suggests that the policy changes very slowly, which is another reason why the normalizing factor is not\\nimportant.\\nB Alternative Interpretation of APA\\nRecall that APAcan be written as\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−logπ⋆(a|s)\\x112i\\n,\\nwhere π⋆=πinit·exp(Adv/λ). In the case when πoldis close to πθ, minimizing the squared loss in APAis\\nequivalent to minimizing the following distance between π⋆andπθ:\\nd(π⋆(· |s), πθ(· |s)) =X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132\\n.\\nThis can be viewed as a new f-divergence with f(x) =xlog2(x). We can show by Cauchy-Schwarz that this\\nis always a upper bound for the KL divergence:\\nd(π⋆(· |s), πθ(· |s)) = X\\naπθ(a|s)! X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132!\\n≥X\\naπθ(a|s)\\x0c\\x0c\\x0c\\x0clog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x0c\\x0c\\x0c\\x0c\\n≥X\\naπθ(a|s) log\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\n.\\nC Additional Experiments\\nC.1 Results on the HH dataset\\nIn this dataset, each item is comprised of a prompt, a chosen response and a rejected response labeled by\\nhuman to evaluate the helpfulness and harmlessness of the responses. For the reward model, we use the\\n14', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 13}),\n",
       " Document(page_content='proxy reward model Dahoas/gptj-rm-static7with 6B parameters trained from the same dataset based on\\nEleutherAI/gpt-j-6b .8For all three algorithms, we run two epochs of update after generating 64 responses\\nfrom randomly sampled prompts. For the 125M model, we use batch size 8and learning rate 8×10−6. For\\nthe 1B model, we use batch size 2and learning rate 10−6. For the 6B and larger models, we use batch size\\n1and learning rate 10−6. We use a 32GB Nvidia V100 GPU for fine-tuning 125M and 1B models, and a\\n64GB AMD Mi200 GPU for fine-tuning the 6B and larger models. The maximum response length is set to\\nbe 128 tokens, and the maximum total sequence length is set to be 1024 tokens. We unfreeze the last two\\nlayers during fine-tuning. For each experiment, we run 20k steps in total. The results are plotted as below.\\nIn the left of Figure 4, we compare the three methods on the HH dataset. For all three models, we repeat\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k2.4\\n2.2\\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\nrewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.0KLKL divergence between the trained and the initial policy for 6B model\\nPPO\\nAWR\\nAPA\\nFigure 4: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\nthe experiments with three random seeds 0,100,1000, and plot their min, mean and max. We see that with\\nthe same amount of data, APAis able to achieve the highest reward in all three cases. We also observe that\\nPPO becomes more stable with large models, potentially due to smaller batch size, or the ability of getting\\nhigher reward with a smaller deviation in KL divergence.\\nOn the right of Figure 4, we show how the KL divergence between the current policy and the initial\\npolicy changes as a function of the training process for the three seeds. We can see that for all three models,\\nAPAprovides similar or better KL control than PPO and AWR, although we note that for the 6B model the\\nKL control for PPO is slightly better than APA. Combined with the left part of the figure, we can see that\\nAPAis more KL-efficient than PPO and AWR; i.e., it attains a better performance on the reward model\\nunder the same KL divergence.\\nWe include more experiment results in Appendix C, where we fine tune databricks/dolly-v2-7b9on\\nthe same HH dataset, and 2.7B and 6B models on the TLDR dataset10for the summarization task.\\nWe also conduct ablation studies on the effect of the adaptive KL controller on PPO and the effect of\\ndifferent choices of λfor AWR; see Appendix D. We show in Appendix D.2 that without KL control, PPO\\ncan be as sample efficient as APA, but less KL-efficient. We also observe instability even without the KL\\ncontroller. On the other hand, we observe that changing λprovides a straightforward tradeoff between KL\\ncontrol and performance in APA.\\n7https://huggingface.co/Dahoas/gptj-rm-static\\n8https://huggingface.co/EleutherAI/gpt-j-6b\\n9https://huggingface.co/databricks/dolly-v2-7b\\n10https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n15', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='C.2 Results on the TLDR Dataset\\nWe fine-tune the EleutherAI/gpt-neo-2.7B11and 6B CarperAIopenai_summarize_tldr_sft12models\\non the TLDR dataset13for the summarization task. For EleutherAI/gpt-neo-2.7B , we first fine-tune it\\nwith supervised fine-tuning on the labeled response in the same summarization dataset, and run RLHF on\\nthe supervised fine-tuned policy. The 6B model CarperAIopenai_summarize_tldr_sft has already gone\\nthrough the supervised fine-tuning stage. The reward model is a pre-trained EleutherAI/gpt-j-6b14reward\\nmodel for summarization dataset CarperAI/openai_summarize_comparisons15.\\nWe follow the default setting in trlX with seed 0 and 100, and plot the results in Figure 5. One can see\\nthatAPAis more sample efficient and provides better KL control than PPO in both 2.7B and 6B models.\\n0 2 4 6 8 10\\nsteps / k2.02.12.22.32.42.52.62.72.8rewardOnline Policy Optimization with 2.7B model\\nPPO\\nAWR\\nAPA\\n0 2 4 6 8\\nsteps / k0.00.51.01.52.02.5KLKL divergence between the trained and the initial policy in 2.7B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k2.42.62.83.03.23.43.6rewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k0.20.30.40.50.60.70.8KLKL divergence between the trained and the initial policy in 6B model\\nPPO\\nAWR\\nAPA\\nFigure 5: Comparisons of the performance on TLDR dataset. Left: The x-axis represents the total steps,\\nwhich are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\n11https://huggingface.co/EleutherAI/gpt-neo-2.7B\\n12https://huggingface.co/CarperAI/openai_summarize_tldr_sft\\n13https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n14https://huggingface.co/EleutherAI/gpt-j-6b\\n15https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n16', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 15}),\n",
       " Document(page_content='C.3 Results on the Dolly Model\\nWe fine-tune the databricks/ dolly-v2-7b16model on the HH dataset. We follow the default setting in\\ntrlX with seed 0 and 100, and plot the results in Figure 6. We only include the results for APAand PPO\\nsince AWR drops directly. Different from all other experiments, here for APAwe set λ= 1rather than 0.1\\nto stablize the training and impose stronger KL control. One can see that APAcan still improve over the\\noriginal dolly 7B model and provide better KL control, while PPO fails to bring further improvement.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nrewardOnline Policy Optimization with Dolly 7B model\\nPPO\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k0.000.020.040.060.080.10KLKL divergence between the trained and the initial policy in dolly 7B model\\nPPO\\nAPA\\nFigure 6: Comparisons of the performance on the dolly 7B model. Left: The x-axis represents the total\\nsteps, which are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\nC.4 StackExchange experiment details\\nThe hyper-parameters for the Fig. 2 are listed in table 1.\\nParameter Value\\nMax sequence length 1024\\nMax output length 128\\nLearning rate 2e-5\\nBatch size 16\\nGradient Accumulation 8\\nSFT LoRA dimension 16\\nRM LoRA dimension 8\\nRL LoRA dimension 16\\nAdaptive KL initial KL coeff (PPO) 0.1\\nAdaptive KL target KL coeff (PPO) 6\\nλ(APA) 0.1\\nTable 1: Hyper-parameters for StackExchange experiments as shown in Fig. 2\\n16https://huggingface.co/databricks/dolly-v2-7b\\n17', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 16}),\n",
       " Document(page_content='D Ablation Studies\\nD.1 KL control in APA\\nIn this section, we show how the performance and KL divergence change with different values of λ. We set\\nλ= 0.1,1for the 125M model and plot their performances in Figure 7 with seed 1000. One can see that the\\nchoice of λdirectly determines the level of KL control, along with the convergent point APAreaches. This\\nshows that λprovides a clear trade-off between KL control and model performance.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nLambda=0.1\\nLambda=1\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.0000.0250.0500.0750.1000.1250.150KLKL divergence between the trained and the initial policy for APA in 125M model\\nLambda = 0.1\\nLambda = 1\\nFigure 7: Comparisons of the performance between different λon the 125M model. Left: The x-axis\\nrepresents the total steps, which are proportional to the number of data used in the training procedure. The\\ny-axis is the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The\\ny-axis is the KL divergence between the trained model and the initial model.\\nD.2 KL control in PPO\\nWe show how the performance and KL divergence change with or without adaptive KL control in PPO. We\\nplot their performances in Figure 8 for 125M model with seed 1000. For PPO with adaptive KL controller,\\nthe initial KL coefficient is set to be 0.05. One can see that without KL control, PPO converges to a higher\\nreward compared to APAin Figure 7, at a cost of a significantly higher KL divergence. On the other hand,\\nthe reward of PPO with adaptive KL control begins to drop in the middle. This is due to the large deviation\\nfrom the original policy, which leads to a much larger KL regularization term that dominates the reward.\\nCompared with Figure 7, one can see that APAprovides more stable and controllable KL regularization.\\nD.3 Experiments for Offline Learning\\nWe conduct experiments for offline learning as well. The offline dataset is selected to be all the prompts and\\nresponses from the HH dataset, with reward labeled by the reward model. We use the trained GPT-J reward\\nfunction to label the reward for all the offline data, and compare ILQL, AWR and APAon the same 125M\\nand 1B model after supervised fine-tuning with seed 1000. The result is given in Figure 9. From the results,\\none can see that AWR performs better than ILQL, and APAcannot be directly adapted to the offline case.\\nFurthermore, offline learning cannot help too much after the supervised fine-tuning stage, potentially due to\\nthe large distribution shift between the offline data and the current policy.\\n18', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 17}),\n",
       " Document(page_content='0 2 4 6 8 10 12 14\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nAdaptive KL Control\\nNo KL Control\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k01234567KLKL divergence between the trained and the initial policy in 125M model\\nAdaptive KL Control\\nNo KL ControlFigure 8: Comparisons of the performance of PPO on the 125M model. Left: The x-axis represents the\\ntotal steps, which are proportional to the number of data used in the training procedure. The y-axis is the\\nreward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the\\nKL divergence between the trained model and the initial model.\\n0 2 4 6 8 10 12\\nsteps / k2.8\\n2.7\\n2.6\\n2.5\\n2.4\\n2.3\\n2.2\\n2.1\\n2.0\\nrewardOffline Policy Optimization with 125M model\\nILQL\\nAWR\\nAPA\\n0 2 4 6 8 10\\nsteps / k2.1\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\nrewardOffline Policy Optimization with 1B model\\nILQL\\nAWR\\nAPA\\nFigure 9: Comparisons of the performance between ILQL, AWR and APA on the offline learning dataset.\\n19', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 18}),\n",
       " Document(page_content='E Proof of Theorem 1\\nProof.From the well-specified assumption π⋆∈ {πθ|θ∈Θ}, we know that there exists some θ⋆∈Θsuch\\nthatπθ⋆=π⋆. For the population loss, we know that\\nLAPA(θ⋆) =E(s,a)∼dπolds,ah\\n(logπθ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n=E(s,a)∼dπolds,ah\\n(logπ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThus for any θ′∈arg minθ∈ΘLAPA(θ), there must be LAPA(θ′) = 0, which is equivalent to\\nE(s,a)∼dπolds,ah\\n(logπθ′(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThis means that for any s, aon the support of dπolds,a, we have πθ′(a|s) =π⋆(a|s).\\nFor the second part of the theorem, we know from Hoeffding’s inequality that for any fixed θ∈Θ,\\n|LAPA(θ)−bLAPA(θ;D)|=\\x0c\\x0c\\x0c\\x0c\\x0c1\\nnnX\\ni=1\\x10\\nlogπθ(ai|si)−Adv(si, ai)/λ−logπinit(ai|si)\\x112\\n−Eh\\x10\\nlogπθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\x0c\\x0c\\x0c\\x0c\\x0c\\n≤C·(B2/λ−2 log( B1))2r\\nlog(1/δ)\\nn.\\nLet the Θϵbeϵ-covering of Θunder ℓ2norm, i.e. for any θ∈Θ, one can find some θ′∈Θϵsuch that\\n∥θ−θ′∥2≤ϵ. We also have |Θϵ| ≤(1/ϵ)d. By taking union bound, we know that for all θ∈Θϵ, with\\nprobability at least 1−δ,\\n|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn. (10)\\nLetˆθbe the minimizer of bLAPA(θ;D). Then we know that there exists some ˆθϵ∈Θϵsuch that ∥ˆθ−ˆθϵ∥ ≤ϵ.\\nThis further implies that\\n|LAPA(ˆθ)− LAPA(ˆθϵ)|\\n=|Eh\\x10\\nlogπˆθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n−Eh\\x10\\nlogπˆθϵ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n|\\n≤C(B2/λ−log(B1))Lϵ. (11)\\nSimilarly, we also have |bLAPA(ˆθ)−bLAPA(ˆθϵ)| ≤C(B2/λ−log(B1))Lϵ.Overall, we have\\nLAPA(ˆθ) = (LAPA(ˆθ)− LAPA(ˆθϵ)) + (LAPA(ˆθϵ)−bLAPA(ˆθϵ)) + (bLAPA(ˆθϵ)−bLAPA(ˆθ)) +bLAPA(ˆθ).\\nFor the first and third difference, from Equation (11) we know that they are both bounded by C(B2/λ−\\nlog(B1))Lϵ.For the second difference, we know from Equation (10) that it is bounded by C(B2/λ−\\nlog(B1))2q\\ndlog(1/(ϵδ))\\nn. Lastly, we know that bLAPA(ˆθ) = 0since ˆθ∈arg minθbLAPA(θ)andbLAPA(θ⋆) = 0.\\nThus overall, we have\\nLAPA(ˆθ)≤C((B2/λ−log(B1))Lϵ+ (B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn).\\nTaking ϵ= 1/(Ln)finishes the proof.\\n20', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./Fine-Tuning_Language_Models.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='form of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human\\nknowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching\\net al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,\\na common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='a common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani\\net al., 2017; Devlin et al., 2018; Brown et al., 2020);\\n•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve\\nthe performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together\\nwith reinforcement learning (RL) algorithms to further align the model with complex and subjective\\nhuman values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).\\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the\\ndistance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;\\nDevlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='Devlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are\\n∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the\\nlanguage model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward\\nmodel produces a scalar score for each generated response, and is, therefore, able to provide the language\\nmodel with online feedback. This accessibility to online feedback allows the language model to be trained\\nvia reinforcement learning (RL), giving rise to the RLHF stage.\\nAmong the RL techniques that are applied to language models, one of the most prominent algorithms\\nis proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO\\n(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample\\nefficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='form of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide\\na theoretical justification supporting the design of our loss function.\\n1 Introduction\\nReinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox\\nand Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including\\ngames (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems\\n(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human\\nknowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching\\net al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,\\na common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='a common protocol is as follows.\\n•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled\\ntext data to produce general features and patterns that can be useful for downstream tasks (Vaswani\\net al., 2017; Devlin et al., 2018; Brown et al., 2020);\\n•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve\\nthe performance and accuracy of the model on specific tasks;\\n•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together\\nwith reinforcement learning (RL) algorithms to further align the model with complex and subjective\\nhuman values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).\\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the\\ndistance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;\\nDevlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='Devlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.\\nAs the ultimate target is to make the language model output conform to human linguistic norms, which are\\n∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of\\nStatistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work\\nwas done at Microsoft when Banghua Zhu was a research intern.\\n1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       " Document(page_content='difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the\\nlanguage model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward\\nmodel produces a scalar score for each generated response, and is, therefore, able to provide the language\\nmodel with online feedback. This accessibility to online feedback allows the language model to be trained\\nvia reinforcement learning (RL), giving rise to the RLHF stage.\\nAmong the RL techniques that are applied to language models, one of the most prominent algorithms\\nis proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO\\n(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample\\nefficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='policies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\\nlonger requires estimating the importance ratio. We show that such differences bring huge benefits in terms\\nof sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='of sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,\\n2022) and the StackExchange Beeching et al. (2023) dataset. We first evaluate using the reward model,\\ntrained on the same dataset to produce a scalar reward for each prompt-response pair. We also evaluate\\nthe human preferences of the resulting language model using GPT-4 to demonstrate the effectiveness of the\\nalgorithm. Our empirical results highlight three major advantages of APAoverPPO:\\n(i)APAis more sample-efficient . Fine-tuned on the same number of samples, the language model\\nobtained via APAscores consistently higher on the evaluation set than the one obtained with PPO.\\n(ii)APAaffords steadier control over the deviation from the language model’s initial policy .\\nMeasured by KLdivergence, the deviation of the ultimate policy generated by APAis comparable\\nwith that of PPO, yetAPAis less prone to sudden performance degradation during training, which\\nis occasionally observed in PPO. Note that previous study has shown that the control over deviations', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='Measured by KLdivergence, the deviation of the ultimate policy generated by APAis comparable\\nwith that of PPO, yetAPAis less prone to sudden performance degradation during training, which\\nis occasionally observed in PPO. Note that previous study has shown that the control over deviations\\nfrom the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).\\n(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable\\nparameter for KL control, whereas in PPOone has to carefully calibrate the combination of various\\nextra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the\\ncoefficients of the KL controller.\\nMore broadly, this work is related to the line of literature on leveraging ideas from RL to improve the\\nperformance of language models. A few notable examples in this literature include Paulus et al. (2017),\\nwho propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='who propose a loss function based on the policy gradient objective to tackle the abstractive summarization\\ntask, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning\\n(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A\\nthorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE\\nbenchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best\\ngenerated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).\\nThe remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section\\n3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.\\nExperimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the\\nexperimental results.\\n2', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       " Document(page_content='2 Preliminaries\\nIn this section, we overview the standard RL setting in Section 2.1, and discuss how language model training\\nfits into this setting in Section 2.2. We use the following notation. For a positive integer n, we will use\\nthe bracket notation [n]to refer to the set of integers {1, . . . , n }; for a finite set Z, we denote by ∆(Z)the\\nset of probability distributions on Z, and |Z|the cardinality of Z. We use Bdto denote the unit ball in\\nd-dimensional space.\\n2.1 Reinforcement Learning\\nReinforcementlearning(RL)capturestheinteractionbetweenanagentandanenvironmentviatheformalism\\nof a Markov decision process (MDP). We consider a finite-horizon MDP represented by a tuple M=\\n(S,A, H, P, r, ρ ), where Sis a finite state space, Ais a finite action space, His the horizon, P:S×A 7→ ∆(S)\\nis a probability transition matrix, r:S × A 7→ [0,1]is a reward function, and ρ:S 7→ ∆(S)is the initial\\nstate distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='state distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive\\nsteps. At the end of an episode, the agent is reset to a state drawn from ρ(·), and a new episode begins.\\nA policy π:S 7→ ∆(A)is a function that maps a state to a distribution over actions. The value function\\nVπ:S 7→Rof policy πis defined as the expected sum of discounted rewards when the agent starts from\\ninitial state sand follows policy πthroughout the episode. Let γ∈[0,1]be the discount factor. For any\\ns∈ S, we have\\nVπ(s):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nGiven a policy π, the state-action value function, also known as the Q-function, can be defined analogously.\\nFor state s∈ Sanda∈ A, we have\\nQπ(s, a):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action\\na, when the agent is in state sat step h.\\nWe also define the occupancy measures dπ\\nstate:S 7→ [0,1]anddπ\\naction :S × A 7→ [0,1]as\\ndπ\\nstate(s):=1\\nHHX\\nh=1P(sh=s|π)and dπ\\naction (s, a):=1\\nHHX\\nh=1P(sh=s, ah=a|π),\\nwhere P(· |π)signifies that all actions are drawn from π. To avoid clutter, we overload the notation dπsuch\\nthatdπ(s)refers to dπ\\nstate(s), and dπ(s, a)refers to dπ\\naction (s, a).\\n2.2 Language Model as Reinforcement Learning Agent\\nIn its simplest form, a language model receives as input a sequence of tokens (x1, . . . , x n), and generates a\\ndistribution over the next token xn+1. All tokens lie in a finite set X. Whenever the agent selects a token\\nthat represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens\\nreaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='that represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens\\nreaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.\\nComparing with the RL formulation in Section 2.1, a language model can be viewed as an agent that\\noperatesin anenvironmentwithstate space S=SH\\nk=0Xkandaction space A=X, where Histhe maximum\\nnumber of tokens. The transitions are always deterministic, with the next state equal to the concatenation\\nof all the previous tokens and the current token P(sh+1= (x1,···, xk)|sh= (x1,···, xk−1), ah=xk) = 1.\\n3', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       " Document(page_content='Traditionally, each episode involves the generation of one complete sequence, and a reward is delivered only\\nwhen an episode terminates. In this context, fine-tuning is equivalent to improving the agent policy π. The\\nfield of RL offers a formidable arsenal for this task. In this work, we will focus on policy-based RL algorithms,\\nwhich parameterize the set of agent policies by a set of parameters θand optimize in the parameter space.\\nIn what follows, we will omit the step index h, as its information is already encoded in each state.\\nWe note that most transformer-based language models map a state (context) sand an action (next\\ntoken) ato a logit qθ(s, a), and the next token is sampled according to the distribution induced by the logits\\n{qθ(s, a)}a∈A. This naturally gives rise to the following parameterization of language model policy:\\nπθ(a|s) =exp(qθ(s, a))P\\na∈Aexp(qθ(s, a)).\\n3 Fine-Tuning Based on Reinforcement Learning\\nAs is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is\\ntrained from a human-labeled dataset. An RL algorithm is then applied to improve the language model', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='πθ(a|s) =exp(qθ(s, a))P\\na∈Aexp(qθ(s, a)).\\n3 Fine-Tuning Based on Reinforcement Learning\\nAs is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is\\ntrained from a human-labeled dataset. An RL algorithm is then applied to improve the language model\\npolicy, using the rewards generated by the reward model. Here we focus mainly on the second step with a\\ngiven reward function.\\nWe summarize a typical policy-based RL algorithm in Algorithm 1. In practice, the parameter update\\nin Equation (1) usually involves several gradient steps rather than a full minimization.\\nAlgorithm 1 Policy Gradient\\n1:Input:An initial policy parameter θ0, a given loss function L(θ;D).\\n2:Setπ0=πinit.\\n3:Foriteration t= 1,2···, T\\n4:Roll out πθt−1to produce dataset Dt=n\\n(s(t)\\n1, a(t)\\n1, r(t)\\n1),···,(s(t)\\nn, a(t)\\nn, r(t)\\nn)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='n)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.\\nAs a first step, for each fixed state s, we consider the following KL-regularized optimization problem as\\na target of policy improvement:\\nmaximize\\nθF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL\\x10\\nπθ(· |s)\\r\\rπinit(· |s)\\x11\\n. (2)\\nHere πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy\\nthat we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,\\nand to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state\\ns. The second term in F(θ;s, π), aKLregularizer, controls the deviation of πθfrom πinit. Such regularization\\nis essential, as language models are prone to over-optimization when rewards are generated by an imperfect\\nreward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='is essential, as language models are prone to over-optimization when rewards are generated by an imperfect\\nreward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem\\nin (2) aims at improving upon policy πin state swithin the proximity of πinit.\\nThe optimization (2) is usually broken down into multiple iterations. In each iteration, we maximize\\nF(θ;s, πold), where πoldis the policy that the agent arrives at in the previous iteration. This technique,\\nreferred to as Conservative Policy Iteration (CPI) , was first presented in Kakade and Langford (2002).\\nThe optimization was subsequently generalized to KL-constrained and regularized methods referred to as\\nTrust Region Policy Optimization (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimization\\n(PPO) (Schulman et al., 2017), respectively. In addition to these core methods, there have been several\\n4', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3}),\n",
       " Document(page_content='other policy optimization methods inspired by (2), with one notable example being the Advantage-Weighted\\nRegression (AWR) method (Peng et al., 2019; Nair et al., 2020).\\nIn the following subsection, we will discuss how F(θ;s, π)is connected with the loss function L(θ;D)in\\nvarious algorithms, and propose a new proximal optimization problem whose solution approximates that of\\n(2). The loss function in APAwill be based on this new proximal optimization problem.\\n3.1 Proximal policy optimization\\nPPO leverages importance sampling to circumvent sampling from πθ, arriving at\\nEa∼πθ(·|s)[Advπold(s, a)] =Ea∼πold(·|s)\\x14πθ(a|s)\\nπold(a|s)Advπold(s, a)\\x15\\n,\\nwhere the expectation on the right-hand side can be estimated in an unbiased manner from finite samples.\\nPPO also involves the following innovation: Instead of penalizing the expected advantage with the\\nestimated KL-divergence as in (2), PPO directly subtracts the KL penalty term from the reward received\\nby the agent. And one may also adaptively adjust the penalty weight λbased on the deviation of πθfrom\\nπinit(Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). The KL-penalized reward is then', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 4}),\n",
       " Document(page_content='by the agent. And one may also adaptively adjust the penalty weight λbased on the deviation of πθfrom\\nπinit(Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). The KL-penalized reward is then\\nused to estimate a new advantage function dAdv. To avoid ill-conditioned gradients caused by large values or\\nimportance ratio estimates, PPO applies clipping to the objective function. The final loss function is thus\\nLPPO(θ;D) =−1\\n|D|X\\n(s,a)∈Dmin\\x1aπθ(a|s)\\nπold(a|s)dAdv(s, a),clip\\x12πθ(a|s)\\nπold(a|s),1−ϵ,1 +ϵ\\x13\\ndAdv(s, a)\\x1b\\n.\\nNote that the loss function relies on extra tunable hyperparameters. The clipping also makes the estimator\\nbiased.\\n3.2 Advantage weighted regression\\nIf the parameterized policy space {πθ}contained all possible policies including the ground truth policy, the\\nmaximizer of F(θ;s, πold)(2) would induce a policy π⋆that satisfies\\nπ⋆(a|s) =1\\nZ(s)πinit(a|s)·exp(Advπold(s, a)/λ), (3)\\nwhere Z(s) =P\\na′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ)is a normalizing factor. In the case that {πθ}does\\nnot contain all policies, a natural way to maximize F(θ;s, πold)is to project π⋆to{πθ}with respect to\\nKL-divergence. From (3),\\nKL\\x00\\nπ⋆(a|s)∥πθ(a|s)\\x01\\n=−πinit(a|s)\\nZ(s)exp\\x12Advπold(s, a)\\nλ\\x13\\nlog\\x00', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 4}),\n",
       " Document(page_content='where Z(s) =P\\na′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ)is a normalizing factor. In the case that {πθ}does\\nnot contain all policies, a natural way to maximize F(θ;s, πold)is to project π⋆to{πθ}with respect to\\nKL-divergence. From (3),\\nKL\\x00\\nπ⋆(a|s)∥πθ(a|s)\\x01\\n=−πinit(a|s)\\nZ(s)exp\\x12Advπold(s, a)\\nλ\\x13\\nlog\\x00\\nπθ(a|s)\\x01\\n+C(s), (4)\\nwhere C(s)is a constant that does not depend on θ.\\nTo facilitate online update, AWR makes three changes from Equation (4):\\n•AWR replaces the first-round policy πinitwith the previous-round policy πold. This ensures that one\\ncan utilize the new roll-out samples from previous-round policy to approximate (4).\\n•The KL-divergence in (4) only accounts for one state s. AWR minimizes a distribution of states dπold.\\n•AWR approximates Z(s)≈1. We also provide a related discussion in Appendix A on why such an\\napproximation is warranted.\\nThese changes lead to the loss function introduced in AWR:\\nLAWR(θ) =−E(s,a)∼dπoldh\\nexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01i\\n. (5)\\n5', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 4}),\n",
       " Document(page_content='Given a finite dataset D={(si, ai) :i= 1, . . . , n }sampled from dπold, the corresponding empirical loss\\ncan be written as\\nLAWR(θ;D) =−1\\n|D|X\\n(s,a)∈Dexp\\x00\\nAdvπold(s, a)/λ\\x01\\nlog\\x00\\nπθ(a|s)\\x01\\n. (6)\\nFor the well-specified case where the parameterized family {πθ}contains the minimizing policy, the\\nminimizer of the population loss is as follows:\\nπ′(a|s) =πold(a|s) exp( Advπold(s, a)/λ)P\\naπold(a|s) exp( Advπold(s, a)/λ). (7)\\nDue to the discrepancies between the original target in Equation (4) and the final loss in Equation (5),\\none can see that the policy AWR converges to is different from the the original target in Equation (3).\\nFurthermore, since πoldchanges in each round, the policy it converges to continues to change.\\nAs we observe in Section 4 and Appendix D.3, AWR can be unstable in the online case due to this reason.\\nThis motivates us to introduce APA, which alleviates this issue, provably converges to the right target in\\nEquation (3), and demonstrates great empirical performance.\\n3.3 Advantage-Induced Policy Alignment\\nTo project the optimal policy π⋆in (3) onto the parameterized policy space, we consider another distance', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 5}),\n",
       " Document(page_content='Equation (3), and demonstrates great empirical performance.\\n3.3 Advantage-Induced Policy Alignment\\nTo project the optimal policy π⋆in (3) onto the parameterized policy space, we consider another distance\\ninstead of KL-divergence. In APA, we employ the squared error between log probabilities in place of the\\nKL-divergence:\\n\\x00\\nlogπ⋆(a|s)−logπθ(a|s)\\x012=\\x10\\nlogπθ(a|s) + log Z(s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n.\\nSimilar to the approximation in AWR, we also apply Z(s)≈1, and minimize the expected loss under a state\\ndistribution dπoldin each round, giving rise to the following population loss:\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112i\\n. (8)\\nThe empirical loss on a finite dataset Dsampled from dπoldis thus\\nLAPA(θ;D) =1\\n|D|X\\n(s,a)∈D\\x10\\nlogπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)\\x112\\n. (9)\\nAssuming that the parameter space is Θ =Bdand that the parameterized policy space is well-specified\\nsuch that π⋆∈ {πθ|θ∈Θ}, where π⋆is defined in Equation (3), we can establish theoretically that the\\nempirical loss is a reasonable surrogate for the population loss.\\nTheorem 1. Letθ⋆∈arg minθ∈ΘLAPA(θ)be a minimizer of the population loss. Then\\nπθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 5}),\n",
       " Document(page_content='such that π⋆∈ {πθ|θ∈Θ}, where π⋆is defined in Equation (3), we can establish theoretically that the\\nempirical loss is a reasonable surrogate for the population loss.\\nTheorem 1. Letθ⋆∈arg minθ∈ΘLAPA(θ)be a minimizer of the population loss. Then\\nπθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).\\nFurthermore, let ˆθ∈arg minθ∈ΘLAPA(θ,D)be an empirical loss minimizer. Assume that min(πθ(a|\\ns), πinit(a|s))≥B1and|Adv(s, a)| ≤B2for any s, a, and that log(πθ)isL-Lipschitz with respect to θ\\nunder ℓ2-norm for any s, a. Then for all δ >0, with probability at least 1−δ, for some universal constant\\nC,\\nLAPA(ˆθ)≤CL(B2−log(B1))2r\\ndlog(nL/δ )\\nn.\\nThe proof is deferred to Appendix E. From the theorem, we see that the minimizer of the population\\nAPAloss is exactly the target policy π⋆if the policy πoldis supported on all state-action pairs. In contrast, as\\nwe discussed earlier, convergence properties of the PPO and AWR algorithms have not yet been established.\\nWe also provide alternative interpretations of the proposed loss in terms of f-divergence and soft-Q\\nlearning in Appendix B.\\n6', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 5}),\n",
       " Document(page_content='4 Experimental Results\\nIn our implementation of all of the algorithms that we test, including APA, we define the advantage function\\ntobe Advπold(s, a), whichisestimatedfromdata. Weusethesamegeneralizedadvantageestimationapproach\\nto estimate the advantage as discussed in earlier work Mnih et al. (2016); Schulman et al. (2015b). In\\nparticular, for the rollout (s0, a0, r0, s1, a1, r1,···, sT−1, aT−1, rT−1, sT), the generalized advantage estimator\\nis\\nˆAπold(st, at) =δt+λγδt+1+···+ (λγ)T−1δT−1,\\nwhere δt=r(st, at) +γVπold(st+1)−Vπold(st).\\nHere the value function is another standalone network that we fit throughout the training process with a\\nsquared loss, ˆLV(D) =P\\nsi,ai(V(si)−ˆAπold(si, ai)−Vπold(si))2. Thus the overall loss function is\\nLAPA\\nθ(D) =ˆLAPA(D) +η·ˆLV(D)\\nLAWR\\nθ(D) =ˆLAWR(D) +η·ˆLV(D).\\nFor the implementation of PPO, we use the PPO2 version from Dhariwal et al. (2017), with the adaptive\\nKL controller from Ziegler et al. (2019). We implement PPO with the same hyperparameters as the\\nimplementation in trlX1, which also follows default hyperparameters suggested by Schulman et al. (2017).', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='KL controller from Ziegler et al. (2019). We implement PPO with the same hyperparameters as the\\nimplementation in trlX1, which also follows default hyperparameters suggested by Schulman et al. (2017).\\nThe main difference between our version of PPO and that in trlXis that we create a completely separate\\nvalue network rather than creating a value head on top of the language model. In APA, we take λ= 0.1\\nto impose a weaker constraint on the KL coefficient. For AWR, we find that setting λ= 0.1leads to an\\nexplosion of the loss; thus we take λ= 1to stabilize training.\\n4.1 Results on the StackExchange dataset\\nIn this section, we present our experimental results with StackExchange dataset. This dataset includes\\nquestions and their corresponding answers from the StackExchange platform (including StackOverflow for\\ncode and many other topics). The answers are then voted by the users on the platform and an accepted\\nanswer is labeled. Following Beeching et al. (2023); Askell et al. (2021), we assign a score to each answer\\ndepending on the number of upvotes:\\nscore =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3−1, upvotes ≤0,\\n1 +⌊log2(1 +upvotes ) + 0.5⌋,if the questioner accepted the answer ,\\n⌊log2(1 +upvotes ) + 0.5⌋,otherwise .', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='answer is labeled. Following Beeching et al. (2023); Askell et al. (2021), we assign a score to each answer\\ndepending on the number of upvotes:\\nscore =\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3−1, upvotes ≤0,\\n1 +⌊log2(1 +upvotes ) + 0.5⌋,if the questioner accepted the answer ,\\n⌊log2(1 +upvotes ) + 0.5⌋,otherwise .\\nFigure 1: Win rates computed by GPT-4 for\\nStackExchange dataset for models trained by SFT,\\nPPO and APA. Compared to SFT and PPO, APA\\ntrained models generated better responses.We used the pre-processed dataset provided in\\nBeeching et al. (2023) for all SFT, reward\\nmodeling and RL training purposes, available\\nintheHuggingFaceDatasetsas lvwerra/stack-exchange-paired2\\n. We use LLaMA-7B Touvron et al. (2023) models\\nfor this experiment. We use Low-Rank Adaptation\\n(LoRA) method Hu et al. (2021) to reduce the\\nmemory consumption while training. We used\\n8xA100 GPUs for our experiments. The hyper-\\nparameters for this experiment are listed in the\\nAppendix C.4. Fig. 2 shows the reward on\\nthe left and KL divergence from the initial policy\\nfor the three algorithms, PPO, APA and AWR.\\nWe adjust the hyper-parameters to achieve similar\\nKL divergence values, allowing us to compare the\\n1https://github.com/CarperAI/trlx', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='Appendix C.4. Fig. 2 shows the reward on\\nthe left and KL divergence from the initial policy\\nfor the three algorithms, PPO, APA and AWR.\\nWe adjust the hyper-parameters to achieve similar\\nKL divergence values, allowing us to compare the\\n1https://github.com/CarperAI/trlx\\n2https://huggingface.co/datasets/lvwerra/stack-exchange-paired\\n7', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 6}),\n",
       " Document(page_content='rewards for various algorithms. In the case of AWR,\\neach hyper-parameter set displayed some level of\\ninstability. Clearly, APA quickly converges to a\\nhigher reward than PPO and AWR.\\nFigure 2: Comparison of the performance of three methods on the StackExchange dataset. Left: The x-axis\\nrepresents the total steps, which are proportional to the amount of data used in the training procedure. The\\ny-axis is the reward computed by the same reward model during training. Right: The x-axis represents the\\ntotal steps. The y-axis is the KL divergence between the trained model and the initial model.\\nGPT-4 Evaluation: We conduct a GPT-4 evaluation to evaluate the models trained by different RL\\nmethods on StackExchange dataset. GPT-4 compares the outputs produced by two models, using a reference\\n(chosen) response as a basis for comparison. Fig. 1 shows the win-rates for comparing SFT vs PPO, SFT\\nvs APA and PPO vs APA models. APA consistently outperforms the other two models.\\n4.2 Results on the HH dataset\\nIn this section, we compare PPO, AWR and APAon the human-labeled Helpfulness and Harmlessnes (HH)', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 7}),\n",
       " Document(page_content='vs APA and PPO vs APA models. APA consistently outperforms the other two models.\\n4.2 Results on the HH dataset\\nIn this section, we compare PPO, AWR and APAon the human-labeled Helpfulness and Harmlessnes (HH)\\ndataset from Bai et al. (2022a).3. We fine tune three models, including Dahoas/pythia-125M-static-sft ,4\\nDahoas/pythia-1B-static-sft ,5andDahoas/pythia-6B-static-sft .6All the models have gone through\\nsupervised fine-tuning with labeled prompt-response pairs, similar to the protocol in Ouyang et al. (2022)\\nand Ramamurthy et al. (2022). We present the performance of the RL algorithms for pythia-125M and\\npythia-1B in Fig. 3. It shows that after some steps, PPO’s performance begins to deteriorate while APA\\nand AWR are stable. APA achieves the higher reward while maintaining KL divergence from the initial\\npolicy smaller. The details about hyper-parameters used for training and additional results for larger models\\nare discussed in Appendix C.1.\\n5 Conclusions\\nInthispaper, westudytheproblemofonlinepolicyoptimizationinRLHF.Webenchmarktheperformanceof\\nexisting algorithms PPOandAWR, and introduce a new method, APA, which has a theoretical convergence', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 7}),\n",
       " Document(page_content='are discussed in Appendix C.1.\\n5 Conclusions\\nInthispaper, westudytheproblemofonlinepolicyoptimizationinRLHF.Webenchmarktheperformanceof\\nexisting algorithms PPOandAWR, and introduce a new method, APA, which has a theoretical convergence\\nguarantee and affords several advantages over existing algorithms. The key takeaways from our study can\\nbe summarized as follows.\\n3https://huggingface.co/datasets/Dahoas/static-hh\\n4https://huggingface.co/Dahoas/pythia-125M-static-sft\\n5https://huggingface.co/Dahoas/pythia-1B-static-sft\\n6https://huggingface.co/Dahoas/pythia-6B-static-sft\\n8', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 7}),\n",
       " Document(page_content='0 2500 5000 7500 10000 12500 15000 17500 20000\\nsteps3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nPPO\\nAWR\\nAPA\\n2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.51.01.52.02.53.03.5KLKL divergence between the trained and the initial policy for 125M model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k2.0\\n1.8\\n1.6\\n1.4\\n1.2\\n1.0\\n0.8\\n0.6\\nrewardOnline Policy Optimization with 1B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.01.21.4KLKL divergence between the trained and the initial policy for 1B model\\nPPO\\nAWR\\nAPAFigure 3: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\n9', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 8}),\n",
       " Document(page_content='Stability. As we discussed in Section 1, one of the challenges in RLHF is instability. It is crucial in\\nRL algorithms to impose control on the divergence between new policies and the initial policy after SFT.\\nHowever, the clipping of the objective function and the adaptive KL controller can make the behavior of\\nPPO unstable; for AWR, the update in (7), which reweighs the previous policy by a multiplicative factor\\nin each iteration, also has unknown ramifications. APA, on the other hand, provably converges to π⋆when\\nthe advantage function is fixed, which is close to the initial policy in KL divergence. From the experimental\\nresults, we see that APAis able to provide better and easy-to-adjust KL control by explicitly tuning the\\nhyperparameter λ. Our experiments reveal different levels of instability for PPO and AWR. Specifically,\\nPPO suffers from occasional performance degradation whenever the model policy diverges too much from\\nthe initial policy πinit, and such effect is more pronounced for smaller models. We attribute this to the KL\\ncontroller in PPO. In Appendix D, we demonstrate that PPO can achieve a similar sample efficiency as', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 9}),\n",
       " Document(page_content='the initial policy πinit, and such effect is more pronounced for smaller models. We attribute this to the KL\\ncontroller in PPO. In Appendix D, we demonstrate that PPO can achieve a similar sample efficiency as\\nAPAwithout the KL penalty, albeit at the cost of weaker KL efficiency.\\nSampleefficiency. WiththesamelevelofcontroloverKL-divergence, APAshowshighersampleefficiency\\nthan PPO and AWR. One possible explanation is that in both PPO and AWR, policy improvement critically\\ndepends on using finite samples to reconstruct the sampling policy πold, whereas in APA, minimizing the\\npopulation loss (8) hinges less on the reconstruction of πold. In fact, the APApopulation loss (8) can be\\neffectivelyminimizedaslongasthedataset Dhasadecentcoverageoverstate-actionpairsthatarefrequently\\nvisited by πold. For more discussions on sample efficiency, please refer to Appendix B.\\nOnline vs. offline learning. Our experiments primarily examine the online case, where new data can be\\ngenerated during the training process. The offline setting, where a fixed dataset is given and new samples are\\nnot available, may yield qualitatively different results. In particular, suppose that the offline dataset consists', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 9}),\n",
       " Document(page_content='generated during the training process. The offline setting, where a fixed dataset is given and new samples are\\nnot available, may yield qualitatively different results. In particular, suppose that the offline dataset consists\\nof rollouts from a policy πoff. In this case, if it were trained with infinitely many samples, AWR would\\nconverge to the policy specified in (3). However, the performance of APAmay suffer from distribution shift\\nbecause it can only learn from state-action pairs covered by πoff, and there is no guarantee that the learned\\npolicy performs well on the state-action pairs visited by the current policy. Such distribution mismatch can\\nlead to a significant performance drop for APA, as we observe in Appendix D.3. We also observe that AWR\\ntypically outperforms ILQL for offline learning, although both perform poorly with larger models.\\n10', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 9}),\n",
       " Document(page_content='References\\nA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,\\nN. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,\\nJ. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for\\nalignment, 2021. 4.1\\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,\\net al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\\npreprint arXiv:2204.05862 , 2022a. 4.2\\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\nC. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 ,\\n2022b. 1\\nE. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert. StackLLaMA:\\nAn RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://\\nhuggingface.co/blog/stackllama . 1, 4.1, 4.1\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='An RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://\\nhuggingface.co/blog/stackllama . 1, 4.1, 4.1\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\nA. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 1\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from\\nhuman preferences. In Advances in Neural Information Processing Systems , pages 4299–4307, 2017. 1\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\\nfor language understanding. arXiv preprint arXiv:1810.04805 , 2018. 1\\nP. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and\\nP. Zhokhov. OpenAI baselines. https://github.com/openai/baselines , 2017. 3.1, 4\\nS. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning\\nwith agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1\\nD.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='S. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning\\nwith agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1\\nD.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,\\net al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.\\narXiv preprint arXiv:2209.07858 , 2022. 1\\nL. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint\\narXiv:2210.10760 , 2022. (ii), 3\\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank\\nadaptation of large language models, 2021. 4.1\\nS. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of\\nthe Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 3\\nW. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE\\nInternational Conference on Development and Learning , pages 292–297. IEEE, 2008. 1\\nA. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='W. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE\\nInternational Conference on Development and Learning , pages 292–297. IEEE, 2008. 1\\nA. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human\\nfeedback. In Robotics research , pages 161–176. Springer, 2018. 1\\nJ. Maghakian, P. Mineiro, K. Panaganti, M. Rucker, A. Saran, and C. Tan. Personalized reward learning\\nwith interaction-grounded learning (IGL). arXiv preprint arXiv:2211.15823 , 2022. 1\\nV. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.\\nAsynchronous methods for deep reinforcement learning. In International Conference on Machine Learning ,\\npages 1928–1937. PMLR, 2016. 4\\n11', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 10}),\n",
       " Document(page_content='A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with offline\\ndatasets. arXiv preprint arXiv:2006.09359 , 2020. 3\\nR.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.\\nWebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 ,\\n2021. 1\\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\\nA. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint\\narXiv:2203.02155 , 2022. 1, 4.2\\nR. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint\\narXiv:1705.04304 , 2017. 1\\nX. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable\\noff-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:\\nYour language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:\\nYour language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1\\nR. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi.\\nIsreinforcementlearning(not)fornaturallanguageprocessing? benchmarks,baselines,andbuildingblocks\\nfor natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022. 1, 4.2\\nD. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.\\nInRobotics: Science and Systems , 2017. 1\\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In\\nInternational Conference on Machine Learning , pages 1889–1897. PMLR, 2015a. 3\\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\\ngeneralized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using\\ngeneralized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4\\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.\\narXiv preprint arXiv:1707.06347 , 2017. 1, 3, 3.1, 4\\nC. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline RL for natural language generation with\\nimplicit language Q-learning. arXiv preprint arXiv:2206.11871 , 2022. 1\\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.\\nLearning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:\\n3008–3021, 2020. 1\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃšre, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient\\nfoundation language models, 2023. 4.1\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\\nAttention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='foundation language models, 2023. 4.1\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\\nAttention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1\\nC. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz. A survey of preference-based reinforcement learning\\nmethods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017. 1\\nR. Yuan, R. M. Gower, and A. Lazaric. A general sample complexity analysis of vanilla policy gradient.\\nProceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.\\n1\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language\\nmodels with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023. 1\\n12', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 11}),\n",
       " Document(page_content='B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or\\nk-wise comparisons. International Conference on Machine Learning , 2023. 1\\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.\\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. 1, 3.1, 4\\n13', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 12}),\n",
       " Document(page_content='A Argument for Z(s)≈1\\nNote that in both advantage-weighted regression and advantage-based squared loss, we approximate Z(s)\\nwith 1. Here we justify why this does not hurt the performance.\\nConsider an infinitesimal scenario where |Adv/λ| ≪ | logπinit|. In the scenario of language model, this\\nis usually true since πinitis supported on approximately 50kdistinct tokens and can be very close to zero,\\nwhile Adv/λcan be adjusted to small numbers by adjusting λ.\\nIn this case, we have\\nZ(s) =X\\na∈Aπinit(a|s) exp( Adv(s, a)/λ)\\n=Ea∼πinit[exp( Adv(s, a)/λ)]\\n=Ea∼πinit[1 +Adv(s, a)/λ+o(Adv2(s, a)/λ2)].\\nThis advantage is usually estimated as Advπold, which can be close to Advπinit. And we have\\nEa∼πinit[Advπold(s, a)/λ]≈Ea∼πinit[Advπinit(s, a)/λ] = 0.\\nThus we know that\\nZ(s)≈1 +Ea∼πinit[o(Adv2(s, a)/λ2)]≈1.\\nIn practice, we observe that the squared loss decreases very slowly due to a small learning rate ( 8e−6).\\nThis suggests that the policy changes very slowly, which is another reason why the normalizing factor is not\\nimportant.\\nB Alternative Interpretation of APA\\nRecall that APAcan be written as\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−logπ⋆(a|s)\\x112i\\n,', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 13}),\n",
       " Document(page_content='This suggests that the policy changes very slowly, which is another reason why the normalizing factor is not\\nimportant.\\nB Alternative Interpretation of APA\\nRecall that APAcan be written as\\nLAPA(θ) =E(s,a)∼dπoldh\\x10\\nlogπθ(a|s)−logπ⋆(a|s)\\x112i\\n,\\nwhere π⋆=πinit·exp(Adv/λ). In the case when πoldis close to πθ, minimizing the squared loss in APAis\\nequivalent to minimizing the following distance between π⋆andπθ:\\nd(π⋆(· |s), πθ(· |s)) =X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132\\n.\\nThis can be viewed as a new f-divergence with f(x) =xlog2(x). We can show by Cauchy-Schwarz that this\\nis always a upper bound for the KL divergence:\\nd(π⋆(· |s), πθ(· |s)) = X\\naπθ(a|s)! X\\naπθ(a|s)\\x12\\nlog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x132!\\n≥X\\naπθ(a|s)\\x0c\\x0c\\x0c\\x0clog\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\x0c\\x0c\\x0c\\x0c\\n≥X\\naπθ(a|s) log\\x12πθ(a|s)\\nπ⋆(a|s)\\x13\\n.\\nC Additional Experiments\\nC.1 Results on the HH dataset\\nIn this dataset, each item is comprised of a prompt, a chosen response and a rejected response labeled by\\nhuman to evaluate the helpfulness and harmlessness of the responses. For the reward model, we use the\\n14', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 13}),\n",
       " Document(page_content='proxy reward model Dahoas/gptj-rm-static7with 6B parameters trained from the same dataset based on\\nEleutherAI/gpt-j-6b .8For all three algorithms, we run two epochs of update after generating 64 responses\\nfrom randomly sampled prompts. For the 125M model, we use batch size 8and learning rate 8×10−6. For\\nthe 1B model, we use batch size 2and learning rate 10−6. For the 6B and larger models, we use batch size\\n1and learning rate 10−6. We use a 32GB Nvidia V100 GPU for fine-tuning 125M and 1B models, and a\\n64GB AMD Mi200 GPU for fine-tuning the 6B and larger models. The maximum response length is set to\\nbe 128 tokens, and the maximum total sequence length is set to be 1024 tokens. We unfreeze the last two\\nlayers during fine-tuning. For each experiment, we run 20k steps in total. The results are plotted as below.\\nIn the left of Figure 4, we compare the three methods on the HH dataset. For all three models, we repeat\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k2.4\\n2.2\\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\nrewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.0KLKL divergence between the trained and the initial policy for 6B model\\nPPO\\nAWR', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k2.4\\n2.2\\n2.0\\n1.8\\n1.6\\n1.4\\n1.2\\nrewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.00.20.40.60.81.0KLKL divergence between the trained and the initial policy for 6B model\\nPPO\\nAWR\\nAPA\\nFigure 4: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents\\nthe total steps, which are proportional to the amount of data used in the training procedure. The y-axis is\\nthe reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is\\nthe KL divergence between the trained model and the initial model.\\nthe experiments with three random seeds 0,100,1000, and plot their min, mean and max. We see that with\\nthe same amount of data, APAis able to achieve the highest reward in all three cases. We also observe that\\nPPO becomes more stable with large models, potentially due to smaller batch size, or the ability of getting\\nhigher reward with a smaller deviation in KL divergence.\\nOn the right of Figure 4, we show how the KL divergence between the current policy and the initial', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='PPO becomes more stable with large models, potentially due to smaller batch size, or the ability of getting\\nhigher reward with a smaller deviation in KL divergence.\\nOn the right of Figure 4, we show how the KL divergence between the current policy and the initial\\npolicy changes as a function of the training process for the three seeds. We can see that for all three models,\\nAPAprovides similar or better KL control than PPO and AWR, although we note that for the 6B model the\\nKL control for PPO is slightly better than APA. Combined with the left part of the figure, we can see that\\nAPAis more KL-efficient than PPO and AWR; i.e., it attains a better performance on the reward model\\nunder the same KL divergence.\\nWe include more experiment results in Appendix C, where we fine tune databricks/dolly-v2-7b9on\\nthe same HH dataset, and 2.7B and 6B models on the TLDR dataset10for the summarization task.\\nWe also conduct ablation studies on the effect of the adaptive KL controller on PPO and the effect of\\ndifferent choices of λfor AWR; see Appendix D. We show in Appendix D.2 that without KL control, PPO', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='the same HH dataset, and 2.7B and 6B models on the TLDR dataset10for the summarization task.\\nWe also conduct ablation studies on the effect of the adaptive KL controller on PPO and the effect of\\ndifferent choices of λfor AWR; see Appendix D. We show in Appendix D.2 that without KL control, PPO\\ncan be as sample efficient as APA, but less KL-efficient. We also observe instability even without the KL\\ncontroller. On the other hand, we observe that changing λprovides a straightforward tradeoff between KL\\ncontrol and performance in APA.\\n7https://huggingface.co/Dahoas/gptj-rm-static\\n8https://huggingface.co/EleutherAI/gpt-j-6b\\n9https://huggingface.co/databricks/dolly-v2-7b\\n10https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n15', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 14}),\n",
       " Document(page_content='C.2 Results on the TLDR Dataset\\nWe fine-tune the EleutherAI/gpt-neo-2.7B11and 6B CarperAIopenai_summarize_tldr_sft12models\\non the TLDR dataset13for the summarization task. For EleutherAI/gpt-neo-2.7B , we first fine-tune it\\nwith supervised fine-tuning on the labeled response in the same summarization dataset, and run RLHF on\\nthe supervised fine-tuned policy. The 6B model CarperAIopenai_summarize_tldr_sft has already gone\\nthrough the supervised fine-tuning stage. The reward model is a pre-trained EleutherAI/gpt-j-6b14reward\\nmodel for summarization dataset CarperAI/openai_summarize_comparisons15.\\nWe follow the default setting in trlX with seed 0 and 100, and plot the results in Figure 5. One can see\\nthatAPAis more sample efficient and provides better KL control than PPO in both 2.7B and 6B models.\\n0 2 4 6 8 10\\nsteps / k2.02.12.22.32.42.52.62.72.8rewardOnline Policy Optimization with 2.7B model\\nPPO\\nAWR\\nAPA\\n0 2 4 6 8\\nsteps / k0.00.51.01.52.02.5KLKL divergence between the trained and the initial policy in 2.7B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k2.42.62.83.03.23.43.6rewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 15}),\n",
       " Document(page_content='PPO\\nAWR\\nAPA\\n0 2 4 6 8\\nsteps / k0.00.51.01.52.02.5KLKL divergence between the trained and the initial policy in 2.7B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k2.42.62.83.03.23.43.6rewardOnline Policy Optimization with 6B model\\nPPO\\nAWR\\nAPA\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0\\nsteps / k0.20.30.40.50.60.70.8KLKL divergence between the trained and the initial policy in 6B model\\nPPO\\nAWR\\nAPA\\nFigure 5: Comparisons of the performance on TLDR dataset. Left: The x-axis represents the total steps,\\nwhich are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\n11https://huggingface.co/EleutherAI/gpt-neo-2.7B\\n12https://huggingface.co/CarperAI/openai_summarize_tldr_sft\\n13https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n14https://huggingface.co/EleutherAI/gpt-j-6b\\n15https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\\n16', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 15}),\n",
       " Document(page_content='C.3 Results on the Dolly Model\\nWe fine-tune the databricks/ dolly-v2-7b16model on the HH dataset. We follow the default setting in\\ntrlX with seed 0 and 100, and plot the results in Figure 6. We only include the results for APAand PPO\\nsince AWR drops directly. Different from all other experiments, here for APAwe set λ= 1rather than 0.1\\nto stablize the training and impose stronger KL control. One can see that APAcan still improve over the\\noriginal dolly 7B model and provide better KL control, while PPO fails to bring further improvement.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k1.6\\n1.5\\n1.4\\n1.3\\n1.2\\nrewardOnline Policy Optimization with Dolly 7B model\\nPPO\\nAPA\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k0.000.020.040.060.080.10KLKL divergence between the trained and the initial policy in dolly 7B model\\nPPO\\nAPA\\nFigure 6: Comparisons of the performance on the dolly 7B model. Left: The x-axis represents the total\\nsteps, which are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 16}),\n",
       " Document(page_content='steps, which are proportional to the number of data used in the training procedure. The y-axis is the reward\\nevaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL\\ndivergence between the trained model and the initial model.\\nC.4 StackExchange experiment details\\nThe hyper-parameters for the Fig. 2 are listed in table 1.\\nParameter Value\\nMax sequence length 1024\\nMax output length 128\\nLearning rate 2e-5\\nBatch size 16\\nGradient Accumulation 8\\nSFT LoRA dimension 16\\nRM LoRA dimension 8\\nRL LoRA dimension 16\\nAdaptive KL initial KL coeff (PPO) 0.1\\nAdaptive KL target KL coeff (PPO) 6\\nλ(APA) 0.1\\nTable 1: Hyper-parameters for StackExchange experiments as shown in Fig. 2\\n16https://huggingface.co/databricks/dolly-v2-7b\\n17', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 16}),\n",
       " Document(page_content='D Ablation Studies\\nD.1 KL control in APA\\nIn this section, we show how the performance and KL divergence change with different values of λ. We set\\nλ= 0.1,1for the 125M model and plot their performances in Figure 7 with seed 1000. One can see that the\\nchoice of λdirectly determines the level of KL control, along with the convergent point APAreaches. This\\nshows that λprovides a clear trade-off between KL control and model performance.\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nLambda=0.1\\nLambda=1\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k0.0000.0250.0500.0750.1000.1250.150KLKL divergence between the trained and the initial policy for APA in 125M model\\nLambda = 0.1\\nLambda = 1\\nFigure 7: Comparisons of the performance between different λon the 125M model. Left: The x-axis\\nrepresents the total steps, which are proportional to the number of data used in the training procedure. The\\ny-axis is the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The\\ny-axis is the KL divergence between the trained model and the initial model.\\nD.2 KL control in PPO', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 17}),\n",
       " Document(page_content='y-axis is the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The\\ny-axis is the KL divergence between the trained model and the initial model.\\nD.2 KL control in PPO\\nWe show how the performance and KL divergence change with or without adaptive KL control in PPO. We\\nplot their performances in Figure 8 for 125M model with seed 1000. For PPO with adaptive KL controller,\\nthe initial KL coefficient is set to be 0.05. One can see that without KL control, PPO converges to a higher\\nreward compared to APAin Figure 7, at a cost of a significantly higher KL divergence. On the other hand,\\nthe reward of PPO with adaptive KL control begins to drop in the middle. This is due to the large deviation\\nfrom the original policy, which leads to a much larger KL regularization term that dominates the reward.\\nCompared with Figure 7, one can see that APAprovides more stable and controllable KL regularization.\\nD.3 Experiments for Offline Learning\\nWe conduct experiments for offline learning as well. The offline dataset is selected to be all the prompts and\\nresponses from the HH dataset, with reward labeled by the reward model. We use the trained GPT-J reward', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 17}),\n",
       " Document(page_content='D.3 Experiments for Offline Learning\\nWe conduct experiments for offline learning as well. The offline dataset is selected to be all the prompts and\\nresponses from the HH dataset, with reward labeled by the reward model. We use the trained GPT-J reward\\nfunction to label the reward for all the offline data, and compare ILQL, AWR and APAon the same 125M\\nand 1B model after supervised fine-tuning with seed 1000. The result is given in Figure 9. From the results,\\none can see that AWR performs better than ILQL, and APAcannot be directly adapted to the offline case.\\nFurthermore, offline learning cannot help too much after the supervised fine-tuning stage, potentially due to\\nthe large distribution shift between the offline data and the current policy.\\n18', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 17}),\n",
       " Document(page_content='0 2 4 6 8 10 12 14\\nsteps / k3.0\\n2.5\\n2.0\\n1.5\\n1.0\\n0.5\\nrewardOnline Policy Optimization with 125M model\\nAdaptive KL Control\\nNo KL Control\\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5\\nsteps / k01234567KLKL divergence between the trained and the initial policy in 125M model\\nAdaptive KL Control\\nNo KL ControlFigure 8: Comparisons of the performance of PPO on the 125M model. Left: The x-axis represents the\\ntotal steps, which are proportional to the number of data used in the training procedure. The y-axis is the\\nreward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the\\nKL divergence between the trained model and the initial model.\\n0 2 4 6 8 10 12\\nsteps / k2.8\\n2.7\\n2.6\\n2.5\\n2.4\\n2.3\\n2.2\\n2.1\\n2.0\\nrewardOffline Policy Optimization with 125M model\\nILQL\\nAWR\\nAPA\\n0 2 4 6 8 10\\nsteps / k2.1\\n2.0\\n1.9\\n1.8\\n1.7\\n1.6\\nrewardOffline Policy Optimization with 1B model\\nILQL\\nAWR\\nAPA\\nFigure 9: Comparisons of the performance between ILQL, AWR and APA on the offline learning dataset.\\n19', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 18}),\n",
       " Document(page_content='E Proof of Theorem 1\\nProof.From the well-specified assumption π⋆∈ {πθ|θ∈Θ}, we know that there exists some θ⋆∈Θsuch\\nthatπθ⋆=π⋆. For the population loss, we know that\\nLAPA(θ⋆) =E(s,a)∼dπolds,ah\\n(logπθ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n=E(s,a)∼dπolds,ah\\n(logπ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThus for any θ′∈arg minθ∈ΘLAPA(θ), there must be LAPA(θ′) = 0, which is equivalent to\\nE(s,a)∼dπolds,ah\\n(logπθ′(a|s)−Adv(s, a)/λ−logπinit(a|s))2i\\n= 0.\\nThis means that for any s, aon the support of dπolds,a, we have πθ′(a|s) =π⋆(a|s).\\nFor the second part of the theorem, we know from Hoeffding’s inequality that for any fixed θ∈Θ,\\n|LAPA(θ)−bLAPA(θ;D)|=\\x0c\\x0c\\x0c\\x0c\\x0c1\\nnnX\\ni=1\\x10\\nlogπθ(ai|si)−Adv(si, ai)/λ−logπinit(ai|si)\\x112\\n−Eh\\x10\\nlogπθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\x0c\\x0c\\x0c\\x0c\\x0c\\n≤C·(B2/λ−2 log( B1))2r\\nlog(1/δ)\\nn.\\nLet the Θϵbeϵ-covering of Θunder ℓ2norm, i.e. for any θ∈Θ, one can find some θ′∈Θϵsuch that\\n∥θ−θ′∥2≤ϵ. We also have |Θϵ| ≤(1/ϵ)d. By taking union bound, we know that for all θ∈Θϵ, with\\nprobability at least 1−δ,\\n|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn. (10)\\nLetˆθbe the minimizer of bLAPA(θ;D). Then we know that there exists some ˆθϵ∈Θϵsuch that ∥ˆθ−ˆθϵ∥ ≤ϵ.\\nThis further implies that', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 19}),\n",
       " Document(page_content='probability at least 1−δ,\\n|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn. (10)\\nLetˆθbe the minimizer of bLAPA(θ;D). Then we know that there exists some ˆθϵ∈Θϵsuch that ∥ˆθ−ˆθϵ∥ ≤ϵ.\\nThis further implies that\\n|LAPA(ˆθ)− LAPA(ˆθϵ)|\\n=|Eh\\x10\\nlogπˆθ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n−Eh\\x10\\nlogπˆθϵ(a|s)−Adv(s, a)/λ−logπinit(a|s)\\x112i\\n|\\n≤C(B2/λ−log(B1))Lϵ. (11)\\nSimilarly, we also have |bLAPA(ˆθ)−bLAPA(ˆθϵ)| ≤C(B2/λ−log(B1))Lϵ.Overall, we have\\nLAPA(ˆθ) = (LAPA(ˆθ)− LAPA(ˆθϵ)) + (LAPA(ˆθϵ)−bLAPA(ˆθϵ)) + (bLAPA(ˆθϵ)−bLAPA(ˆθ)) +bLAPA(ˆθ).\\nFor the first and third difference, from Equation (11) we know that they are both bounded by C(B2/λ−\\nlog(B1))Lϵ.For the second difference, we know from Equation (10) that it is bounded by C(B2/λ−\\nlog(B1))2q\\ndlog(1/(ϵδ))\\nn. Lastly, we know that bLAPA(ˆθ) = 0since ˆθ∈arg minθbLAPA(θ)andbLAPA(θ⋆) = 0.\\nThus overall, we have\\nLAPA(ˆθ)≤C((B2/λ−log(B1))Lϵ+ (B2/λ−log(B1))2r\\ndlog(1/(ϵδ))\\nn).\\nTaking ϵ= 1/(Ln)finishes the proof.\\n20', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents =  text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# db = FAISS.from_documents(documents[:20],OllamaEmbeddings())\n",
    "db = FAISS.from_documents(documents[:20],OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7c7acea5cc10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Advantage-Induced Policy Alignment for fine tunning of LLMs?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Desgin the Prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $100 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}                   \n",
    "</context>\n",
    "                                          \n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "\n",
    "## Create Stuff Document Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRetrievers\\n\\nA retriever is an interface that returns documents given an unstructured query. \\nIt is more general than a vector store. A retriever does not need to be able to store documents, \\nonly to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, \\nbut there are other types of retrievers as well.\\n\\nRetrievers accept a string query as input and return a list of Document's as output.\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. \n",
    "It is more general than a vector store. A retriever does not need to be able to store documents, \n",
    "only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, \n",
    "but there are other types of retrievers as well.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Document's as output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7c7acea5cc10>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retriver\n",
    "retriver = db.as_retriever()\n",
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Combine Retriver and chain => \n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrival_chain = create_retrieval_chain(retriver,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Advantage-Induced Policy Alignment for fine tunning of LLMs?',\n",
       " 'context': [Document(page_content='Fine-Tuning Language Models\\nwith Advantage-Induced Policy Alignment\\nBanghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄\\nChenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡\\nNovember 6, 2023\\nAbstract\\nReinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning\\nlarge language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal\\npolicy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO\\nmay suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can\\nbe alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),\\nwhich leverages a squared error loss function based on the estimated advantages. We demonstrate\\nempiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate\\nreward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable\\nform of control over the deviation from the model’s initial policy, ensuring that the model improves its\\nperformance without collapsing to deterministic output. In addition to empirical results, we also provide', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 0}),\n",
       "  Document(page_content='policies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model\\nwith a target policy in each training epoch. The target policy combines the initial language model policy\\nbefore the RLHF stage and a correction term based on the advantage function estimated from online samples.\\nWecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.\\nAt a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization\\nproblem, relying on the estimated importance ratio between consecutive policies to compute the policy\\ngradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no\\nlonger requires estimating the importance ratio. We show that such differences bring huge benefits in terms\\nof sample efficiency.\\nTo demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B\\nlanguage models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       "  Document(page_content='difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the\\nlanguage model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward\\nmodel produces a scalar score for each generated response, and is, therefore, able to provide the language\\nmodel with online feedback. This accessibility to online feedback allows the language model to be trained\\nvia reinforcement learning (RL), giving rise to the RLHF stage.\\nAmong the RL techniques that are applied to language models, one of the most prominent algorithms\\nis proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO\\n(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample\\nefficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate\\npolicies (Yuan et al., 2022; Dong et al., 2022).\\nTo address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),\\nwhich leverages a squared error loss function that directly aligns the output policy of the language model', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 1}),\n",
       "  Document(page_content='n)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.\\nAs a first step, for each fixed state s, we consider the following KL-regularized optimization problem as\\na target of policy improvement:\\nmaximize\\nθF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL\\x10\\nπθ(· |s)\\r\\rπinit(· |s)\\x11\\n. (2)\\nHere πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy\\nthat we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,\\nand to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state\\ns. The second term in F(θ;s, π), aKLregularizer, controls the deviation of πθfrom πinit. Such regularization\\nis essential, as language models are prone to over-optimization when rewards are generated by an imperfect\\nreward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3})],\n",
       " 'answer': \"Based on the provided context, Advantage-Induced Policy Alignment (APA) is a novel algorithm designed to fine-tune large language models (LLMs). APA leverages a squared error loss function that directly aligns the output policy of the language model with a target policy in each training epoch. The target policy combines the initial language model policy before the RLHF stage and a correction term based on the advantage function estimated from online samples.\\n\\nAPA is proposed to address issues such as mode collapse, instability, and poor sample efficiency in proximal policy optimization (PPO), which is one of the most widely used methods for aligning LLMs with human preferences. APA's key innovation lies in its use of a squared error loss function that regularizes the deviation of the model policy from the initial policy, rather than relying on the estimated importance ratio as PPO and other algorithms do.\\n\\nBy leveraging this squared error loss function, APA is able to consistently outperform PPO in language tasks by a large margin when a separate reward model is employed as the evaluator. Additionally, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output.\\n\\nAPA has been applied empirically to fine-tune up to 7B language models using the human-labeled Anthropic Helpfulness and Harmlessness dataset, demonstrating its efficacy in improving the performance of LLMs.\"}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_chain.invoke({\"input\":\"What is Advantage-Induced Policy Alignment for fine tunning of LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'We consider a finite-horizon MDP represented by a tuple M =(S, A, H, P, r, ρ), where S is a finite state space,',\n",
       " 'context': [Document(page_content='state distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive\\nsteps. At the end of an episode, the agent is reset to a state drawn from ρ(·), and a new episode begins.\\nA policy π:S 7→ ∆(A)is a function that maps a state to a distribution over actions. The value function\\nVπ:S 7→Rof policy πis defined as the expected sum of discounted rewards when the agent starts from\\ninitial state sand follows policy πthroughout the episode. Let γ∈[0,1]be the discount factor. For any\\ns∈ S, we have\\nVπ(s):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nGiven a policy π, the state-action value function, also known as the Q-function, can be defined analogously.\\nFor state s∈ Sanda∈ A, we have\\nQπ(s, a):=E\"HX\\nτ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#\\n.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       "  Document(page_content='2 Preliminaries\\nIn this section, we overview the standard RL setting in Section 2.1, and discuss how language model training\\nfits into this setting in Section 2.2. We use the following notation. For a positive integer n, we will use\\nthe bracket notation [n]to refer to the set of integers {1, . . . , n }; for a finite set Z, we denote by ∆(Z)the\\nset of probability distributions on Z, and |Z|the cardinality of Z. We use Bdto denote the unit ball in\\nd-dimensional space.\\n2.1 Reinforcement Learning\\nReinforcementlearning(RL)capturestheinteractionbetweenanagentandanenvironmentviatheformalism\\nof a Markov decision process (MDP). We consider a finite-horizon MDP represented by a tuple M=\\n(S,A, H, P, r, ρ ), where Sis a finite state space, Ais a finite action space, His the horizon, P:S×A 7→ ∆(S)\\nis a probability transition matrix, r:S × A 7→ [0,1]is a reward function, and ρ:S 7→ ∆(S)is the initial\\nstate distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and\\ntransitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       "  Document(page_content='.\\nWe also define the important notion of an advantage function . For a policy π, state sand action a, the\\nadvantage, defined as\\nAdvπ(s, a) =Qπ(s, a)−Vπ(s),\\nquantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action\\na, when the agent is in state sat step h.\\nWe also define the occupancy measures dπ\\nstate:S 7→ [0,1]anddπ\\naction :S × A 7→ [0,1]as\\ndπ\\nstate(s):=1\\nHHX\\nh=1P(sh=s|π)and dπ\\naction (s, a):=1\\nHHX\\nh=1P(sh=s, ah=a|π),\\nwhere P(· |π)signifies that all actions are drawn from π. To avoid clutter, we overload the notation dπsuch\\nthatdπ(s)refers to dπ\\nstate(s), and dπ(s, a)refers to dπ\\naction (s, a).\\n2.2 Language Model as Reinforcement Learning Agent\\nIn its simplest form, a language model receives as input a sequence of tokens (x1, . . . , x n), and generates a\\ndistribution over the next token xn+1. All tokens lie in a finite set X. Whenever the agent selects a token\\nthat represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens\\nreaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 2}),\n",
       "  Document(page_content='πθ(a|s) =exp(qθ(s, a))P\\na∈Aexp(qθ(s, a)).\\n3 Fine-Tuning Based on Reinforcement Learning\\nAs is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is\\ntrained from a human-labeled dataset. An RL algorithm is then applied to improve the language model\\npolicy, using the rewards generated by the reward model. Here we focus mainly on the second step with a\\ngiven reward function.\\nWe summarize a typical policy-based RL algorithm in Algorithm 1. In practice, the parameter update\\nin Equation (1) usually involves several gradient steps rather than a full minimization.\\nAlgorithm 1 Policy Gradient\\n1:Input:An initial policy parameter θ0, a given loss function L(θ;D).\\n2:Setπ0=πinit.\\n3:Foriteration t= 1,2···, T\\n4:Roll out πθt−1to produce dataset Dt=n\\n(s(t)\\n1, a(t)\\n1, r(t)\\n1),···,(s(t)\\nn, a(t)\\nn, r(t)\\nn)o\\n5:Update policy parameter according to\\nθt= arg min\\nθL(θ;Dt). (1)\\nIn the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal\\nof maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions\\nbehind it.', metadata={'source': './Fine-Tuning_Language_Models.pdf', 'page': 3})],\n",
       " 'answer': 'Based on the provided context, the answer to your question is:\\n\\nThe state distribution.\\n\\nThis is because in the given context, an MDP (Markov Decision Process) is being considered, and the tuple M represents this process. The components of the tuple are: S (finite state space), A (finite action space), H (horizon), P (probability transition matrix), r (reward function), and ρ (initial state distribution).'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrival_chain.invoke({\"input\":\"We consider a finite-horizon MDP represented by a tuple M =(S, A, H, P, r, ρ), where S is a finite state space,\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the answer to your question is:\\n\\nThe state distribution.\\n\\nThis is because in the given context, an MDP (Markov Decision Process) is being considered, and the tuple M represents this process. The components of the tuple are: S (finite state space), A (finite action space), H (horizon), P (probability transition matrix), r (reward function), and ρ (initial state distribution).'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
